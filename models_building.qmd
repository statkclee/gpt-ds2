---
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

[@coelho2015building]

# 모형 개발 {#sec-model-building}

통계학은 실제로 데이터를 다루는 가장 오래된 학문 중 하나이며, 기본적으로
두 가지 큰 줄기, 즉 **기술 통계**(Descriptive Statistics)와 **추론
통계**(Inferential Statistics)로 나눌 수 있다. 기술 통계는 데이터를
요약하고 설명하는 데 초점을 맞추며, 추론 통계는 표본 데이터를 바탕으로
보다 큰 모집단에 대한 결론이나 예측을 하는 데 사용된다.

기계학습은 통계학의 기술 통계와 추론 통계 모두와 관련이 있지만, 그
중에서도 주로 추론 통계에 무게를 두고 있다. 기계학습의 주된 목적은
데이터를 바탕으로 모형을 구축하여 새로운 데이터에 대한 예측이나 일반화를
수행하는 것을 목표로 한다.

기계학습[@abu2012learning]은 다음 세가지 사항을 기반으로 하고 있다.
패턴은 존재한다고 가정하는 부분에서 통계학의 회귀분석과 유사하나,
수학적으로 명세를 할 수 없다는 점에서 차이가 난다. 기계학습이나
회귀모형이나 둘다 데이터를 기반으로 한다는 면에서 공통점을 갖는다.

1.  패턴이 존재한다.
2.  수학적으로 명시적으로 명세할 수 없다.
3.  데이터를 갖고 있다.

기계학습을 구성하는 이론은 편향-분산(bias-variance), 복잡성,
[Vapnik–Chervonenkis
이론](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory),
베이즈통계가 이론이 되고, 선형회귀모형을 비롯한 다양한 모형이 존재하고,
모형의 성능과 신뢰성을 높이고자 데이터 전처리, 교차타당성(cross
validation), 정규화(regularization)등이 동원된다.

![기계학습 지도](images/ml-map.png){#fig-ml-map fig-align="center"
width="585"}

## 기계학습 구성요소 {#ml-basic-elements}

데이터 과학에서 모형은 현실 세계의 복잡한 현상을 단순화하고 추상화하여
표현하는 도구이다. 모형을 통해 데이터에 내재된 패턴과 관계를 발견하고,
미래를 예측하며, 의사 결정을 지원할 수 있다. 모형을 구축할 때,
데이터에서 진정한 신호를 추출하고 잡음을 제거하는 것을 목표로 한다. 일반
모형을 "신호 + 잡음(signal + noise)"로 가정하고 다음과 같은 수식으로
표현할 수 있다.

$$y = f(x) + \epsilon$$

1.  출력 : $y$, 관심갖고 있는 결과변수
2.  입력 : $x$, 설명/예측 변수
    -   $y$의 변동성을 설명하는 목적의 모형을 구축하는 경우 $x$는
        설명변수
    -   $y$의 변동성을 예측하는 목적의 모형을 구축하는 경우 $x$는
        예측변수
3.  가설: : $g: x \rightarrow y$, $x$는 $y$에 영향을 주는 인과관계가
    존재한다.
4.  목적함수 : $f: x \rightarrow y$, $y$와 $x$를 연관시켜주는 함수
5.  데이터: $(x_1 , y_1 ), (x_2 , y_2 ), \dots, (x_n , y_n )$
6.  오차: $\epsilon$, $f: x \rightarrow y$으로 설명되지 않는 부분

결국, 잡음이 낀 데이터에서 잡음을 제거하고 신호만 뽑아내는 것이
회귀모형, 기계학습 모형이라고 볼 수 있다. 회귀모형과 기계학습 모형은
회귀모형이 특정 함수형태를 가정하고 데이터에서 신호와 잡음을 구부하는데
초점이 과거 맞춰졌다면, 기계학습모형은 $x$는 $y$의 인과관계를 가정으로
놓고 신호와 잡음을 가장 잘 발라낼 수 있는 함수를 찾아내는데 초점을 두고
있다.

![기계학습 도해](images/ml-process-math.png){#fig-ml-process-diagram
fig-align="center" width="395"}

## 기계학습 세 가지 원칙

기계학습 알고리즘 개발에는 세 가지 기본 원칙이 적용된다. 첫째, **오컴의
면도날** 원칙에 따라 더 단순한 간결한 모형을 선택한다. 둘째, **표집
편향**을 고려해 모집단을 잘 대표하는 데이터를 사용하여 정확성을
확보한다. 셋째, **데이터 염탐 편향**을 회피해 알고리즘 선택을 객관적으로
한다. [^models_building-1]

[^models_building-1]: [Caltech MOOC, Yaser Abu-Mostafa, Introductory
    Machine Learning, 2012](https://work.caltech.edu/telecourse.html)

1.  **오컴의 면도날(Occam's Razor)**: 절약의 원리(Principle of
    Parsimony)라고도 불리며, 동일한 현상을 설명하는 여러 모형이 있을 때,
    가장 단순한 모형을 선택하는 것이 좋다는 원리다.
    과적합(overfitting)을 방지하고 모형의 일반화 성능을 높이는 데 도움이
    된다.

2.  **표집 대표성(Sampling Representativeness)**: 기계학습 알고리즘을
    훈련시키는 데 사용되는 데이터는 전체 모집단을 대표할 수 있어야 한다.
    **표집 편향(Sampling Bias)**이 발생하면, 즉 훈련 데이터가 모집단을
    제대로 반영하지 못하면 알고리즘의 성능 저하는 물론 일반화 능력도
    떨어진다.

3.  **데이터 염탐 편향(Data Snooping Bias)**: 데이터를 본 후에 기계학습
    알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습
    알고리즘을 선정해야 된다. 데이터를 보고 나서 알고리즘을 선택하면,
    데이터에 포함된 편향이나 패턴을 알고리즘이 학습할 수 있기 때문에
    일반화 능력이 떨어진다.

![기계학습 세가지
원칙](images/ml-three-principle.png){#fig-ml-three-principle
fig-align="center" width="414"}

::: callout-note
### 중요한 세가지 원칙

데이터에 기반한 모형개발에서 준수해야 할 세 가지 원칙은 다음과 같다.

-   오컴의 면도날

동일한 조건이면 더 단순한 것을 선택하는 것으로, 가장 큰 이유는 갖고 있는
데이터를 벗어나 새로운 데이터를 갖게 될 경우 학습시킨 기계학습
알고리즘이 더 좋은 성능을 보인다는 것이다. 결국 수많은 가능한 모형중에서
하나를 선택하는 기준이 된다.

> An explanation of the data should be made as simple as possible, but
> no simpler -- Albert Einstein

-   표집 편향 제거

[1948년 미국
대통령선거](https://ko.wikipedia.org/wiki/1948%EB%85%84_%EB%AF%B8%EA%B5%AD_%EB%8C%80%ED%86%B5%EB%A0%B9_%EC%84%A0%EA%B1%B0)에서
트루먼이 듀이 후보를 물리치고 대통령이 된 것은 알려진 사실이다. 하지만,
대부분의 여론조사에서 듀이의 승리를 예상했지만, 사실은 그 반대로
나타났다. 그 당시 여론조사를 전화기를 사용하였는데, 문제는 전화기가
부유층이 많이 소유하고 있어 미국 대통령선거 모집단을 대표하는 대표성에
문제가 있어 왜곡된 결과가 도출된 것이다.

상업적으로 개인금융의 신용카드발급, 신용평가에도 동일한 문제가 발생한다.
수익성은 저신용자가 높아 이를 살펴보면, 신용평가에 사용될 데이터는
저신용자는 카드를 발급받을 수 없어 데이터베이스에는 표집편향된
고객정보만 존재하는 것을 어렵지 않게 볼 수 있다.

> If the data is sampled in a biased way, learning will produce a
> similarly biased outome.

-   데이터 염탐 편향 제거

데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기
전에 기계학습 알고리즘을 선정해야 하지만, 현실적으로 현업에서 작업하는
사람들이 흔히 범하는 실수다. 동일한 데이터에 대해 갖가지 기계학습
알고리즘을 적용해서 가장 좋은 성능이 나오는 알고리즘을 선정한다. 문제는
데이터가 바뀌면 어떨까? 아마 기대했던 성능이 나오지 못할 가능성이 크다.

> If you torture the data long enough, it will confess
:::

## 모형 개발과정

통계모형 개발과정은 데이터 과학 프로세스에서 크게 차이가 나지 않는다.
다만, 일반적인 통계모형을 개발할 경우 다음과 같은 과정을 거치게 되고,
지난한 과정이 될 수도 있다.

1.  데이터를 정제하고, 모형에 적합한 데이터(R/파이썬과 모형 패키지와
    소통이 될 수 있는 데이터 형태)가 되도록 준비한다.
2.  변수에 대한 분포를 분석하고 기울어짐이 심한 경우 변수변환도
    적용한다.
3.  변수와 변수간에, 종속변수와 설명변수간에 산점도와 상관계수를
    계산한다. 특히 변수간 상관관계가 $r > 0.9$ 혹은 근처인 경우 변수를
    빼거나 다른 방법을 강구한다.
4.  동일한 척도로 회귀계수를 추정하고 평가하려는 경우, `scale()` 함수로
    척도로 표준화한다.
5.  모형을 적합시킨 후에 잔차를 보고, 백색잡음(whitenoise)인지 확인한다.
    만약, 잔차에 특정한 패턴이 보이는 경우 패턴을 잡아내는 모형을 새로
    개발한다.
    1.  `plot()` 함수를 사용해서 이상점이 있는지, 비선형관계를 잘
        잡아냈는지 시각적으로 확인한다.
    2.  다양한 모형을 적합시키고 `R^2` 와 `RMSE`, 정확도 등 모형평가
        결과가 가장 좋은 것을 선정한다.
    3.  절약의 원리(principle of parsimony)를 필히 준수하여 가장 간결한
        모형이 되도록 노력한다.[^models_building-2]
6.  최종 모형을 선택하고 모형에 대한 해석결과와 더불어 신뢰구간 정보를
    넣어 마무리한다.

[^models_building-2]: 간결성 원칙으로도 번역되며, 오캄의 면도날("Occam's
    razor")라는 이름으로도 알려져 있으며, 연구나 문제 해결의 맥락에서
    가장 단순한 설명이나 가설을 우선적으로 고려해야 한다는 의미로, 두 개
    이상의 설명이 관찰된 현상을 동등하게 설명할 수 있을 때, 더 적은
    가정이 필요한 또는 더 단순한 설명을 선호해야 한다는 것이다.

::: callout-note
### 키보드로 통계모형 표현법

수학적 표현을 프로그래밍 언어(R, 파이썬 등)로 전환하는 필요성은 키보드
입력의 제한성 때문에 발생한다. 키보드 특수문자를 최적으로 활용하여,
R에서의 구현은 아래와 같이 가장 읽기 쉽고 입력하기 편리한 형식으로
다음과 같이 정리할 수 있다.

1.  주효과에 대해 변수를 입력으로 넣을 `+`를 사용한다.
2.  교호작용을 변수간에 표현할 때 `:`을 사용한다. 예를 들어 `x*y`는
    `x+y+x:z`와 같다.
3.  모든 변수를 표기할 때 `.`을 사용한다.
4.  종속변수와 예측변수를 구분할 때 `~`을 사용한다. `y ~ .`은
    데이터프레임에 있는 모든 변수를 사용한다는 의미다.
5.  특정변수를 제거할 때는 `-`를 사용한다. `y ~ . -x`는 모든 예측변수를
    사용하고, 특정 변수 `x`를 제거한다는 의미다.
6.  상수항을 제거할 때는 `-1`을 사용한다.

| 구문     | 수학적 표현                                                                      | 설명                                                    |
|---------|--------------------|------------------------------------------|
| `y~x`    | $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$                                       | `x`를 `y`에 적합시키는 1차 선형회귀식                   |
| `y~x -1` | $y_i = \beta_1 x_i + \epsilon_i$                                                 | `x`를 `y`에 적합시 절편 없는 1차 선형회귀식             |
| `y~x+z`  | $y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i +\epsilon_i$                          | `x`와 `z`를 `y`에 적합시키는 1차 선형회귀식             |
| `y~x:z`  | $y_i = \beta_0 + \beta_1 x_i \times z_i +\epsilon_i$                             | `x`와 `z` 교호작용 항을 `y`에 적합시키는 1차 선형회귀식 |
| `y~x*z`  | $y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \beta_1 x_i \times z_i +\epsilon_i$ | `x`와 `z`, 교호작용항을 `y`에 적합시키는 1차 선형회귀식 |
:::

## 과적합 모형 시각화

$y=x^2 + \epsilon$ 오차는 정규분포 평균 0, 표준편차 0.2를 갖는 모형을
따른다고 가정하고, 이를 차수가 높은 다항식을 사용하여 적합시킨 결과를
확인하는 절차는 다음과 같다.

1.  `tidyr`, `modelr`, `ggplot2` 팩키지를 불러와서 환경을 설정한다.
2.  $y=x^2 + \epsilon$, 오차는 $N(0, 0.25)$을 따르는 모형을 생성하고,
    `df` 데이터프레임에 결과를 저장한다.
3.  `poly_fit_model` 함수를 통해 7차 다항식으로 적합시킨다.
4.  적합결과를 `ggplot`을 통해 시각화한다.

1차에서 10차까지 차수를 달리하여 적합시켜 시각적으로 적합도를 확인할 수
있다.

```{r}
#--------------------------------------------------------------------------------
# 01. 환경설정
#--------------------------------------------------------------------------------
library(tidyverse)
library(modelr)

#--------------------------------------------------------------------------------
# 02. 참모형 데이터 생성: y = x**2
#--------------------------------------------------------------------------------

true_model <- function(x) {
  y = x ** 2 + rnorm(length(x), sd=0.25)
  return(y)
}

x <- seq(-1, 1, length=20)
y <- true_model(x)
df <- tibble(x,y)

#--------------------------------------------------------------------------------
# 03. 10차 다항식 적합
#--------------------------------------------------------------------------------

poly_fit_model <- function(df, order) {
  lm(y ~ poly(x, order), data=df)
}

# fitted_mod <- poly_fit_model(df, 10)

#--------------------------------------------------------------------------------
# 04. 적합결과 시각화
#--------------------------------------------------------------------------------

fitted_tbl <- tibble(차수 = 1:10) |> 
  mutate(데이터 = list(df)) |> 
  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |> 
  mutate(예측  = map(적합된모형, fitted)) |> 
  mutate(x = list(seq(-1, 1, length=20))) |>
  mutate(fitted_df = map2(x, 예측, ~tibble(x_value = .x, 
                                         prediction = .y, 
                                         .name_repair = "unique"))) |> 
  select(차수, fitted_df) |> 
  mutate(차수 = as.factor(차수)) |>   
  unnest(fitted_df) 


df %>% 
  ggplot(aes(x, y)) +
    geom_point() +
    geom_line(data = fitted_tbl, aes(x = x_value, y = prediction, color = 차수, group = 차수))

```

다항식 차수를 달리하여 데이터에 1\~10차 다항식을 적합시킨 후에 오차를
계산했다. 1차보다 2차 다항식으로 적합시킬 때 오차가 확연히 줄어들지만 그
이후에는 오차의 감소폭이 크지는 않다. 물론 오차는 다항식 차수를 높일수록
낮아지지만 절약성의 원칙을 생각하면 이와 같은 고차 다항식 함수가
필요한지는 의문이다.

```{r}
#| label: fig-poly-fit
#| fig-cap: 데이터 과적합 모형 시각화
tibble(차수 = 1:10) |> 
  mutate(데이터 = list(df)) |> 
  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |> 
  mutate(예측  = map(적합된모형, fitted)) |> 
  mutate(true_y = map(데이터, 2)) |>
  # 참값과 모형예측값
  mutate(error_df = map2(true_y, 예측, ~tibble(true_y = .x,  fitted_y = .y))) |> 
  # 오차
  mutate(rmse = map_dbl(error_df, ~ sqrt(mean((.x$true_y - .x$fitted_y)^2)))) |> 
  mutate(차수 = factor(차수)) |> 
  mutate(color = c("gray30", "blue", rep("gray30", 8))) |> 
  # 시각화
  ggplot(aes(x = fct_rev(차수), y = rmse, fill = color)) +
    geom_col(width = 0.3) +
    coord_flip() +
    labs( x = "차수",
          y = "RMSE 오차",
          title = "다항식 차수를 달리하여 계산한 RMSE 오차") +
    scale_fill_manual(values = c("blue", "gray30", rep("blue", 8))) +
    theme(legend.position = "none")
```
