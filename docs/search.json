[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "서문",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#책의-구성",
    "href": "index.html#책의-구성",
    "title": "챗GPT 데이터 과학",
    "section": "책의 구성",
    "text": "책의 구성",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#감사의-글",
    "href": "index.html#감사의-글",
    "title": "챗GPT 데이터 과학",
    "section": "감사의 글",
    "text": "감사의 글\n\n이 책이 탄생할 수 있도록 도움을 주신 여러분께 깊은 감사의 마음을 표합니다.\n공익법인 한국 R 사용자회가 없었다면 데이터 과학분야 챗GPT 시리즈가 세상에 나오지 못했을 것입니다. 한국 R 사용자회의 유충현 회장님, 신종화 사무처장님, 홍성학 감사님, 올해부터 새롭게 공익법인 한국 R 사용자를 이끌어주실 형환희 회장님께 감사드립니다.\n또한 이 책은 2014년 처음 몸담게 된 소프트웨어 카펜트리 그렉 윌슨 박사님과 Python for Informatics 저자인 미시건 대학 찰스 세브란스 교수님을 비롯한 전세계 수많은 익명의 기여자들의 노력과 지원이 있었고, 서울 R 미트업에서 발표해주시고 참여해주신 수많은 분들이 격려와 영감을 주셨기에 가능했습니다.\n이 책이 출간되는데 있어 이들 모든 분들의 도움 없이는 어려웠을 것입니다. 그동안의 관심과 지원에 깊은 감사를 드리며, 이 책이 데이터 과학의 발전과 독자들에게 도움이 될 수 있기를 바라는 마음으로 마무리하겠습니다.\n\n2024년 4월 속초 영금정\n이광춘",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  들어가며",
    "section": "",
    "text": "1.1 좋은 소프트웨어 작성법\n좋은 소프트웨어를 작성하려면,\n위의 원칙을 지키면 다른 동료 연구자들과 미래의 자신에게 읽기 쉽고, 이해하기 쉽고, 확장 가능한 코드를 선물할 수 있을 것이다.",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>들어가며</span>"
    ]
  },
  {
    "objectID": "intro.html#좋은-소프트웨어-작성법",
    "href": "intro.html#좋은-소프트웨어-작성법",
    "title": "1  들어가며",
    "section": "",
    "text": "프로젝트를 적절히 구조화하고,\n필요한 사항은 문서화하고,\n복잡한 문제는 작은 모듈로 나누어 해결하고,\n코드 작동을 검증하는 테스트를 만들고,\n반복되는 코드는 함수로 묶어서 처리하고,\n일관된 코딩 스타일을 고수해야 한다.\n\n\n\n1.1.1 프로젝트 폴더 구조화\n하위 폴더를 코드, 매뉴얼, 데이터, 바이너리, 출력 그래프 등으로 구분하여 프로젝트 폴더를 구조화하고, 잘 조직화하고, 깔끔하게 만든다. 완전 수작업으로 할 수도 있고, RStudio New Project 기능을 활용하거나 ProjectTemplate 같은 패키지를 사용한다.\n\n\n\n\n\n\nProjectTemplate - 가능한 해결책\n\n\n\n프로젝트 관리를 자동화하는 한 방식은 제3자 패키지, ProjectTemplate을 설치하는 것이다. 해당 패키지는 프로젝트 관리에 대한 이상적인 디렉토리 구조를 설정해 놓는다. 패키지가 자동으로 분석 파이프라인/작업 흐름을 구성해서 구조화해 놓는다. RStudio 기본 설정된 프로젝트 관리 기능과 Git을 섞어 사용하면, 작업을 기록할 뿐만 아니라, 동료 연구원과 작업 산출물을 공유할 수 있게 한다.\n\n\nProjectTemplate을 설치한다.\n라이브러리를 불러 적재한다.\n프로젝트를 초기화한다.\n\n\ninstall.packages(\"ProjectTemplate\")\n\nlibrary(ProjectTemplate)\ncreate.project(\"../my_project\", merge.strategy = \"allow.non.conflict\")\n\nProjectTemplate과 기능에 대한 자세한 사항은 ProjectTemplate 홈페이지를 방문한다.\n\n\n\n1.1.2 가독성 높은 코드 생성\n코드 작성에 있어 가장 중요한 부분은 코드를 가독성 있고 이해할 수 있게 작성하는 것이다. 누군가 여러분이 작성한 코드를 보고 무슨 작업을 수행하는지 이해할 수 있어야 한다. 흔히 누군가는 6개월 후에 바로 당신이 될 수 있고, 만약 그렇게 작성하지 않았다면 과거 자기 자신을 분명히 저주하게 될 것이다.\n\n1.1.3 문서화\n처음 코드를 작성할 때, 주석은 명령어가 무엇을 수행하는지 기술한다. 왜냐하면, 여전히 학습 중이라 개념을 명확히 하고, 나중에 다시 상기하는 데 도움이 되기 때문이다. 하지만, 이러한 주석은 나중에 작성한 코드가 어떤 문제를 해결하고자 하는지 기억하지 못하면 그다지 도움이 되지 않는다.\n왜(why) 문제를 해결하려고 하는지, 그리고 어떤(what) 문제인지 전달하는 주석을 달려고 노력한다. 어떻게(how)는 그 다음에 온다. 정말 걱정할 필요가 없는 사항은 구체적인 구현이다.\n\n1.1.4 코드 모듈화\n소프트웨어 카펜트리에서 추천하는 것은 작성한 함수를 분석 스크립트와 구별해서 별도 파일에 저장하는 것이다. 프로젝트 R세션을 열 때, source 함수로 불러올 수 있게 별도 파일로 저장한다.\n분석 스크립트를 깔끔하게 유지하고, 유용한 함수 저장소를 프로젝트 분석 스크립트에 적재할 수 있게 함으로써 이러한 접근법이 깔끔하다. 또한 관련된 함수를 쉽게 그룹화한다.\n\n1.1.5 문제를 작게 분해\n처음 시작할 때, 문제 해결과 함수 작성은 어마어마한 작업이고, 코드를 쪼개는 것도 힘들다. 문제를 소화 가능한 덩어리로 쪼개고, 나중에 구현에 관한 구체적인 사항을 걱정한다. 해결책을 코드로 작성할 수 있는 지점까지 문제를 더 작게 그리고 더 작은 함수로 계속 쪼개 나간다. 그리고 나서 다시 거꾸로 조립해서 만들어 낸다.\n\n1.1.6 코드 테스트\n작성한 코드가 올바른 작업을 수행하도록 만든다. 작성한 함수를 테스트해서 확실히 동작하게 만든다.\n\n1.1.7 복붙 금지\n복붙은 코드를 작성하는 데 있어서 가장 나쁜 방법이다. 함수는 프로젝트 내부에서 재사용을 쉽게 한다. 프로젝트를 통해서 유사한 코드 라인 덩어리를 보게 되면, 대체로 함수로 옮겨져야 하는 대상을 찾은 것이다.\n연산 작업이 연속된 함수를 통해 실행되면, 프로젝트는 모듈로 만들기 쉽고, 변경하기 쉽다. 항상 특정한 입력값을 넣으면 특정한 출력값이 나오는 경우에 특히 그렇다.\n\n1.1.8 스타일 고집\n코드에 일관된 스타일을 지킨다. 이것은 코드를 읽고 이해하는 데 도움이 된다.\n\n1.1.9 코드 스타일과 가독성\n일관된 코딩 스타일은 가독성을 높이고 버그를 최소화한다. R 커뮤니티에서 가장 널리 사용되는 스타일 가이드 중 하나는 해들리 위컴 스타일 가이드다. Google의 R 스타일 가이드도 널리 사용된다.\n일관된 스타일에 프로젝트를 고수하면, 다른 사람들이 코드를 더 쉽게 읽고 확장할 수 있다. RStudio의 코드 정리 기능을 활용하여 코드 스타일을 자동으로 적용할 수도 있다.\n\n1.1.10 핵심은 일관성\n함수 이름 붙이기, 코드 들여쓰기, 주석 달기 등 코딩 스타일의 어떤 요소든 한번 선택하면 이후로도 꾸준히 같은 스타일을 고수해야 한다. 코드 작성자에게는 편한 스타일이 좋지만, 팀 단위로 협업한다면 팀에서 합의한 코딩 스타일을 따르는 것이 바람직하다.",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>들어가며</span>"
    ]
  },
  {
    "objectID": "intro_tidyverse.html",
    "href": "intro_tidyverse.html",
    "title": "2  데이터 사이언스 운영체제",
    "section": "",
    "text": "3 패러다임의 변화 1\n정보를 잃지 않고 압축하거나 최대한 많은 정보를 추출하고자 하는 노력으로 통계학 이론들이 많이 활용되었지만, 최근에는 기계학습/딥러닝을 통해 다양한 통계 모형이 이론적인 면뿐만 아니라 실무에서도 많이 사용되고 있다. 이런 점에서 만 개가 넘는 R 패키지가 개발되어 활용된다는 점은 긍정적이지만, 각자의 설계 원칙에 맞춰 제각기 개발되고, 손을 바꿔 여러 다른 사람들이 유지보수하게 되면서 초기 세워진 설계 원칙과 철학이 많이 무너진 점이 지속 가능한 데이터 사이언스 발전을 가로막고 있는 커다란 장애물로 떠올랐다. 이를 인지한 데이터 과학자는 다수 존재하지만, 이를 체계적으로 실제로 활용할 수 있도록 아마도 가장 크게 기여한 분을 꼽으라면 다들 “해들리 위컴”(Hadley Wickham)을 꼽는 데 주저하지 않을 것이다. tidyverse는 수많은 기여자들의 도움을 받아 해들리 위캄이 오랜 동안 나름대로의 방식으로 체계화시킨 것을 확대 발전시킨 것으로 데이터 사이언스를 체계적으로 집대성하였다는 평가를 받고 있다. 특히, 데이터 과학자 및 실무자에게 큰 도움을 주었는데, 어떻게 보면 기존 SAS/SPSS/미니탭과 같은 상용 패키지 중심에서 R을 중심으로 한 오픈소스 소프트웨어로 흐름을 바꾸었다고 볼 수 있다. 데이터를 다루려면 컴퓨터가 필요하고 컴퓨터와 대화하기 위해서는 언어가 필요한데, 다행히 통계학에서는 R 언어가 1990년대 초반 개발되자마자 오픈소스로 공개되어 자연스럽게 통계학 전공자들이 오픈소스 소프트웨어에 친숙하게 되는 계기가 되었다.\n마이크로소프트 창업자 빌 게이츠가 소프트웨어 상업화 모델로 큰 성공을 거두면서 기존 IBM이 만들어 놓았던 생태계가 크게 변화했지만, 지나친 소프트웨어 상업화에 대한 반발로 리차드 스톨만이 씨를 뿌리고 리누스 토발즈가 튼튼한 뿌리를 내린 리눅스 운영체제, 인터넷의 보급으로 오픈소스 소프트웨어(Open Source Software)에 대한 확산, 개방과 공유를 가치로 하는 개발자 특유의 문화를 기반으로 이제 대세는 오픈소스 소프트웨어가 되었다. 이러한 소프트웨어에 대한 인식의 전환은 GNU 선언문에 잘 표현되어 있다.\n데이터를 많이 다루는 통계학도 소프트웨어 패러다임의 변화에 맞춰 진화해 왔다. SAS/SPSS/미니탭으로 대표되는 통계 패키지와 더불어 가우스/매트랩/매스매티카를 필두로 한 고급 상용 소프트웨어가 한 시대를 풍미했다면, 이제는 R과 파이썬으로 대표되는 오픈소스 소프트웨어가 그 빈자리를 급격히 채워 나가고 있다. 이러한 변화를 직감했던지 해들리 위컴은 2017년 11월 13일 Tidyverse 선언문(Tidy Tools Manifesto)을 직접 작성하여 웹사이트에 공개하였다.\n데이터 사이언스 언어 R을 사용해서 산적한 과제를 해결해 나가면서 쌓인 여러 지적 자산을 패키지로 개발하여 공개하였고, 이를 통칭하여 과거 Hadleyverse로 불렀다. 이유는 dplyr, ggplot2 패키지를 해들리 위컴이 제작했고, 데이터 사이언스 문제를 풀려고 하면 해들리 위컴이 제작한 패키지를 조합해서 접근해야 수월히 풀 수 있었기 때문이다. 하지만, 오픈소스 소프트웨어 개발은 한 사람의 노력으로만 가능한 것이 아니고, 전 세계 수많은 개발자와 사용자의 노력으로 이뤄낸 성과이기에 이를 tidyverse라는 명칭으로 통일하면서 데이터 사이언스 운영체제와 같은 역할과 위상을 가지게 되었다.",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "intro_tidyverse.html#tidyverse-workflow",
    "href": "intro_tidyverse.html#tidyverse-workflow",
    "title": "2  데이터 사이언스 운영체제",
    "section": "4.1 tidyverse 작업 흐름",
    "text": "4.1 tidyverse 작업 흐름\ntidyverse의 핵심적인 내용은 다양한 형태의 데이터를 가져와서 최종 산출물을 사람과 기계가 커뮤니케이션할 수 있는 형태로 제작하는 과정을 추상화한 것으로 이해할 수 있다. 시각화(Visualization)는 데이터에 대한 통찰력(insight)과 탄성, 놀라움을 줄 수 있지만, 확장성(Scalability) 측면에서는 한계가 명확히 존재하게 되는데 이는 사람이 작업 흐름 루프에 포함되기 때문이다. 반대로 모형(Model)은 자동화와 확장성에는 장점이 있지만, 주어진 모형 틀 안에서만 이뤄지기 때문에 통찰력, 놀라움, 탄성을 주지는 못하는 아쉬움이 있다. 따라서, tidyverse는 시각화와 모형을 통해 통찰력과 함께 자동화에 대한 부분도 충분히 반영한 체계적인 작업 흐름을 제시하고 있는데, 이를 관통하는 핵심적인 개념이 파이프(pipe)로 이미 이런 개념은 유닉스 파이프 연산자를 통해 검증되었다.\n\n\n\nTidyverse 데이터 사이언스 작업흐름",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "intro_tidyverse.html#tidy-data",
    "href": "intro_tidyverse.html#tidy-data",
    "title": "2  데이터 사이언스 운영체제",
    "section": "4.2 깔끔한 데이터",
    "text": "4.2 깔끔한 데이터\n기존 소프트웨어와 다르게 tidyverse는 데이터를 중심으로 다루기 때문에 깔끔한 데이터(tidy data)에 대한 이해도 추가로 필요로 한다. 깔끔한 데이터(tidy data)의 정의는 데이터를 통해 정보를 추출하고, 인사이트를 도출하기 위해서 시각화를 하고, 데이터를 모형으로 자동화하고, 커뮤니케이션을 위한 웹앱을 개발하고 발표 보고서를 작성할 때 수월한 자료구조를 갖는 데이터를 의미한다. 엄밀한 의미로 깔끔한 데이터를 전산학 데이터베이스 이론을 가져와서 설명할 수도 있지만, 비전산 전공자의 관점에서 풀어보자면 깔끔한 데이터가 준비되면 정제작업과 변형, 모형 개발, 시각화, 보고서 작성을 원활히 할 수 있는 반면 엉망진창인 데이터(messy data)는 그렇지 않은 데이터로 볼 수 있다.\n깔끔한 데이터는 특정한 구조를 갖추고 있는데 변수는 열(column)이고, 관측점은 행(row)이며, 관측 단위에 대한 형태는 데이터셋 즉, 테이블(table)로 구성된다.\n깔끔한 데이터 원칙은 전산학 코드(Codd) 박사의 관계대수(Relational Algebra)와 깊은 관련이 있어, 통계학 전공자들은 해당 데이터셋에서 관측점과 변수를 각각 식별하는 작업을 쉽게 생각하지만, 일반적으로 변수와 관측점을 정확하게 정의하는 것이 보통 어려운 것은 아니다. 따라서, 행과 행보다는 변수 간 기능적 관계(functional relationship)를 기술하는 것이 더 쉽고, 칼럼 그룹 집단 간 비교보다 관측점 그룹 집단 사이 비교를 하는 것이 더 쉽다.\n깔끔한 데이터(tidy data)는 데이터셋의 의미를 구조에 매칭하는 표준적인 방식으로 이와 같이 데이터가 구조화되면, 데이터 분석, 조작, 시각화, 모형 작업을 수월히 진행할 수 있다.\n\n각 변수가 칼럼이 된다.\n각 관측점은 행이 된다.\n관측 단위에 대한 형태는 테이블로 구성한다.\n\n\n\n\n저장 구분\n의미\n\n\n\n\n테이블/파일(table/file)\n데이터셋 (dataset)\n\n\n행(row)\n관측점 (observation)\n\n\n열(column)\n변수 (variable)\n\n\n\n깔끔하지 않는 데이터(messy data)는 위와는 다른 형태의 데이터를 지칭한다. 전산학에서 말하는 코드 제3 정규형이지만, 통계적 언어로 다시 표현한 것이다. 또한, 깔끔한 데이터는 R같은 벡터화 프로그래밍 언어에 특히 잘 맞는다. 왜냐하면 동일한 관측점에 대한 서로 다른 변수 값이 항상 짝으로 매칭되는 것을 보장하기 때문이다.\n시각화와 모형을 새로 개발하는 데 별도 자료구조(data structure)를 다시 창조하는 대신에 가능하면 기존 자료구조를 재사용하는 것을 원칙으로 삼고 있다. ggplot2, dplyr, tidyr을 포함한 대다수 R 패키지는 칼럼에 변수, 행에 관측점을 갖는 직사각형 형태 데이터셋을 가정하고 있다. 그리고, 일부 패키지는 특정한 변수 자료형에 집중한다. stringr은 문자열, lubridate는 날짜/시간, forcats는 요인 자료형에 집중하고 있지만 모두 기존 자료구조 재사용을 염두에 두고 있는 것도 사실이다.",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "intro_tidyverse.html#tidyverse-functional-programming",
    "href": "intro_tidyverse.html#tidyverse-functional-programming",
    "title": "2  데이터 사이언스 운영체제",
    "section": "4.3 함수형 프로그래밍",
    "text": "4.3 함수형 프로그래밍\n복잡한 문제를 해결하는 강력한 전략은 레고 블록처럼 다수의 간단한 조각으로 나누고 이를 조합하는 것이다. 단, 각 조각은 격리되어 쉽게 파악되며, 다른 조각과 조합할 수 있는 표준 위에서 성립되어야 한다. tidyverse 밑바탕에는 이런 전략이 파이프 연산자를 통해 구현되어 있고, 파이프 연산자(예를 들어, %&gt;%)로 단순한 함수를 조합하여 시스템 전체의 힘을 극대화시킨다.\n이를 위해서는 무엇보다 레고 블록 같은 패키지 혹은 함수가 동일한 인터페이스 표준을 준수해야만 한다. %&gt;% 연산자를 통해 많은 패키지에 걸쳐 동작되도록 만드려면, 함수를 작성할 때 다음 원칙을 준수하여 작성하면 된다.\n\n함수를 가능하면 단순하게 작성한다. 일반적으로 각 함수는 한 가지 작업을 매우 잘해야 되고, 한 문장으로 함수 존재 목적을 기술할 수 있어야 한다.\n변형(transformation)과 부작용(side-effect)을 섞지 말아야 한다. 함수가 객체를 반환하거나, 부작용을 일으키는 둘 중 하나만 동작하게 만든다.\n함수명은 동사로 작성해야 한다. 하지만, 다수의 함수가 동일한 동사를 사용하는 경우는 예외로 한다. 예를 들어 modify, add, compute 등을 들 수 있다. 이런 경우 반복되는 동사가 중복되지 않도록 명사에 집중한다. ggplot2가 좋은 예인데, 기존 플롯에 좌표, 점, 범례 등 거의 모든 함수가 추가되기 때문이다.\n\nR은 데이터를 위해 개발된 함수형 언어를 근본에 두고 있지만, 객체 지향 언어(OOP)나 다른 언어 패러다임과 싸우려고 하지 말고 적극적으로 받아들이라고 충고하고 있는데, 이것이 의미하는 바는 다음과 같다.\n\n상태 불변 객체: 작성된 코드에 대한 추론이 쉬워진다.\nS3, S4에서 제공하는 제네릭 함수: 상태 변형 가능한 상태가 필요하다면, 파이프 내부에서 구현한다.\nfor 루프를 추상화한 도구: apply 함수 가족과 purrr 맵(map) 함수\n\n데이터 사이언스에서 병목점으로 문제가 발생되는 곳은 공통적으로 컴퓨터 실행 시간(computing time)이 아니라 사람의 생각(thinking time)의 시간이다. 따라서, 함수명을 작성할 때 생각이 잘 연상되는 이름으로 작명하고 시간을 적절히 안분하고, 명시적이며 긴 명칭을 변수명, 함수명, 객체명에 사용하고, 짧은 명칭은 가장 중요한 이름으로 아껴서 사용한다. RStudio 소스 편집기의 자동 완성 기능을 사용하는 경우 접두어가 접미어보다 왜 중요한지 알 수 있고, stringr, xml2, rvest 패키지를 살펴보면 접두어에 일관된 명칭을 부여한 장점을 알 수 있다.",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "intro_tidyverse.html#footnotes",
    "href": "intro_tidyverse.html#footnotes",
    "title": "2  데이터 사이언스 운영체제",
    "section": "",
    "text": "Hadley Wickham (2017-11-13), “The tidy tools manifesto”↩︎",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "intro_whole_game.html",
    "href": "intro_whole_game.html",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "",
    "text": "3.1 나이팅게일 신화탄생",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>데이터 과학 맛보기</span>"
    ]
  },
  {
    "objectID": "intro_whole_game.html#나이팅게일-신화탄생",
    "href": "intro_whole_game.html#나이팅게일-신화탄생",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "",
    "text": "3.1.1 배경\n크림 전쟁은 1853년부터 1856년까지 일어난 큰 전쟁이었다. 한쪽에는 러시아, 반면 다른 한쪽에는 영국, 프랑스, 오스만 제국 (현대 투르키에), 그리고 나중에 사르디니아 (현대 이탈리아의 일부)가 동맹을 구성하여 전쟁을 치뤘다. 전쟁이 바로 시작된 이유는 러시아가 오스만 제국 내 정교회 신자들을 보호하려 하려는 명분을 내세웠지만, 사실 더많은 영토를 차지하기 위함이였다. 양측간 전쟁은 흑해를 두고 남하하는 러시아에 맞서 동맹군이 크림반도에서 발생하여 “크림전쟁”(Crimean War)으로 불린다. 영화로 소개된 경기병대의 돌격 (“Charge of the Light Brigade”), 영국 간호사 플로렌스 나이팅게일의 활약, 전신과 철도의 본격적인 도입으로 큰 의미를 갖는 전쟁이기도 하다. 많은 전투와 많은 사람들이 죽은 후, 1856년 파리 조약으로 전쟁은 마무리되어, 러시아 확장은 잠시 멈추게 돼었고, 오스만 제국도 한숨 돌린 계기가 되었다.\n크림 전쟁 중 스쿠타리 막사는 투루키에 스쿠타리 병원(Scutari Hospital, Turkey)은 영국 군 병원으로 개조되었다. 크림전쟁에서 부상을 당한 수많은 병사가 치료를 위해 이곳으로 보내졌지만, 병자와 부상병들을 감당할 수 있도록 설계되지 않았고 제대로된 역할도 수행하지 못했다. 1854년 나이팅게일이 간호사 일행과 함께 도착했을 때, 비위생적인 환경과 고통받는 병사들을 보고 경악했다. 나이팅게일의 스쿠타리 병원에서 경험은 병원과 의료 서비스를 개선하여 이와 같은 고통과 비극이 재발하지 않도록 향후 프로젝트의 중요한 동기와 방향이 되었다.\n\n\n스쿠타리 병원의 한 병동 석판화 그림 (William Simpson)\n\n환자의 사망율을 42%에서 2%로 낮추고 집중치료실(ICU)을 설치하여 상태가 중한 환자를 격리하여 집중관리하는 등 근대적인 간호체계를 수립하는 데 기여하였다.\n\n3.1.2 원본 데이터\n크림 전쟁 중 스쿠타리 막사는 투루키에 스쿠타리 병원에서 몇년간에 걸쳐 수작업으로 종이에 분석가능한 형태의 자료를 만들어내는 것은 결코 쉬운 작업이 아니다.\n\n\n원본 데이터\n\n\n3.1.3 그래프 진화\n출처: How Florence Nightingale Changed Data Visualization Forever - The celebrated nurse improved public health through her groundbreaking use of graphic storytelling\n복잡한 논거를 제시하는 대신 구체적인 주장에 데이터 시각화와 데이터 스토리텔링(Storytelling)을 통해 청중에 한걸음 더 다가섰다. 나이팅게일의 스토리텔링은 열악한 위생 상태와 과밀로 인해 불필요한 죽음이 얼마나 많이 발생하는지 이해하기 쉬운 비교를 통해 이야기를 구성해서 설득해 나갔다. 예를 들어, 군대 사망률을 민간인 사망률(유사한 환경의 맨체스터)과 비교하는 프레임을 제시하고, 군대 막사에서 생활하는 평시 병사들이 비슷한 연령대 민간인 남성보다 더 높은 비율로 사망하는 것을 제시했다. 이를 통해, 데이터가 보여주는 현실을 부정할 수 없게 만들었고, 군대 행정에 극적인 개혁을 이끌어냈다.\n\n\n\n\n\n\n\n\n\n(a) 막대그래프\n\n\n\n\n\n\n\n\n\n(b) 맨체스터 사망\n\n\n\n\n\n\n\n\n\n(c) 빅토리아 여왕 보고(I)\n\n\n\n\n\n\n\n\n\n(d) 빅토리아 여왕 보고(II)\n\n\n\n\n\n\n\n\n\n(e) 빅토리아 여왕 보고(III)\n\n\n\n\n\n\n그림 3.1: 나이팅게일 그래프 진화과정\n\n\n\n3.1.4 설득\n나이팅게일은 크림 전쟁 중 병원에서의 위생 문제와 관련된 데이터를 수집하고 분석하여 그 결과를 시각화했고, 병원에서의 사망 원인 중 대부분이 감염성 질병으로 인한 것을 발견했다. 이러한 감염성 질병은 부적절한 위생 조건과 밀접한 관련이 있음을 확인했다.\n나이팅게일은 병원의 위생 상태를 개선을 통해 수많은 생명을 구할 수 있다는 사실을 확인했고 연구결과와 권장 사항을 다양한 영국 정부부처에 제출했고, 특히 1858년에 영국의 장관들에게 보고서를 제출했다. 이를 통해서 군 병원의 위생 조건을 개선하는 데 큰 영향을 미쳤다.\n\n\n나이팅게일과 빅토리아 여왕\n\n\n3.1.5 성과와 영향\n나이팅게일 캠페인이 민간 공중보건에 미친 가장 큰 영향은 실현되기까지 오랜 기간에 걸쳐 다각도로 검토되었고, 마침내 1875년 영국 공중보건법(British Public Health Act)에 법제화되었다. 이 법에는 잘 정비된 하수도, 깨끗한 수돗물, 건축법 규제 등의 요건이 담겨있다. 질병에 대한 면역력을 강화하는 백신과 농작물 수확량을 획기적으로 늘리는 인공비료 개발과 함께 이 제도적인 노력으로 평균 수명을 두 배로 늘리는 원동력이 되었다.",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>데이터 과학 맛보기</span>"
    ]
  },
  {
    "objectID": "intro_whole_game.html#작업과정",
    "href": "intro_whole_game.html#작업과정",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.2 작업과정",
    "text": "3.2 작업과정\n\n3.2.1 디지털 데이터\nrladies/spain_nightingale GitHub 저장소에서 엑셀 형태로 된 데이터를 가져와서 전처리할 수 있다.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndeath_raw &lt;- read_excel(\"data/datos_florence.xlsx\", sheet = \"Sheet1\", skip = 1)\n\ndeath_tbl &lt;- death_raw |&gt; \n  janitor::clean_names() |&gt; \n  set_names(c(\"Month\", \"Army\", \"Disease\", \"Wounds\", \"Other\", \"Disease.rate\", \"Wounds.rate\", \"Other.rate\")) |&gt; \n  mutate(Date = lubridate::my(Month)) |&gt; \n  separate(Month, into = c(\"Month\", \"Year\"), sep = \" |_\") |&gt; \n  select(Date, Month, Year, everything()) \n\ndeath_tbl\n#&gt; # A tibble: 24 × 10\n#&gt;    Date       Month Year   Army Disease Wounds Other Disease.rate Wounds.rate\n#&gt;    &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 1854-04-01 Apr   1854   8571       1      0     5          1.4         0  \n#&gt;  2 1854-05-01 May   1854  23333      12      0     9          6.2         0  \n#&gt;  3 1854-06-01 Jun   1854  28333      11      0     6          4.7         0  \n#&gt;  4 1854-07-01 Jul   1854  28722     359      0    23        150           0  \n#&gt;  5 1854-08-01 Aug   1854  30246     828      1    30        328.          0.4\n#&gt;  6 1854-09-01 Sep   1854  30290     788     81    70        312.         32.1\n#&gt;  7 1854-10-01 Oct   1854  30643     503    132   128        197          51.7\n#&gt;  8 1854-11-01 Nov   1854  29736     844    287   106        341.        116. \n#&gt;  9 1854-12-01 Dec   1854  32779    1725    114   131        632.         41.7\n#&gt; 10 1855-01-01 Jan   1855  32393    2761     83   324       1023.         30.7\n#&gt; # ℹ 14 more rows\n#&gt; # ℹ 1 more variable: Other.rate &lt;dbl&gt;\n\nHistDate 패키지에 동일한 데이터셋이 잘 정제되어 있어 이를 바로 활용해도 좋다.\n\nlibrary(HistData)\n\nHistData::Nightingale |&gt; \n  as_tibble()\n#&gt; # A tibble: 24 × 10\n#&gt;    Date       Month  Year  Army Disease Wounds Other Disease.rate Wounds.rate\n#&gt;    &lt;date&gt;     &lt;ord&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;  &lt;int&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 1854-04-01 Apr    1854  8571       1      0     5          1.4         0  \n#&gt;  2 1854-05-01 May    1854 23333      12      0     9          6.2         0  \n#&gt;  3 1854-06-01 Jun    1854 28333      11      0     6          4.7         0  \n#&gt;  4 1854-07-01 Jul    1854 28722     359      0    23        150           0  \n#&gt;  5 1854-08-01 Aug    1854 30246     828      1    30        328.          0.4\n#&gt;  6 1854-09-01 Sep    1854 30290     788     81    70        312.         32.1\n#&gt;  7 1854-10-01 Oct    1854 30643     503    132   128        197          51.7\n#&gt;  8 1854-11-01 Nov    1854 29736     844    287   106        341.        116. \n#&gt;  9 1854-12-01 Dec    1854 32779    1725    114   131        632.         41.7\n#&gt; 10 1855-01-01 Jan    1855 32393    2761     83   324       1023.         30.7\n#&gt; # ℹ 14 more rows\n#&gt; # ℹ 1 more variable: Other.rate &lt;dbl&gt;\n\n\n3.2.2 데이터와 사투\n앞서 준비한 death_tbl 데이터프레임에서 사망 관련 데이터를 처리하고 시각화하기 위한 전처리를 수행하여 시각화를 위한 준비작업을 수행한다. 먼저 Date, Disease.rate, Wounds.rate, Other.rate 칼럼을 선택하고, pivot_longer 함수를 사용해 시각화에 적합한 데이터로 재구조화한다. str_replace_all 함수를 사용하여 칼럼 이름에서 “.rate”를 제거하고, ifelse 함수를 이용해 날짜를 기준으로 나이팅게일 팀이 준비한 방식을 적용하기 전과후 “이전”과 “이후”로 체제로 구분한다. factor 함수를 사용하여 범주 순서를 정의하고, 마지막으로 month 함수를 이용해 날짜에서 해당 월을 추출하고 death_viz에 저장한다.\n\ndeath_viz &lt;- death_tbl %&gt;% \n  select(Date, Disease.rate, Wounds.rate, Other.rate) %&gt;% \n  pivot_longer(-Date, names_to = \"사망원인\", values_to = \"사망자수\") |&gt; \n  mutate(사망원인 = str_replace_all(사망원인, \"\\\\.rate\", \"\"), \n         체제 = ifelse(Date &lt;= as.Date(\"1855-03-01\"), \"조치이전\", \"조치이후\")) %&gt;% \n  mutate(체제 = factor(체제, levels = c(\"조치이전\", \"조치이후\"))) %&gt;%  \n  mutate(해당월 = month(Date, label = TRUE, abbr = TRUE)) |&gt; \n  mutate(사망원인 = case_when(사망원인 == \"Disease\" ~ \"질병\",\n                              사망원인 == \"Wounds\" ~ \"부상\",\n                              사망원인 == \"Other\" ~ \"기타\")) |&gt; \n  mutate(사망원인 = factor(사망원인, levels = c(\"질병\", \"부상\", \"기타\")))\n\ndeath_viz\n#&gt; # A tibble: 72 × 5\n#&gt;    Date       사망원인 사망자수 체제     해당월\n#&gt;    &lt;date&gt;     &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;    &lt;ord&gt; \n#&gt;  1 1854-04-01 질병          1.4 조치이전 4     \n#&gt;  2 1854-04-01 부상          0   조치이전 4     \n#&gt;  3 1854-04-01 기타          7   조치이전 4     \n#&gt;  4 1854-05-01 질병          6.2 조치이전 5     \n#&gt;  5 1854-05-01 부상          0   조치이전 5     \n#&gt;  6 1854-05-01 기타          4.6 조치이전 5     \n#&gt;  7 1854-06-01 질병          4.7 조치이전 6     \n#&gt;  8 1854-06-01 부상          0   조치이전 6     \n#&gt;  9 1854-06-01 기타          2.5 조치이전 6     \n#&gt; 10 1854-07-01 질병        150   조치이전 7     \n#&gt; # ℹ 62 more rows",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>데이터 과학 맛보기</span>"
    ]
  },
  {
    "objectID": "intro_whole_game.html#시각화",
    "href": "intro_whole_game.html#시각화",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.3 시각화",
    "text": "3.3 시각화\n‘ggplot2’ 패키지를 이용하여 크림전쟁 나이팅게일 활약상을 담은 데이터를 시각화한다. 나이팅게일 활약 전과 후로 데이터(death_viz)를 나눠 “크림전쟁 병사 사망원인”에 대한 극좌표계 시각화를 통해 이해하기 쉬운 설득력있는 시각화 결과물을 제시하고 있다. 추가적으로, ‘showtext’ 패키지로 구글 “Noto Serif KR” 글꼴을 선택적용하고, ‘hrbrthemes’ 라이브러리를 이용하여 뒷 배경 검정색을 사용하여 붉은색 질병으로 인한 사망자수 확연한 감소를 시각적으로 강조한다.\n\nlibrary(hrbrthemes) \nlibrary(showtext)\nshowtext.auto()\nfont_add_google(name = \"Noto Serif KR\", family = \"noto_serif\")\nnoto_font &lt;- \"noto_serif\"\n\ndeath_gg &lt;- death_viz %&gt;% \n  ggplot(aes(x = 해당월, y = 사망자수, fill = 사망원인)) +\n  geom_col(color = \"grey20\") + \n  theme_modern_rc(base_family = noto_font, subtitle_family = noto_font) + \n  scale_fill_manual(values = c(\"firebrick\", \"orange\", \"#365181\"), name = \"\") +\n  scale_y_sqrt() +\n  facet_wrap(~ 체제) + \n  coord_equal(ratio = 1) +  \n  coord_polar() +\n  labs(title = \"크림전쟁 병사 사망원인\", \n       subtitle = \"데이터 시각화와 커뮤니케이션\", \n       caption = \"데이터 출처: 크림전쟁 사망자\") + \n  theme(legend.position = \"top\", \n        text = element_text(family = noto_font, size = 18),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        plot.margin = unit(rep(0.7, 4), \"cm\"),\n        plot.title = element_text(color = \"white\", family = noto_font, size = 25),\n        plot.caption = element_text(color = \"grey70\", family = noto_font, size = 12),\n        plot.subtitle = element_text(color = \"grey70\", size = 13),\n        legend.text = element_text(color = \"white\", size = 15),\n        strip.text = element_text(color = \"white\", size = 25, face = \"bold\", family = noto_font, hjust = 0.5))\n\ndeath_gg\n\n\n\n\n\n\n\n\n3.3.1 선그래프\n나이팅게일은 간호 분야의 선구자로 잘 알려져 있지만, 통계학자로서 “콕스콤(CoxComb)” 또는 “장미 다이어그램”(Rose Diagram)으로 알려진 원그래프를 제시하였지만 현재는 시간의 흐름에 따라 병사 사망자수 변화를 조치 전후로 명확히 하는 방법으로 선그래프가 기본 기법으로 자리잡고 있다.\n\nextrafont::loadfonts()\n\ndeath_new_gg &lt;- death_viz |&gt; \n  ggplot(aes(x = Date, y = 사망자수, color = 사망원인)) +\n    geom_line() +\n    geom_point() +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 18),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 35),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 17),\n          plot.subtitle = element_text(color = \"grey5\", size = 13),\n          legend.text = element_text(color = \"black\", size = 15)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 8.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 8.5, color = \"gray15\", family = noto_font)          \n\ndeath_new_gg\n\n\n\n\n\n\n\n\n3.3.2 막대그래프\n동일한 정보를 막대그래프를 통해 시각화를 할 수도 있다. 원그래프와 비교하여 보면 명확하게 사망자수를 직관적으로 비교할 수 있다는 점에서 큰 장점이 있다.\n\ndeath_viz |&gt; \n  ggplot() +\n    geom_col(aes(x = Date, y = 사망자수, fill = 사망원인), colour=\"white\") +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    scale_fill_manual(values = c(\"firebrick\", \"orange\", \"#365181\")) + \n    # theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 18),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 35),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 17),\n          plot.subtitle = element_text(color = \"grey5\", size = 13),\n          legend.text = element_text(color = \"black\", size = 15)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 8.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 8.5, color = \"gray15\", family = noto_font)",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>데이터 과학 맛보기</span>"
    ]
  },
  {
    "objectID": "intro_whole_game.html#표-문법",
    "href": "intro_whole_game.html#표-문법",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.4 표 문법",
    "text": "3.4 표 문법\n데이터 문법, 그래프 문법에 이어 최근 “표 문법”이 새롭게 자리를 잡아가고 있다. 표 문법에 맞춰 나이팅게일 크림전쟁 사망자수를 조치 이전과 조치 이후로 나눠 요약하면 확연한 차이를 파악할 수 있다.\ngt와 gtExtras 패키지를 활용하여 death_viz 데이터프레임을 사망 원인별 사망자 수를 “조치 이전”과 “조치 이후”로 구분하여 표를 두개 생성한다. 각 표은 날짜, 질병, 부상, 기타 범주로 사망자 수와 그 합계를 표시하며, 총 사망자수가 250명을 초과하는 행에 대한 강조 색상을 입히고 나서 두 표를 나란히 배치하여 조치 전후 효과를 시각적으로 비교한다.\n\nlibrary(gt)\nlibrary(gtExtras)\n\nbefore_tbl &lt;- death_viz |&gt; \n  filter(체제 == \"조치이전\")\n\nafter_tbl &lt;- death_viz |&gt; \n  filter(체제 == \"조치이후\")\n\nbefore_gt &lt;- before_tbl |&gt; \n  pivot_wider(names_from = 사망원인, values_from = 사망자수) |&gt; \n  select(날짜 = Date, 질병, 부상, 기타) |&gt; \n  mutate(합계 = 질병 + 부상 + 기타) |&gt; \n  gt() |&gt; \n    gt_theme_538() |&gt; \n    cols_align(\"center\") |&gt; \n    fmt_integer( columns = 질병:합계) |&gt; \n    tab_spanner(label = \"조치 이전\", columns = c(질병, 부상, 기타)) |&gt; \n    data_color(\n      columns = c(질병, 부상, 기타, 합계),\n      rows = 합계 &gt; 250,      \n      method = \"numeric\",\n      palette = \"ggsci::red_material\")\n\nafter_gt &lt;- after_tbl |&gt; \n  pivot_wider(names_from = 사망원인, values_from = 사망자수) |&gt; \n  select(날짜 = Date, 질병, 부상, 기타) |&gt; \n  mutate(합계 = 질병 + 부상 + 기타) |&gt; \n  gt() |&gt; \n    gt_theme_538() |&gt; \n    cols_align(\"center\") |&gt; \n    fmt_integer( columns = 질병:합계) |&gt; \n  tab_spanner(label = \"조치 이후\", columns = c(질병, 부상, 기타)) |&gt; \n  data_color(\n    columns = c(질병, 부상, 기타, 합계),\n    rows = 합계 &gt; 250,      \n    method = \"numeric\",\n    palette = \"ggsci::red_material\")\n\ngtExtras::gt_two_column_layout(list(before_gt, after_gt))\n\n\n\n\n\n\n\n\n\n날짜\n조치 이전\n합계\n\n\n질병\n부상\n기타\n\n\n\n\n1854-04-01\n1\n0\n7\n8\n\n\n1854-05-01\n6\n0\n5\n11\n\n\n1854-06-01\n5\n0\n2\n7\n\n\n1854-07-01\n150\n0\n10\n160\n\n\n1854-08-01\n328\n0\n12\n341\n\n\n1854-09-01\n312\n32\n28\n372\n\n\n1854-10-01\n197\n52\n50\n299\n\n\n1854-11-01\n341\n116\n43\n499\n\n\n1854-12-01\n632\n42\n48\n721\n\n\n1855-01-01\n1,023\n31\n120\n1,174\n\n\n1855-02-01\n823\n16\n140\n979\n\n\n1855-03-01\n480\n13\n69\n562\n\n\n\n\n\n\n\n\n\n\n\n날짜\n조치 이후\n합계\n\n\n질병\n부상\n기타\n\n\n\n\n1855-04-01\n178\n18\n21\n217\n\n\n1855-05-01\n172\n17\n12\n201\n\n\n1855-06-01\n248\n64\n10\n322\n\n\n1855-07-01\n108\n38\n9\n154\n\n\n1855-08-01\n130\n44\n7\n181\n\n\n1855-09-01\n48\n69\n5\n122\n\n\n1855-10-01\n33\n14\n5\n51\n\n\n1855-11-01\n56\n10\n10\n77\n\n\n1855-12-01\n25\n5\n8\n38\n\n\n1856-01-01\n11\n0\n13\n25\n\n\n1856-02-01\n7\n0\n5\n12\n\n\n1856-03-01\n4\n0\n9\n13",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>데이터 과학 맛보기</span>"
    ]
  },
  {
    "objectID": "intro_whole_game.html#커뮤니케이션",
    "href": "intro_whole_game.html#커뮤니케이션",
    "title": "\n3  데이터 과학 맛보기\n",
    "section": "\n3.5 커뮤니케이션",
    "text": "3.5 커뮤니케이션\n데이터를 기반으로 뭔가 유용한 것을 창출한 후에 이를 알리기 위해 커뮤니케이션 단계를 거치게 된다. 가장 흔히 사용하는 방식은 엑셀, 워드, 파워포인트와 같은 MS 오피스 제품을 활용하는 방식이다. 과거 SAS, SPSS, 미니탭 등 외산 통계 팩키지로 데이터를 분석하고 유용한 모형 등을 찾아낸 후에 이를 커뮤니케이션하기 위해 MS 오피스 제품을 통해 커뮤니케이션을 하기도 했다. 하지만, 각각은 별개의 시스템으로 분리되어 있어 일일이 사람손이 가는 번거러움이 많았다. 이를 해결하기 하는 방법은 하나의 도구 혹은 언어로 모든 작업을 처리하는 것이다. [^meghan]\n[^meghan] : Meghan Hall (June 15, 2021), “Extending R Markdown”, RStudio: R in Sports Analytics,\n우선 엑셀은 tidyverse 로 대체가 되고, 워드는 R 마크다운을 거쳐 쿼토(Quarto), 파워포인트도 R 마크다운(xaringan 등)에서 진화한 reveal.js 기반 쿼토 슬라이드가 빠르게 자리를 잡아가고 있다.\n\n\n오피스 기반 커뮤니케이션 현재 상태점검\n\n데이터 과학을 커뮤니케이션하는 방식은 다양한 방식이 존재하지만 직장상사 뿐만 아니라 집단지성을 넘어 AI를 적극 도입하여 데이터 분석 역량을 고도화하는데 동료 개발자 및 협업하시는 분들과 커뮤니케이션 뿐만 아니라 불특정 다수를 대상으로 한 인터넷에 공개와 공유를 통해 새로운 관계를 맺어가는 것도 그 중요성을 더해가고 있다.\n\n동료 개발자나 협업하시는 분: .qmd 파일\n직장상사\n\nPDF 파일: quarto, pandoc\n\n파워포인트 슬라이스덱: reveal.js 기반 quarto\n\n대쉬보드: flexdashboard\n\n\n\n일반 공개\n\n웹사이트: distill을 지나 quarto\n\n블로그: blogdown을 지나 quarto\n\n책: bookdown을 지나 quarto\n\n\n\n\n\n\n\n\ntidyverse 작업흐름\n스쿠타리 병원의 한 병동 석판화 그림 (William Simpson)\n원본 데이터\n그림 3.1 (a): 막대그래프\n그림 3.1 (b): 맨체스터 사망\n그림 3.1 (c): 빅토리아 여왕 보고(I)\n그림 3.1 (d): 빅토리아 여왕 보고(II)\n그림 3.1 (e): 빅토리아 여왕 보고(III)\n나이팅게일과 빅토리아 여왕\n오피스 기반 커뮤니케이션 현재 상태점검",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>데이터 과학 맛보기</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html",
    "href": "intro_penguins.html",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "",
    "text": "5 펭귄 데이터셋",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html#펭귄-데이터-출현",
    "href": "intro_penguins.html#펭귄-데이터-출현",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "\n5.1 펭귄 데이터 출현",
    "text": "5.1 펭귄 데이터 출현\n미국에서 “George Floyd”가 경찰에 의해 살해되면서 촉발된 “Black Lives Matter” 운동은 아프리카계 미국인을 향한 폭력과 제도적 인종주의에 반대하는 사회운동이다. 한국에서도 소수 정당인 정의당에서 여당 의원 176명 중 누가?…차별금지법 발의할 ’의인’을 구합니다로 기사로 낼 정도로 적극적으로 나서고 있다.\n데이터 과학에서 최근 R.A. Fisher의 과거 저술한 “The genetical theory of natural selection” (fisher1958genetical?) 우생학(Eugenics) 대한 관점이 논란이 되면서 R 데이터 과학의 첫 데이터셋으로 붓꽃 iris 데이터를 다른 데이터, 즉 펭귄 데이터로 대체하는 움직임이 활발히 전개되고 있다. palmerpenguins (penguin2020?) 데이터셋이 대안으로 많은 호응을 얻고 있다. (Levy2019?)",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html#penguins-study",
    "href": "intro_penguins.html#penguins-study",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "\n5.2 펭귄 공부",
    "text": "5.2 펭귄 공부\n팔머(Palmer) 펭귄은 3종이 있으며 자세한 내용은 다음 나무위키를 참조한다. 1\n\n\n젠투 펭귄(Gentoo Penguin): 머리에 모자처럼 둘러져 있는 하얀 털 때문에 알아보기가 쉽다. 암컷이 회색이 뒤에, 흰색이 앞에 있다. 펭귄들 중에 가장 빠른 시속 36km의 수영 실력을 자랑하며, 짝짓기 할 준비가 된 펭귄은 75-90cm까지도 자란다.\n\n아델리 펭귄(Adelie Penguin): 프랑스 탐험가인 뒤몽 뒤르빌(Dumont D’Urville) 부인의 이름을 따서 ’아델리’라 불리게 되었다. 각진 머리와 작은 부리 때문에 알아보기 쉽고, 다른 펭귄들과 마찬가지로 암수가 비슷하게 생겼지만 암컷이 조금 더 작다.\n\n턱끈 펭귄(Chinstrap Penguin): 언뜻 보면 아델리 펭귄과 매우 비슷하지만, 몸집이 조금 더 작고, 목에서 머리 쪽으로 이어지는 검은 털이 눈에 띈다. 어린 고삐 펭귄들은 회갈색 빛을 띄는 털을 가지고 있으며, 목 아래 부분은 더 하얗다. 무리를 지어 살아가며 일부일처제를 지키기 때문에 짝짓기 이후에도 부부로써 오랫동안 함께 살아간다.\n\n\n\n팔머 펭귄 3종 세트\n\n다음으로 iris 데이터와 마찬가지로 펭귄 3종을 구분하기 위한 변수로 조류의 부리에 있는 중앙 세로선의 융기를 지칭하는 능선(culmen) 길이(culmen length)와 깊이(culmen depth)를 이해하면 된다.\n\n\n팔머 펭귄 능선 변수",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html#penguin-home",
    "href": "intro_penguins.html#penguin-home",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "\n5.3 펭귄 서식지",
    "text": "5.3 펭귄 서식지\nleaflet 팩키지로 펭귄 서식지를 남극에서 특정한다. geocoding을 해야 하는데 구글에서 위치 정보를 구글링하면 https://latitude.to/에서 직접 위경도를 반환하여 준다. 이 정보를 근거로 하여 펭귄 서식지를 시각화한다.\n\n\n\n\n\n\n\n\n파머 연구소와 펭귄 서식지\n\n\n\n\n\n펭귄 3종\n\n\n\n\n\n\n\n아델리, 젠투, 턱끈 펭귄이 함께한 사진\n\n\n\n\n\n토르거센 섬에서 새끼를 키우는 아델리 펭귄\n\n\n\n\n\n비스코 지점 젠투 펭귄 서식지\n\n\n\n\n\n펭귄과 함께 현장에서 일하는 크리스틴 고먼 박사\n\n\n\n\n\n파머 펭귄 데이터셋\n\n\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(palmerpenguins)\n# library(tidygeocoder)\n\npenguins %&gt;% \n  count(island)\n#&gt; # A tibble: 3 × 2\n#&gt;   island        n\n#&gt;   &lt;fct&gt;     &lt;int&gt;\n#&gt; 1 Biscoe      168\n#&gt; 2 Dream       124\n#&gt; 3 Torgersen    52\n\nisland_df &lt;- tribble(~\"address\", ~\"lat\", ~\"lng\",\n                     \"Torgersen Island antarctica\", -64.772819, -64.074325,\n                     \"Dream Island antarctica\", -64.725558, -64.225562,\n                     \"Biscoe Island antarctica\", -64.811565, -63.777947,\n                     \"Palmer Station\", -64.774312, -64.054213)\n\nisland_df %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  addMarkers(lng=~lng, lat=~lat, \n                   popup = ~ as.character(paste0(\"&lt;strong&gt;\", paste0(\"명칭:\",`address`), \"&lt;/strong&gt;&lt;br&gt;\",\n                                                 \"-----------------------------------------------------------&lt;br&gt;\",\n                                                 \"&middot; latitude: \", `lat`, \"&lt;br&gt;\",\n                                                 \"&middot; longitude: \", `lng`, \"&lt;br&gt;\"\n                   )))",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html#데이터-설치",
    "href": "intro_penguins.html#데이터-설치",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "\n5.4 데이터 설치",
    "text": "5.4 데이터 설치\nremotes 팩키지 install_github() 함수로 펭귄 데이터를 설치한다.\n\n# install.packages(\"remotes\")\nremotes::install_github(\"allisonhorst/palmerpenguins\")\n\ntidyverse 팩키지 glimpse() 함수로 펭귄 데이터를 일별한다.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n#&gt; $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html#penguin-EDA-skimr",
    "href": "intro_penguins.html#penguin-EDA-skimr",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "\n5.5 자료구조 일별",
    "text": "5.5 자료구조 일별\nskimr 팩키지를 사용해서 penguins 데이터프레임 자료구조를 일별한다. 이를 통해서 344개 펭귄 관측값이 있으며, 7개 칼럼으로 구성된 것을 확인할 수 있다. 또한, 범주형 변수가 3개, 숫자형 변수가 4개로 구성되어 있다. 그외 더 자세한 사항은 범주형, 숫자형 변수에 대한 요약 통계량을 참조한다.\n\nskimr::skim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n데이터가 크지 않아 DT 팩키지를 통해 데이터 전반적인 내용을 살펴볼 수 있다.\n\npenguins %&gt;% \n  reactable::reactable()",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html#penguin-EDA",
    "href": "intro_penguins.html#penguin-EDA",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "\n5.6 탐색적 데이터 분석",
    "text": "5.6 탐색적 데이터 분석\npalmerpenguins 데이터셋 소개에 포함되어 있는 미국 팔머 연구소 (palmer station) 펭귄 물갈퀴(flipper) 길이와 체질량(body mass) 산점도를 그려보자.\n\nlibrary(tidyverse)\nlibrary(extrafont)\nloadfonts()\n\nmass_flipper &lt;- ggplot(data = penguins, \n                       aes(x = flipper_length_mm,\n                           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 3,\n             alpha = 0.8) +\n  theme_minimal(base_family = \"NanumGothic\") +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(title = \"펭귄 크기\",\n       subtitle = \"남극 펭귄 3종 물갈퀴 길이와 체질량 관계\",\n       x = \"물갈퀴 길이 (mm)\",\n       y = \"체질량 (g)\",\n       color = \"펭귄 3종\",\n       shape = \"펭귄 3종\") +\n  theme(legend.position = c(0.2, 0.7),\n        legend.background = element_rect(fill = \"white\", color = NA),\n        plot.title.position = \"plot\",\n        plot.caption = element_text(hjust = 0, face= \"italic\"),\n        plot.caption.position = \"plot\")\n\nmass_flipper",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html#펭귄-데이터-저장",
    "href": "intro_penguins.html#펭귄-데이터-저장",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "\n5.7 펭귄 데이터 저장",
    "text": "5.7 펭귄 데이터 저장\n\n5.7.1 .csv 파일\n\nlibrary(palmerpenguins)\n\npenguins |&gt; \n  drop_na() |&gt; \n  write_csv(\"data/penguins.csv\")\n\n\n5.7.2 .xlsx 엑셀 파일\n\nlibrary(writexl)\n\npenguins |&gt; \n  drop_na() |&gt; \n  write_xlsx(\"data/penguins.xlsx\")\n\n\n5.7.3 .sqlite 데이터베이스\n\nlibrary(DBI)\nlibrary(RSQLite)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), dbname = \"data/penguins.sqlite\")\n\n# 데이터프레임을 SQLite 테이블로 저장합니다. 'my_table'이라는 이름으로 저장됩니다.\n# 데이터베이스에 같은 이름의 테이블이 이미 존재한다면, append, overwrite 또는 fail 중 하나를 선택할 수 있습니다.\ndbWriteTable(con, \"penguin\", penguins |&gt; drop_na() , overwrite = TRUE)\n\n# 연결을 닫습니다.\ndbDisconnect(con)\n\n\n5.7.4 pins\n\n\nlibrary(pins)\n\nboard &lt;- board_folder(\"C:/Users/statkclee/OneDrive/pins\") \n\nmetadata &lt;- list(owner       = \"한국 R 사용자회\",\n                 deptartment = \"R&D\",\n                 URL         = \"https://r2bit.com\")\n\nboard  |&gt; pin_write(penguins |&gt; drop_na(), \n                    name        = \"penguins\",\n                    title       = \"펭귄 데이터셋\",\n                    description = \"남극 파머 연구소 서식 펭귄 데이터셋\",\n                    metadata    = metadata)\n\n\n5.7.5 구글시트\n\nlibrary(googlesheets4)\nlibrary(googledrive)\n\ngoogledrive::drive_auth()\n\ngs4_create(\n  name = \"penguins\",\n  sheets = list(\"penguins\" = penguins |&gt; drop_na())\n)\n\n#&gt; ✔ Creating new Sheet: penguins.\n#&gt; Waiting for authentication in browser...\n#&gt; Press Esc/Ctrl + C to abort\n#&gt; Authentication complete.\n\n\n\n\n\n\n팔머 펭귄 3종 세트\n팔머 펭귄 능선 변수\n파머 연구소와 펭귄 서식지\n펭귄 3종\n아델리, 젠투, 턱끈 펭귄이 함께한 사진\n토르거센 섬에서 새끼를 키우는 아델리 펭귄\n비스코 지점 젠투 펭귄 서식지\n펭귄과 함께 현장에서 일하는 크리스틴 고먼 박사",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "intro_penguins.html#footnotes",
    "href": "intro_penguins.html#footnotes",
    "title": "\n4  펭귄 데이터셋\n",
    "section": "",
    "text": "신발끈 여행사, 관광안내자료↩︎",
    "crumbs": [
      "**1부** 들어가며",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>펭귄 데이터셋</span>"
    ]
  },
  {
    "objectID": "data_structure.html",
    "href": "data_structure.html",
    "title": "5  자료구조",
    "section": "",
    "text": "5.1 자료형\n2.1 더하기 \"black\"이 말이 되지 않기 때문에 마지막 문장이 오류를 뱉어낼 것이라고 추측한다면, 제대로 이해하고 있는 것이다. 이미 자료형(data type)으로 불리는 프로그래밍의 중요한 개념에 대한 직관을 갖추고 있는 것이다. 데이터 자료형이 무엇인지 다음을 통해 물어보게 된다:\n다섯 가지 주요 자료형이 있다. 실수형(double), 정수형(integer), 복소수형(complex), 논리형(logical), 문자형(character).\n분석이 얼마나 복잡해지냐에 관계없이, R에서 모든 데이터는 이러한 기본 자료형 중 하나로 해석된다. 이러한 엄격함이 정말로 중요한 결과를 잉태하게 된다.\n사용자가 또다른 고양이에 대한 상세내용을 추가했고, 추가 정보는 data/feline-data_v2.csv 파일에 저장되어 있다.\n# file.show(\"data/feline-data-v2.csv\")\nfile.show(\"feline-data-v2.csv\")\n앞서와 마찬가지로 새로운 고양이 데이터를 불러와서 weight 칼럼에 데이터 자료형이 무엇인지 확인한다.\n이러면 안되는데, 고양이 몸무게가 더이상 실수형 자료형이 아니다! 앞서 수행했던 동일한 수학연산을 취하게 되면 문제에 봉착한다.\n무슨 일이 일어난 걸까? R이 csv 파일을 불러올 때, 칼럼에 모든 것이 동일한 자료형이 되는 것을 요구한다; 칼럼에 모든 원소가 실수형으로 인식되지 않으면, 칼럼에 어떤 원소도 실수형이 될 수 없다. 고양이 데이터를 불러온 테이블을 데이터프레임(data.frame)이라고 부르고, 자료구조(data structure)로 불리는 첫번째 사례가 된다. 즉, 자료구조는 기본 자료형에서 R이 생성할 줄 아는 구조가 된다.\nclass 함수를 호출해서 데이터프레임인지를 알 수 있다.\nR에서 데이터를 성공적으로 사용하려면 기본 자료구조가 무엇이고 어떻게 동작하는지 이해할 필요가 있다. 지금으로서는 추가된 마지막 줄을 제거하고 좀더 살펴보도록 하자:\nfeline-data.csv:\nRStudio로 다시 돌아와서,",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#자료형",
    "href": "data_structure.html#자료형",
    "title": "5  자료구조",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\ncoat,weight,likes_string\ncalico,2.1,1\nblack,5.0,0\ntabby,3.2,1\ntabby,2.3 or 2.4,1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\ncoat,weight,likes_string\ncalico,2.1,1\nblack,5.0,0\ntabby,3.2,1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#벡터와-자료형-강제변환",
    "href": "data_structure.html#벡터와-자료형-강제변환",
    "title": "5  자료구조",
    "section": "\n5.2 벡터와 자료형 강제변환",
    "text": "5.2 벡터와 자료형 강제변환\n강제변환을 보다 잘 이해하기 위해서, 또 다른 자료구조(벡터(vector))를 만나보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR에서 벡터는 본질적으로 무언가 순서를 갖는 리스트로, 특별한 성질을 갖는데 벡터에 모든 것은 동일한 자료형을 갖는다는 점이다. 자료형을 지정하지 않게 되면, 기본디폴트 설정은 논리형(logical)이 되거나; 자료형에 관계없이 공벡터를 선언할 수 있다. 자료가 벡터인지 다음과 같이 확인한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n상기 명령어로부터 다소 암호스러운 출력결과가 나오는데 해당 벡터에서 발견된 기본 자료형은 이 경우 chr 문자; 벡터에 나와있는 숫자는 벡터의 인덱스로 이 경우 [1:3]; 그리고 벡터에 실제로 들어있는 몇가지 예로, 이 경우 빈 문자열이 된다. 유사하게 다음을 실행하게 되면,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncats$weight도 벡터임을 알 수 있다. R 데이터프레임으로 불러온 데이터 칼럼은 모두 벡터다. 이러한 연유로 칼럼에 있는 모든 원소는 동일한 자료형을 갖게 강제하는 이유가 된다.\n\n\n\n\n\n\n토론 주제\n\n\n\n왜 R에서는 데이터 칼럼에 무엇을 넣는지에 대해서 고집스럽게 주장을 할까? 이러한 점은 어떻게 우리에게 도움이 될까?\n\n\n\n\n\n\n토론 정리\n\n\n\n\n\n칼럼에 모든 것을 동일하게 둠으로써, 데이터에 관해서 단순한 가정을 할 수 있게 한다; 만약 칼럼의 첫번째 입력값이 숫자라면, 모든 입력값을 숫자로 해석할 수 있게 되고, 그렇게 함으로써 모든 것을 확인할 필요가 없게 된다. 깨끗한 데이터(clean data)라고 사람들이 회자할 때, 사람들이 의미하는 것이 이러한 일관성이다. 장기적으로 엄격한 일관성이 R에서 우리 삶을 풍요롭고 수월하게 만들 것이다.\n\n\n\n\n\n결합 함수(c())를 사용해서 벡터를 생성할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n지금까지 학습한 것을 바탕으로, 다음 문장은 어떤 결과를 출력하게 될까?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이것을 자료형 강제변환(type coercion)라고 부른다. 이것이 많은 놀라움의 원천이고, 왜 기본 자료형에 대해서 인지하고 있어야 되는 이유가 되고, R이 해석하는 방식도 알아야 된다. R에서 혼합된 자료형(상기 예제는 숫자와 문자)의 경우 단하나의 벡터로 변환시킬 때, 모든 자료를 동일한 자료형으로 강제 변환시킨다. 다음을 생각해 보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n강제 변환규칙은 다음과 같이 적용된다: 논리형 -&gt; 정수형 -&gt; 숫자형 -&gt; 복소수형 -&gt; 문자형. 여기서 -&gt; 표현은 다음으로 변환된다로 읽힌다. 이런 자동 변환규칙에 거슬러 자료형을 강제로 변환시키려면 as. 함수를 사용한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR이 기본 자료형을 다른 자료형으로 강제 변환할 때, 놀라운 일이 생겨난다! 자료형 강제변환에 대한 핵심사항은 차치하고 중요한 점은 다음과 같다. 만약 내 데이터가 생각했던 것과 다르게 보인다면, 자료형 강제변환이 원인일 가능성이 높다. 벡터와 데이터프레임의 칼럼 자료형이 동일하도록 확실히 하라. 그렇지 않으면 끔찍한 놀라운 경험을 하게 될 것이다!\n하지만, 경우에 따라서는 자료형 강제변환이 매우 유용할 수도 있다! 예를 들어, cats 데이터프레임 likes_string 칼럼은 숫자형이지만, 1과, 0이 실제로 TRUE와 FALSE를 표현한다는 것을 알고 있다. 이 경우 두 상태(TRUE 혹은 FALSE)를 갖는 논리형 자료형을 사용해야 한다. as.logical 함수를 사용해서 칼럼을 논리형(logical)으로 ‘강제변환(coerce)’ 시킨다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n결합 함수(c())는 기존 벡터에 무언가 추가하는 역할을 수행한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n숫자 순열도 생성할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n벡터에 관해 궁금한 점도 물어볼 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n마지막으로, 벡터의 각 원소에 명칭을 부여하는 것도 가능하다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n1 부터 26까지 숫자를 갖는 벡터를 생성하면서 시작해 보자. 생성한 벡터에 2를 곱해서 다시 자신에게 할당한다. 벡터에 A 부터 Z까지 이름을 부여한다. (힌트: LETTERS라는 내장벡터가 있다.)\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#데이터프레임",
    "href": "data_structure.html#데이터프레임",
    "title": "5  자료구조",
    "section": "\n5.3 데이터프레임",
    "text": "5.3 데이터프레임\n데이터프레임의 칼럼이 벡터라고 앞에서 언급했다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n말이 된다. 다음은 어떤가?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#요인",
    "href": "data_structure.html#요인",
    "title": "5  자료구조",
    "section": "\n5.4 요인",
    "text": "5.4 요인\n또 다른 중요한 자료구조가 요인(factor)이다. 요인은 보통 문자 데이터처럼 생겼다. 하지만, 일반적으로 범주형 정보를 나타내는데 사용된다. 예를 들어, 연구중인 모든 고양이에 대한 색상을 문자열 벡터로 만들어보자:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n문자열 벡터를 요인형으로 바꾸면 다음과 같다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이제 R은 데이터에 3가지 가능한 범주가 있음을 파악하게 되었다. 하지만, 놀라운 것도 함께 수행했다; 문자열을 출력하는 대신에, 숫자가 대량으로 출도 되었다. R은 내부적으로 사람이 읽을 수 있는 범주를 숫자 인덱스로 치환시킨다. 이런 기능은 대다수 통계 계산에서 범주형 데이터를 숫자형으로 표현되는 기능을 활용하기 때문에 꼭 필요하다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\ncats 데이터프레임에 요인형 칼럼이 있나요? 요인형 칼럼의 이름은 무엇인가요? ?read.csv 명령어를 사용해서 텍스트 칼럼을 요인형 대신에 문자형으로 그대로 유지시키는 방법을 찾아내세요; 그리고 나서 cat 데이터프레임의 요인이 실제로 문자벡터임을 확인하는 명령문을 작성하시오.\n\n\n\n\n\n\n해답\n\n\n\n\n\n해법으로 stringAsFactors 인자를 사용하면 된다.:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n또다른 해법은 colClasses 인자를 사용해서 칼럼을 좀더 면밀히 제어하는 것이다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n주의: 도움말 파일이 이해하기 어렵다는 학생이 다수 있다; 도움말 파일을 이해하기 어렵다는 것이 일반적이라서, 확신하지는 못하더라도, 문맥에 기초하여 최대한 추측하도록 용기를 주도록 한다.\n\n\n\n\n\n모형 함수에서 기준 수준(baseline level)이 무엇인지 파악하는 것이 중요하다. 요인의 첫번째 범주로 가정하지만, 기본디폴트는 알파벳순으로 정해지게 되어 있다. 수준을 다음과 같이 지정해서 변경할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n상기 경우 “control”를 1로, “case”를 2로 명시적으로 지정하도록 했다. 이러한 지정이 통계 모형 결과를 해석하는데 있어 매우 중요하다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#리스트",
    "href": "data_structure.html#리스트",
    "title": "5  자료구조",
    "section": "\n5.5 리스트",
    "text": "5.5 리스트\n데이터 과학자로서 알고 있어야 되는 또다른 자료구조가 리스트(list)다. 리스트는 다른 자료형과 비교하여 몇 가지 점에서 더 단순하다. 왜냐하면 원하는 무엇이든 넣을 수 있기 때문이다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임에서 다소 놀라운 것을 이제 이해할 수 있다. 다음을 실행하게 되면 무슨 일이 발생될까?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임은 ‘내부적으로(under the hood)’ 리스트라는 것을 알 수 있다 - 이유는 데이터프레임이 실제로 벡터와 요인으로 구성된 리스트이기 때문이다. 벡터와 요인으로 뒤섞인 칼럼을 붙잡아 두려면, 데이터프레임은 모든 칼럼을 유사한 표에 담을 수 있는 벡터보다 더 유연할 필요가 있다. 다른 말로, data.frame은 모든 벡터가 동일한 길이를 갖는 특별한 리스트로 정의할 수 있다.\ncats 사례에서는 정수형, 숫자형, 논리형 변수로 구성된다. 이미 살펴봤듯이, 데이터프레임 각 칼럼은 벡터다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n각 행은 다른 변수의 관측점(observation)으로 그 자체로 데이터프레임이다. 따라서, 서로 다른 자료형을 갖는 원소로 구성된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n데이터프레임에서 변수와 관측점과 원소를 호출하는 미모하지만 다른 방식이 존재한다: - cats[1] - cats[[1]] - cats$coat - cats[\"coat\"] - cats[1, 1] - cats[, 1] - cats[1, ]\n상기 예제를 시도해보고, 각각이 반환하는 것을 설명해 본다. 힌트: typeof() 함수를 사용해서 각각의 경우에 반환되는 것을 꼼꼼히 살펴본다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임을 벡터로 구성된 리스트로 간주할 수 있다. 단일 꺾쇠 [1]은 리스트의 첫번째 원소를 리스트로 반환한다. 이번 경우, 데이터프레임의 첫번째 칼럼이 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이중 꺾쇠 [[1]]은 리스트의 원소 내용물을 반환한다. 이번 경우, 리스트가 아닌 요인형 벡터로 첫번째 칼럼 내용물을 반환한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n명칭으로 항목을 끄집어내는데 $ 기호를 사용한다. _coat_가 데이터프레임의 첫번째 칼럼으로 요인형 벡터가 반환된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n[\"coat\"] 방식은 칼럼 인덱스를 명칭으로 바꾸고 동시에 꺾쇠를 사용한 경우다. 예제 1과 마찬가지로 반환되는 객체는 리스트가 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n단일 꺾쇠를 사용했는데 이번에는 행과 열의 좌표도 넣어 전달했다. 반환되는 객체는 첫번째 행, 첫번째 열에 교차하는 값이 된다. 반환되는 객체는 정수형이지만,요인형 벡터의 일부분이라 정수값과 연관된 라벨 “calico”도 함께 출력한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n앞선 예제와 마찬가지로 꺾쇠를 하나만 사용했고, 행과 열 좌표도 전달했다. 행좌표를 지정하지 않은 경우, R에서 결측값은 해당 칼럼 벡터의 모든 원소로 해석된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다시 한번, 꺽쇠를 하나만 사용했고, 행과 열 좌표도 전달했다. 칼럼 좌표가 지정되어 있지 않기 때문에, 첫번째 행의 모든 값을 포함하는 리스트가 반환된다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "data_structure.html#행렬",
    "href": "data_structure.html#행렬",
    "title": "5  자료구조",
    "section": "\n5.6 행렬",
    "text": "5.6 행렬\n마지막으로 중요한 자료형이 행렬이다. 0으로 가득찬 행렬을 다음과 같이 선언한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다른 자료구조와 마찬가지로, 행렬에 질문을 다음과 같이 던질 수 있다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\nlength(matrix_example) 실행결과는 어떻게 나올까? 시도해보자. 생각한 것과 일치하는가? 왜 그런가/ 왜 그렇지 않는가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n행렬은 차원 속성이 추가된 벡터라서, length() 함수는 행렬의 전체 원소 갯수를 반환시킨다.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n또다른 행렬을 만들어 보자. 이번에는 1:50 숫자를 담고 있는 칼럼이 5, 행이 10일 행렬이다. matrix() 함수로 칼럼기준으로 혹은 행기준으로 채울 수 있나요? 행과 열을 바꿔 변경할 수 있는 방법을 찾아보자. (힌트: matrix 도움말 문서를 참조한다!)\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n이번 워크샵에서 다룬 각 섹션별 문자벡터를 포함하는 길이 2를 갖는 리스트를 생성하시오.\n\n자료형\n자료구조\n\n지금까지 살펴본 자료형과 자료구조를 명칭으로 갖는 문자 벡터를 채워넣는다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n주목: 칠판이나 벽에 자료형과 자료구조를 모두 적어두는 것이 도움이 될 수 있다 - 워크샵 동안 참여자들에게 기본 자료형과 구조의 중요성을 상기할 수 있기 때문이다.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n아래 행렬의 출력 결과를 생각해보자:\n     [,1] [,2]\n[1,]    4    1\n[2,]    9    5\n[3,]   10    7\n이 행렬을 작성하는 올바른 명령어는 다음 중 무엇일까? 직접 타이핑하기 전에 각 명령어를 살펴보고, 정답을 생각해보자. 다른 명령어는 어떤 행렬을 만들어낼지도 생각해본다.\n\nmatrix(c(4, 1, 9, 5, 10, 7), nrow = 3)\nmatrix(c(4, 9, 10, 1, 5, 7), ncol = 2, byrow = TRUE)\nmatrix(c(4, 9, 10, 1, 5, 7), nrow = 2)\nmatrix(c(4, 1, 9, 5, 10, 7), ncol = 2, byrow = TRUE)\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>자료구조</span>"
    ]
  },
  {
    "objectID": "dataframe.html",
    "href": "dataframe.html",
    "title": "6  데이터프레임 탐색",
    "section": "",
    "text": "6.1 행과 열 추가\n데이터프레임의 칼럼은 벡터라는 것을 배웠다. 따라서, 데이터는 칼럼에서 자료형의 일관성을 유지해야 한다. 이를테면, 칼럼을 새로 추가하려면 벡터를 새로 만들어서 시작한다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#행과-열-추가",
    "href": "dataframe.html#행과-열-추가",
    "title": "6  데이터프레임 탐색",
    "section": "",
    "text": "R\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nage를 칼럼으로 다음과 같이 추가한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임의 행의 개수와 다른 개수를 갖는 age 벡터를 추가하게 되면 추가되지 않고 오류가 발생된다는 점에 주의한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n정상 동작이 되지 않는 이유가 무엇일까? R은 테이블의 모든 행마다 신규 칼럼에서도 원소 하나가 있길 원한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n그래서, 정상 동작하려면 nrow(cats) = length(age)이 되어야 한다. cats 내용을 새로운 데이터프레임으로 덮어써보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이제 행을 추가하면 어떻게 될까? 이미 데이터프레임의 행이 리스트라는 사실을 알고 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#요인",
    "href": "dataframe.html#요인",
    "title": "6  데이터프레임 탐색",
    "section": "\n6.2 요인",
    "text": "6.2 요인\n살펴볼 것이 하나 더 있다. 범주형 자료 처리를 위한 요인(factor)에서 각기 다른 값을 범주 수준(level)이라고 한다. 요인형 “coat” 변수는 수준이 3으로 구성된다. “black”, “calico”, “tabby”. R은 세 가지 수준 중 하나와 매칭되는 값만 받아들인다. 완전 새로운 값을 추가하게 되면, 추가되는 신규 값은 NA가 된다.\n경고 메시지를 통해서 coat 요인변수에 “tortoiseshell” 값을 추가하는데 성공하지 못했다고 알려준다. 하지만, 3.3 (숫자형), TRUE (논리형), 9 (숫자형)은 모두 weight, likes_string, age 변수에 성공적으로 추가된다. 왜냐하면 이 변수들이 요인형이 아니기 때문이다. “tortoiseshell”을 coat 요인변수에 성공적으로 추가하려면 요인의 수준(level)로 “tortoiseshell”을 추가하면 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n대안으로, 요인형 벡터를 문자형 벡터로 변환시키면 된다. 요인변수의 범주를 잃게 되지만, 요인 수준을 조심스럽게 다룰 필요 없이 칼럼에 추가하고자 하는 임의의 단어를 추가할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n\n\ncats$age 벡터에 7을 곱해서 human_age 벡터를 생성하자.\n\nhuman_age를 요인형으로 변환시키자.\n\nas.numeric() 함수를 사용해서 human_age 벡터를 다시 숫자형 벡터로 변환시킨다. 이제 7로 나눠서 원래 고양이 나이로 되돌리자. 무슨 일이 생겼는지 설명하자.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nhuman_age &lt;- cats$age * 7\n\nhuman_age &lt;- factor(human_age). as.factor(human_age) 도 잘 동작한다.\n\nas.numeric(human_age)을 실행하면 1 2 3 4 4이 된다.\n\n왜냐하면 요인형 변수는 정수형(여기서 1:4)으로 자료를 저장하기 때문이다. 정수 라벨과 연관된 값은 여기서 28, 35, 56, 63이다. 요인형 변수를 숫자형 벡터로 변환시키면 라벨이 아니라 그 밑단의 정수를 반환시킨다. 원래 숫자를 원하는 경우, human_age를 문자형 벡터로 변환시키고 나서 숫자형 벡터로 변환시키면 된다. (왜 이 방식은 정상 동작할까?) 실수로 숫자만 담긴 칼럼 어딘가에 문자가 포함된 csv 파일로 작업할 때 이런 일이 실제로 종종 일어난다. 데이터를 불러 읽어올 때 stringsAsFactors=FALSE 설정을 잊지 말자.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#행-제거",
    "href": "dataframe.html#행-제거",
    "title": "6  데이터프레임 탐색",
    "section": "\n6.3 행 제거",
    "text": "6.3 행 제거\n이제 데이터프레임에 행과 열을 추가하는 방법을 알게 되었다. 하지만, 데이터프레임에 “tortoiseshell” 고양이를 처음으로 추가하면서 우연히 쓰레기 행을 추가시켰다. 데이터프레임에 문제가 되는 행을 마이너스 연산자를 사용해서 빼자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n-4, 다음에 아무것도 지정하지 않아서 4번째 행 전체를 제거함에 주목한다. 주목: 벡터 내부에 행 다수를 넣어 한번에 행을 제거할 수도 있다: cats[c(-4,-5), ] 대안으로, NA 값을 갖는 모든 행을 제거시킨다. 출력결과를 cats에 다시 대입하여 변경사항이 데이터프레임에 영구히 남도록 조치한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#칼럼-제거",
    "href": "dataframe.html#칼럼-제거",
    "title": "6  데이터프레임 탐색",
    "section": "\n6.4 칼럼 제거",
    "text": "6.4 칼럼 제거\n데이터프레임의 칼럼도 제거할 수 있다. “age” 칼럼을 제거하고자 한다면 어떻게 해야 할까? 변수명과 변수 인덱스, 두가지 방식으로 칼럼을 제거할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n,-4 앞에 아무것도 없는 것에 주목한다. 이는 모든 행을 간직한다는 의미를 갖는다. 대안으로, 색인명을 사용해서 컬럼을 제거할 수도 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#덧붙이기",
    "href": "dataframe.html#덧붙이기",
    "title": "6  데이터프레임 탐색",
    "section": "\n6.5 덧붙이기",
    "text": "6.5 덧붙이기\n데이터프레임에 데이터를 추가할 때 기억할 것은 칼럼은 벡터, 행은 리스트라는 사실이다. rbind() 함수를 사용해서 데이터프레임 두 개를 본드로 붙이듯이 결합시킬 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n하지만 이제 행 이름이 불필요하게 복잡해졌다. 행 이름을 제거하면 R이 자동으로 순차적인 이름을 다시 붙여준다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n다음 구문을 사용해서 R 내부에서 직접 데이터프레임을 새로 만들 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다음 정보를 갖는 데이터프레임을 직접 제작해 보자.\n\n이름(first name)\n성(last name)\n좋아하는 숫자\n\nrbind를 사용해서 옆사람을 항목에 추가한다. 마지막으로 cbind() 함수를 사용해서 “지금이 커피 시간인가요?”라는 질문의 답을 칼럼으로 추가한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dataframe.html#현실적인-예제",
    "href": "dataframe.html#현실적인-예제",
    "title": "6  데이터프레임 탐색",
    "section": "\n6.6 현실적인 예제",
    "text": "6.6 현실적인 예제\n지금까지 고양이 데이터를 가지고 데이터프레임 조작에 대한 기본적인 사항을 살펴봤다. 이제 학습한 기술을 사용해서 좀 더 현실적인 데이터셋을 다뤄보자. 앞에서 다운로드 받은 gapminder 데이터셋을 불러오자.\n\ngapminder &lt;- read.csv(\"data/gapminder_data.csv\")\n\n\n\n\n\n\n\n데이터셋 설명\n\n\n\n\n흔히 맞닥드리는 또다른 유형의 파일이 탭구분자를 갖는 파일(.tsv)이다. 탭을 구분자로 명세하는데, \"\\\\t\"을 사용하고, read.delim() 함수로 불러 읽어온다.\n파일을 download.file() 함수를 사용해서 인터넷으로부터 직접 본인 컴퓨터 폴더로 다운로드할 수 있다. read.csv() 함수를 실행해서 다운로드 받은 파일을 읽어온다. 예를 들어,\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n대안으로, read.csv() 함수 내부에 파일 경로를 웹 주소로 치환해서 인터넷에서 직접 파일을 불러올 수도 있다. 이런 경우 로컬 컴퓨터에 CSV 파일이 전혀 저장되지 않았다는 점을 주의한다. 예를 들어,\n\n\ngapminder &lt;- read.csv(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/main/episodes/data/gapminder_data.csv\")\n\n\n\nreadxl 패키지를 사용해서, 엑셀 스프레드시트를 평범한 텍스트로 변환하지 않고 직접 불러올 수도 있다.\n\n\n\ngapminder 데이터셋을 좀 더 살펴보자. 항상 가장 먼저 해야 되는 작업은 str 명령어로 데이터가 어떻게 생겼는지 확인하는 것이다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ntypeof() 함수로 데이터프레임 칼럼 각각을 면밀히 조사할 수도 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터프레임 차원에 대한 정보를 얻어낼 수도 있다. str(gapminder) 실행결과 gapminder 데이터프레임에 관측점 1704개, 변수 6개가 있음을 상기한다. 다음 코드 실행결과는 무엇일까? 그리고 왜 그렇게 되는가?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n공정한 추측은 아마도 데이터프레임 길이가 행의 길이(1704)라고 보는 것이다. 하지만, 이번에는 다르다. 데이터프레임은 벡터와 요인으로 구성된 리스트라는 사실을 기억하라.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nlength() 함수가 6을 제시하는 이유는 gapminder가 6개 칼럼을 갖는 리스트로 만들어졌기 때문이다. 데이터셋에서 행과 열 숫자를 얻는 데 다음 함수를 던져보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n혹은 한번에 보려면 다음과 같이 한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n또한, 모든 칼럼의 칼럼명이 무엇인지 파악하고자 하면 다음과 같이 질문을 던진다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n현 단계에서, R이 제시하는 구조가 우리의 직관이나 예상과 부합되는지 묻어보는 것이 중요하다. 각 칼럼에 대한 기본 자료형은 이해가 되는가? 만약 납득이 가지 않는다면, 후속 작업에서 나쁜 놀라운 사실로 전환되기 전에 문제를 해결해야 한다. 문제를 해결하는 데, R이 데이터를 이해하는 방법과 데이터를 기록할 때 엄격한 일관성(strict consistency)의 중요성에 관해 학습한 것을 동원한다.\n자료형과 자료구조가 타당해 보이게 되면, 데이터를 제대로 파고들 시간이 되었다. gapminder 데이터의 처음 몇 줄을 살펴보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n데이터 마지막 몇 줄, 중간 몇 줄을 점검하는 것도 좋은 습관이다. 그런데 어떻게 점검할 수 있을까?\n중간 몇 줄을 찾아보는 것이 너무 어렵지는 않지만, 임의로 몇 줄을 추출할 수도 있다. 어떻게 할 수 있을까?\n\n\n\n\n\n\n해답\n\n\n\n\n\n마지막 몇 줄을 점검하려면, R에 내장된 함수가 있어서 상대적으로 간단하다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터가 온전한지(혹은 관점에 따라 데이터가 온전하지 않은지)를 점검하는 데 몇 줄을 추출할 수 있을까?\n\n팁: 몇 가지 방법이 존재한다.\n\n중첩 함수(또 다른 함수에 인자로 전달되는 함수)를 사용한 해법도 있다. 새로운 개념처럼 들리지만, 사실 이미 사용하고 있다. my_dataframe[rows, cols] 명령어는 데이터프레임을 화면에 뿌려준다. 데이터프레임에 행이 얼마나 많은지 알지 못하는데 어떻게 마지막 행을 뽑아낼 수 있을까? R에 내장된 함수가 있다. (의사) 난수를 얻어보는 것은 어떨까? R은 난수추출 함수도 갖추고 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n분석 결과를 재현 가능하게 확실히 만들려면, 코드를 스크립트 파일에 저장해서 나중에 다시 볼 수 있어야 한다.\n\n\n\n\n\n\n도전과제\n\n\n\nfile -&gt; new file -&gt; R script로 가서, gapminder 데이터셋을 불러오는 R 스크립트를 작성한다. scripts/ 디렉토리에 저장하고 버전 제어 시스템에도 추가한다. 인자로 파일 경로명을 사용해서 source() 함수를 사용해서 스크립트를 실행하라. (혹은 RStudio “source” 버튼을 누른다)\n\n\n\n\n\n\n해답\n\n\n\n\n\nscripts/load-gapminder.R 파일에 담긴 내용물은 다음과 같다.\n\ndownload.file(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/main/episodes/data/gapminder_data.csv\", destfile = \"data/gapminder_data.csv\")\ngapminder &lt;- read.csv(file = \"data/gapminder_data.csv\")\n\n스크립트를 실행시키면 데이터를 gapminder 변수에 적재시킨다.\n\nsource(file = \"scripts/load-gapminder.R\")\n\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\nstr(gapminder) 출력결과를 다시 불러오자. 이번에는 gapminder 데이터에 대해 str() 함수가 출력하는 모든 것이 의미하는 바를 설명한다. 지금까지 학습한 요인, 리스트와 벡터 뿐만 아니라, colnames(), dim() 같은 함수도 동원한다. 이해하지 못한 부분이 있다면, 주변 동료와 상의한다!\n\n\n\n\n\n\n해답\n\n\n\n\n\ngapminder 객체는 다음 칼럼을 갖는 데이터프레임이다. - country와 continent 변수는 요인형 벡터 - year 변수는 정수형 벡터 - pop, lifeExp, gdpPercap 변수는 숫자형 벡터",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터프레임 탐색</span>"
    ]
  },
  {
    "objectID": "dplyr.html",
    "href": "dplyr.html",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "",
    "text": "7.1 dplyr 팩키지\n운 좋게도, dplyr 팩키지가 데이터프레임을 솜씨 있게 조작하는 데 있어 유용한 함수를 많이 제공한다. 이를 통해서, 위에서 언급된 반복을 줄이고, 실수를 범할 확률도 줄이고, 심지어 타이핑 수고도 줄일 수 있다. 보너스로, dplyr 문법은 훨씬 더 가독성도 높다.\n가장 흔히 사용되는 6가지 함수뿐만 아니라, 이런 함수를 조합하는 데 사용되는 파이프 (%&gt;%) 연산자 사용법도 다룬다.\n이전 수업에서 팩키지를 설치하지 않았다면, 설치해서 직접 실습해 보기 바란다. 이제 팩키지를 불러와서 적재한다.\ninstall.packages('dplyr')\nlibrary(\"dplyr\")",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#dplyr-팩키지",
    "href": "dplyr.html#dplyr-팩키지",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "",
    "text": "select()\nfilter()\ngroup_by()\nsummarize()\nmutate()",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#select-동사",
    "href": "dplyr.html#select-동사",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "\n7.2 select() 동사",
    "text": "7.2 select() 동사\n예를 들어, 데이터프레임에서 변수 일부만 뽑아서 작업해 나가고자 한다면, select() 함수를 사용한다. 이 함수는 선택한 변수만 갖도록 지정한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n그림 7.1: select함수 데이터프레임 적용 도식화\n\n\nyear_country_gdp 데이터프레임을 열어보면, year, country, gdpPercap 변수만 담겨있는 것을 볼 수 있다. 위에서는 정규 문법이 사용되었지만, dplyr 팩키지의 장점은 파이프를 사용해서 함수 다수를 조합하는 데 있다. 파이프 문법은 이전에 R에서 살펴봤던 것과는 사뭇 다르다. 위에서 파이프를 사용했던 것을 다시 작성해본다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n파이프를 사용해서 작성한 이유에 대한 이해를 돕기 위해서, 단계별로 살펴보자. 먼저 gapminder 데이터프레임을 불러오고 나서, %&gt;% 파이프 기호를 사용해서 다음 작업단계(select() 함수)로 전달했다. 이번 경우에는 select() 함수에 데이터 객체를 명세하지 않았는데, 이는 이전 파이프로부터 데이터를 전달받았기 때문이다. 재미난 사실: 유닉스 셸 강의에서 이미 파이프를 접해봤을 것이다. R에서 파이프 기호가 %&gt;%인 반면, 셸에서는 |을 사용한다. 하지만 개념은 동일하다!",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#filter-동사",
    "href": "dplyr.html#filter-동사",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "\n7.3 filter() 동사",
    "text": "7.3 filter() 동사\n이제 앞선 작업을 바탕으로 작업을 진척시켜 보자. 유럽대륙만 갖고 작업하고자 한다면 select와 filter를 조합하면 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n명령어 하나 (여러 행에 걸칠 수 있고, 파이프도 포함한다)를 작성하는데, lifeExp, country, year 변수에 대해서 아프리카 대륙(African)만 갖는 데이터프레임을 작성한다. 하지만, 다른 대륙은 포함되면 안 된다. 데이터프레임 행의 개수는 얼마나 되는가? 그리고 이유는 무엇인가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n지난번과 마찬가지로, gapminder 데이터프레임을 filter() 함수에 전달하고 나서, 필터링된 gapminder 데이터프레임 버전을 select() 함수에 전달한다. 주의: 연산순서가 이번 경우에 무척 중요하다. select() 함수를 먼저 실행하면, filter() 함수는 대륙 변수를 찾을 수 없는데, 이는 이전 단계에서 제거했기 때문이다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#group_by-summarize-조합",
    "href": "dplyr.html#group_by-summarize-조합",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "\n7.4 group_by() + summarize() 조합",
    "text": "7.4 group_by() + summarize() 조합\n이제, 기본 베이스(base) R로 작업함으로써 실수를 범하기 쉬운 반복작업을 줄일 것으로 생각했지만, 현재까지는 목표를 달성하지 못했다. 왜냐하면, 각 대륙마다 상기 작업을 반복해야 되기 때문이다. filter() 대신에, group_by()를 사용한다. filter()는 특정 기준을 만족하는 관측점만 넘겨준다(이번 경우: continent==\"Europe\"). group_by()는 본질적으로, 필터에서 사용할 수 있는 모든 유일무이한 기준을 사용할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ngroup_by() 함수를 사용한 데이터프레임 구조(grouped_df)가 원래 gapminder 데이터프레임 구조(data.frame)와 같지 않음에 주목한다. grouped_df는 list 리스트로 간주될 수 있는데, list에 각 항목은 data.frame으로, 각 데이터프레임은 특정 대륙 continent에 대응되는 행만 담겨진다(적어도 상기 예제의 경우).\n\n\n\n\n\n그림 7.2: group_by() 데이터프레임 적용 도식화",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#summarize-동사",
    "href": "dplyr.html#summarize-동사",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "\n7.5 summarize() 동사",
    "text": "7.5 summarize() 동사\n상기 예제는 그다지 특별한 점이 없다. 왜냐하면 group_by() 함수는 summarize()와 함께 사용할 때 훨씬 더 흥미롭기 때문이다. 두 함수를 조합하면 새로운 변수가 생성되는데, 각 대륙별 데이터프레임에 대해 반복적인 함수 작업을 수행할 수 있다. 다시 말해, group_by() 함수를 사용해서, 최초 데이터프레임을 다수의 조각으로 쪼갠 후, 각각에 대해 함수(예를 들어 mean() 혹은 sd())를 summarize() 내부에서 실행시킨다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n그림 7.3: summarize() 데이터프레임 적용 도식화\n\n\n상기 코드는 각 대륙별로 평균 gdpPercap를 계산할 수 있게 하지만, 훨씬 더 개선할 수 있다.\n\n\n\n\n\n\n도전과제\n\n\n\n국가별로 평균 기대수명을 계산한다. 어느 국가가 가장 평균 기대수명이 길고, 어느 국가가 가장 평균 기대수명이 짧은가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n문제를 푸는 또 다른 방식은 dplyr 패키지의 arrange() 함수를 사용하는 것이다. arrange() 함수는 변수에 따라 데이터프레임을 행으로 정렬하는 기능을 갖고 있으며, dplyr 팩키지의 다른 함수처럼 유사한 구문을 갖추고 있다. arrange() 함수 내부에 desc() 함수를 사용해서 내림차순으로 정렬할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\ngroup_by() 함수에 변수 다수를 사용해서 집단으로 그룹을 만들 수도 있다. year 와 continent 변수로 그룹을 만들어 보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이미 매우 막강한 기능이지만, 더 좋게 만들 수 있다! summarize() 함수에 변수 하나를 정의하는 것에 한정되지 않고, 확장도 가능하다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#count와-n-함수",
    "href": "dplyr.html#count와-n-함수",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "\n7.6 count()와 n() 함수",
    "text": "7.6 count()와 n() 함수\n매우 흔한 연산 중 하나는 각 그룹마다 관측점 수를 세는 것이다. dplyr 패키지에는 개수를 도와주는 연관 함수가 2개 있다.\n예를 들어, 2002년 데이터셋에 포함된 국가 수를 확인하고자 한다면, count() 함수를 사용하는데, 관심 있는 그룹을 포함하는 칼럼을 하나 이상 지정할 수도 있다. 선택사항으로 sort=TRUE를 인자로 추가하면 내림차순으로 결과를 정렬할 수 있다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n계산과정에서 관측점 개수를 파악할 필요가 있는 경우, n() 함수가 유용하다. 예를 들어, 각 대륙별 기대수명 표준오차를 다음과 같이 구할 수도 있다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n요약 연산을 몇 개 엮어서 계산할 수도 있다. 이 경우 각 대륙별 기대수명에 대한 minimum, maximum, mean, se 값을 다음과 같이 계산한다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#mutate-동사",
    "href": "dplyr.html#mutate-동사",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "\n7.7 mutate() 동사",
    "text": "7.7 mutate() 동사\nmutate() 함수를 사용해서 정보를 요약하기 전에(혹은 후에도) 새로운 변수를 생성할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#mutate-논리-필터와-연결-ifelse",
    "href": "dplyr.html#mutate-논리-필터와-연결-ifelse",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "\n7.8 mutate() 논리 필터와 연결: ifelse\n",
    "text": "7.8 mutate() 논리 필터와 연결: ifelse\n\n변수를 새로 만들 때, mutate() 함수를 논리 조건문과 엮을 수 있다. mutate()와 ifelse()의 단순 조합을 통해서 필요한 것만 적절히 필터링할 수 있다. 즉, 새로운 무언가를 생성하는 순간에 말이다. 이러한 가독성 높은 문장을 통해서 (데이터프레임 전체 차원을 변경시키지 않고도) 특정 데이터를 버리거나, 조건에 따라 값을 갱신하는 데 신속하고 강력한 방식을 제공할 수 있게 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "dplyr.html#dplyr와-ggplot2-조합",
    "href": "dplyr.html#dplyr와-ggplot2-조합",
    "title": "\n7  dplyr 데이터프레임 조작\n",
    "section": "\n7.9 dplyr와 ggplot2 조합",
    "text": "7.9 dplyr와 ggplot2 조합\n데이터 시각화를 위해서 ggplot2를 설치하고 불러온다.\n\ninstall.packages('ggplot2')\nlibrary(\"ggplot2\")\n\nggplot2을 사용해서 패싯(facet) 패널 계층을 추가해서 작은 창에 그래프를 담아내는 방식을 앞선 시각화 수업에서 확인했다. 다음에 앞서 사용한 코드가 나와 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n상기 코드는 원하는 그래프를 만들어 주지만, 다른 용도로는 뚜렷이 사용되지 않는 변수(starts.with, az.countries)도 생성시키게 된다. %&gt;% 연산자를 사용해서 dplyr 함수를 엮어 데이터를 파이프에 흘러 보냈듯이, ggplot() 함수에 데이터를 흘러 보낼 수 있다. %&gt;% 파이프 연산자가 함수의 첫 번째 인자를 대체하기 때문에,ggplot() 함수에 data = 인자를 명세할 필요는 없다. dplyr, ggplot2 함수를 조합하게 되면, 동일한 그래프를 생성하는 데 있어 변수를 새로 생성시키거나 데이터를 변경할 필요가 없어진다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ndplyr 함수를 사용하게 되면 문제를 단순화할 수 있다. 예를 들어, 첫 두 단계를 다음과 같이 조합할 수도 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n고급 도전과제\n\n\n\n각 대륙별로 국가를 두 개씩 임의로 뽑아서 2002년 평균 기대수명을 계산해보자. 그리고 나서, 역순으로 대륙명을 정렬한다. 힌트: dplyr 패키지의 arrange(), sample_n() 함수를 사용한다. 두 함수 모두 다른 dplyr 함수와 유사한 구문을 갖고 있다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n그림 7.1: select함수 데이터프레임 적용 도식화\n그림 7.2: group_by() 데이터프레임 적용 도식화\n그림 7.3: summarize() 데이터프레임 적용 도식화",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`dplyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html",
    "href": "tidyr.html",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "",
    "text": "8.1 시작하기\n먼저 설치하지 않았다면 tidyr 패키지를 설치한다(아마도 앞에서 dplyr 패키지는 설치했을 것이다):\n#install.packages(\"tidyr\")\n#install.packages(\"dplyr\")\n패키지를 로드한다\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\n먼저 원래 gapminder 데이터프레임의 구조를 살펴보자:\nstr(gapminder)\n#&gt; 'data.frame':    1704 obs. of  6 variables:\n#&gt;  $ country  : chr  \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n#&gt;  $ year     : int  1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n#&gt;  $ pop      : num  8425333 9240934 10267083 11537966 13079460 ...\n#&gt;  $ continent: chr  \"Asia\" \"Asia\" \"Asia\" \"Asia\" ...\n#&gt;  $ lifeExp  : num  28.8 30.3 32 34 36.1 ...\n#&gt;  $ gdpPercap: num  779 821 853 836 740 ...\ngapminder 데이터셋처럼, 관측된 데이터에는 다양한 자료 형식이 있다. 대부분 순도 100% ‘long’ 혹은 순도 100% ‘wide’ 자료 형식 사이 어딘가에 위치하게 된다. gapminder 데이터셋에는 “ID” 변수가 3개(continent, country, year), “관측변수”가 3개(pop, lifeExp, gdpPercap)가 있다. 저자는 일반적으로 대부분의 경우에 중간단계 형식 데이터를 선호한다. 칼럼 1곳에 모든 관측점이 3가지 서로 다른 단위를 갖는 일은 거의 없다(예를 들어, ID변수 4개, 관측변수 1개).\n흔히 벡터 기반인 다수의 R 함수를 사용할 때, 서로 다른 단위를 갖는 값에 수학적 연산작업을 수행하지는 않는다. 예를 들어, 순수 ‘long’ 형식을 사용할 때, 인구, 기대수명, GDP의 모든 값에 대한 평균은 의미가 없는데, 이는 상호 호환되지 않는 3가지 단위를 갖는 평균값을 계산하여 반환하기 때문이다. 해법은 먼저 집단으로 그룹지어서 데이터를 솜씨 있게 다루거나(dplyr 학습교재 참조), 데이터프레임 구조를 변경시키는 것이다. 주의: R에서 일부 도식화 함수는 ‘wide’ 형식 데이터에 더 잘 작동한다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#시작하기",
    "href": "tidyr.html#시작하기",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "",
    "text": "도전과제\n\n\n\ngapminder는 순수한 ‘long’ 형식인가, ‘wide’ 형식인가, 혹은 두 가지 특징을 갖는 중간 형식인가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n원 gapminder 데이터프레임은 두 가지 특징을 갖는 중간 형식이다. 데이터프레임에 다수의 관측변수(pop, lifeExp, gdpPercap)가 있다는 점에서, 순수한 long 형식이라고 보기는 어렵다.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#pivot_longer-wide에서-long-형식-전환",
    "href": "tidyr.html#pivot_longer-wide에서-long-형식-전환",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "\n8.2 pivot_longer(): wide에서 long 형식 전환",
    "text": "8.2 pivot_longer(): wide에서 long 형식 전환\n지금까지 깔끔한 형식을 갖는 원본 gapminder 데이터셋으로 작업을 했다. 하지만 ‘실제’ 데이터(즉, 자체 연구 데이터)는 절대로 잘 구성되어 있지 못하다. gapminder 데이터셋에 대한 wide 형식 버전을 가지고 시작해보자.\n\n이곳에서 ‘wide’ 형태를 갖는 gapminder 데이터를 다운로드 받아서, 로컬 data 폴더에 저장시킨다.\n\n데이터 파일을 불러와서 살펴보자. 주의: continent, country 칼럼이 요인형 자료형이 될 필요가 없으므로 read.csv() 함수 인자로 stringsAsFactors을 거짓(FALSE)으로 설정한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nwide 형식 데이터프레임\n\n깔끔한 중간 데이터 형식을 얻는 첫 단추는 먼저 ‘wide’ 형식에서 ‘long’ 형식으로 변환하는 것이다. tidyr 패키지의 pivot_longer() 함수는 관측 변수를 모아서(gather) long 형식 단일 변수로 변환한다. wide에서 long 형식으로 변환하기 위해 pivot_longer() 함수를 사용한다. pivot_longer()는 행의 수를 늘리고 열의 수를 줄임으로써 데이터셋을 더 길게 만들거나 관측 변수를 단일 변수로 ’연장’한다.\n\n\n\n\n\n그림 8.2: wide 형식에서 long 형식 전환과정 도식화\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n위에서 파이프 구문을 사용했는데, 이는 앞서 dplyr로 작업한 것과 유사하다. 사실, dplyr과 tidyr은 상호 호환되어 파이프 구문으로 dplyr과 tidyr 팩키지 함수를 파이핑하여 혼합하여 사용할 수 있다.\n먼저 pivot_longer()에 longer 형식으로 피벗될 열 이름 벡터를 제공한다. 모든 관측 변수를 입력할 수도 있지만 dplyr 레슨의 select() 함수처럼 starts_with() 인수를 사용하여 원하는 문자열로 시작하는 모든 변수를 선택할 수 있다. pivot_longer()는 피벗하지 않을 변수(즉, ID 변수)를 식별하기 위해 - 기호를 사용하는 대체 구문도 허용한다. pivot_longer()에 대한 다음 인수는 새 ID 변수(obstype_year)를 포함할 열의 이름을 지정하는 names_to와 새로 합쳐진 관측 변수(obs_value)의 이름을 지정하는 values_to이다. 새 열 이름을 문자열로 제공하여 후속 작업 가독성을 높인다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n특정 데이터프레임에서는 사소해 보일 수 있지만, 때로는 ID 변수 1개와 불규칙한 변수 이름을 가진 관측 변수 40개를 가질 수 있다. 이런 유연성은 시간을 상당히 절약해 준다!\n이제 obstype_year은 정보가 두 조각으로 나뉜다. 관측 유형(pop, lifeExp, gdpPercap)과 연도(year). separate() 함수를 사용하여 문자열을 여러 변수로 분할할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\ngap_long을 사용해서 각 대륙별로 평균 기대수명, 인구, 1인당 GDP를 계산한다. 힌트: dplyr에서 학습한 group_by()와 summarize() 함수를 사용한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "tidyr.html#pivot_wider-long에서-중간-형식으로",
    "href": "tidyr.html#pivot_wider-long에서-중간-형식으로",
    "title": "\n8  tidyr 데이터프레임 조작\n",
    "section": "\n8.3 pivot_wider(): ’long’에서 중간 형식으로",
    "text": "8.3 pivot_wider(): ’long’에서 중간 형식으로\n작업을 항상 확인하는 것이 좋다. pivot_wider()는 pivot_longer()의 반대로, 열의 수를 늘리고 행의 수를 줄여 데이터셋을 더 넓게 만든다. pivot_wider()를 사용하여 gap_long을 원래의 중간 형식 또는 가장 넓은 형식으로 피벗하거나 재구성할 수 있다. 중간 형식에서부터 시작해보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n이제 최초 데이터프레임 gapminder와 동일한 차원을 갖는 중간 데이터프레임 gap_normal이 있다. 하지만 변수 순서가 다르다. 순서를 수정하기 전에 all.equal() 함수를 사용해서 동일한지 확인한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n거의 다 왔다. 최초 데이터프레임은 country로 정렬된 다음 year로 정렬되었다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n훌륭하다! ‘long’ 형식에서 다시 중간 형식으로 돌아왔지만, 코드에 어떤 오류도 스며들지 않았다.\n이제 long에서 wide로 완전히 변환해 보자. wide 형식에서는 country와 continent를 ID 변수로 유지하고 관측치를 3개의 측정 기준(pop, lifeExp, gdpPercap)과 시간(year)에 걸쳐 피벗할 것이다. 먼저 모든 새 변수(시간*측정 기준 조합)에 대한 적절한 레이블을 만들어야 하며 gap_wide를 정의하는 과정을 단순화하기 위해 ID 변수를 통합해야 한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nunite()를 사용하여 이제 continent와 country의 조합인 단일 ID 변수를 가지고 있고 변수 이름을 정의했다. 이제 pivot_wider()로 파이핑할 준비가 되었다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n도전과제\n\n\n\n국가, 연도 및 3개의 측정 기준에 대해 피벗하여 gap_ludicrously_wide 형식 데이터를 만드시오.\n힌트 이 새로운 데이터 프레임은 행이 5개만 있어야 한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n이제 훌륭한 ‘wide’ 형식 데이터프레임을 가지고 있지만 ID_var가 더 사용하기 편할 수 있다. separate()를 사용하여 2개의 변수로 분리해 보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다시 되돌아왔다!\n\n\n\n그림 8.1: wide와 long 데이터프레임 비교\nwide 형식 데이터프레임\n그림 8.2: wide 형식에서 long 형식 전환과정 도식화",
    "crumbs": [
      "**2부** 데이터 다루기",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`tidyr` 데이터프레임 조작</span>"
    ]
  },
  {
    "objectID": "ggplot2_concept.html",
    "href": "ggplot2_concept.html",
    "title": "9  그래프 문법",
    "section": "",
    "text": "9.1 그래프 문법의 존재이유\nLeland Wilkinson의 “The Grammar of Graphics”는 1999년 첫 출간된 이후 데이터 그래픽(data graphics)에 많은 영향을 주어 ggplot2, Polaris → Tableau, Vega-Lite 등의 형태로 우리곁에 다가섰다. 기존 데이터를 시각화한 다양한 그래픽 객체를 만드려면 각 그래픽 객체별로 따로 사용법을 익혀야만 되었다. 이것이 갖는 한계는 그래프 종류가 적은 경우 유용하지만 그래프 종류가 많아지면 매번 따로 사용법을 배워야되서 확장가능성이 무척 떨어지게 된다. 그래서, 이를 일반화한 무언가 필요한데 데이터 그래픽 객체를 분해해서 8개 계층으로 분해하여 조립하여 그래프를 제작하게 되면 앞선 문제를 일거에 해소할 수 있다. 그래프 문법(grammar of graphics)을 통해 데이터를 가장 잘 표현할 수 있는 그래프를 생성할 수 있게 되었다.",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>그래프 문법</span>"
    ]
  },
  {
    "objectID": "ggplot2_concept.html#why-ggplot2",
    "href": "ggplot2_concept.html#why-ggplot2",
    "title": "9  그래프 문법",
    "section": "",
    "text": "그림 9.1: ggplot이 필요한 이유",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>그래프 문법</span>"
    ]
  },
  {
    "objectID": "ggplot2_concept.html#ggplot2-steps",
    "href": "ggplot2_concept.html#ggplot2-steps",
    "title": "9  그래프 문법",
    "section": "\n9.2 그래프 문법",
    "text": "9.2 그래프 문법\n그래프 문법은 총 8가지 층으로 구성되어 있는데 이를 각 층별로 나눠보면 다음과 같다.\n\n9.2.1 데이터(Data)\n그래프 문법 ggplot에 데이터는 깔끔한 데이터(tidy data)를 가정한다. 이를 위해서 기존 wide형태 데이터는 key-value long 형태로 바뀌어 준비한다.\n\n\n\n\n\n그림 9.2: 깔끔한 데이터 wide, long 데이터\n\n\n\n9.2.2 매핑(Mapping)\n깔끔한 데이터가 준비되면 다음 단계로 칼럼에 해당되는 각 변수를 aes() 함수를 사용해서 aes(x=x, y=y, color=z, ...)와 같은 방식으로 데이터와 그래프를 매핑한다.\n\n\n\n\n\n그림 9.3: 데이터를 그래프에 매핑\n\n\n\n9.2.3 통계량(statistics)\n범주형 그래프를 시각화할 경우 빈도수를 통계량으로 계산해놔야 하고, 연속형 변수를 히스토그램으로 표현할 때도 마찬가지 방식으로 구간별 빈도수를 계산해놔야 하고, 특히 상자그림(boxplot)을 시각화할 경우 각 분위수는 물론이고 중위수도 및 interquantile도 계산해서 수염의 끝도 계산해놔야 제대로된 상자그림을 시각화할 수 있다.\n\n\n\n\n\n그림 9.4: 그래프별 기본설정 통계량\n\n\n\n9.2.4 척도(scales)\nX축, Y축의 척도를 그래프에서 자동으로 인식하는데는 한계가 있어 이를 필요한 경우 적절한 형태로 설정한다. scale_&lt;x, y, color, fill, ...&gt;_&lt;유형&gt;() 구문을 갖는다. 예를 들어 Y축을 담당하는 변수가 로그척도(log)가 적합한 경우 이를 scale_y_log10()와 같이 변수를 특성을 반영한 척도를 설정한다. 변수가 날짜나 시간인 경우 scale_x_date(), scale_x_datetime()을 활용하여 적절한 형태로 설정한다.\n\n\n\n\n\n그림 9.5: aes 매핑된 축 척도지정\n\n\n\n9.2.5 기하형상(geometries)\n예를 들어, 범주형 변수를 aes()로 지정하고 이를 적절한 그래프로 표현하기 위한 결정과정으로 geom_*() 방식으로 원그래프, 막대그래프, 점그래프 등으로 변수를 시각화객체로 지정한다.\n\n\n\n\n\n그림 9.6: 기하형상 지정\n\n\n\n9.2.6 작은 창(facet)\n원본 데이터를 그룹으로 쪼개 작은 창에 동일한 시각화 객체를 표현하는 방법으로 다차원 데이터를 차원별로 나눠 볼 수 있다. 중요한 점은 각 작은 창이 동일한 유형의 그래프라는 점이 중요하다.\n\n\n\n\n\n그림 9.7: 그룹으로 쪼갠 작은 창(facet)\n\n\n\n9.2.7 좌표계(coordinate)\n깔끔한 데이터를 그래프에 매칭하여 시각화한 후에 경우에 따라서 좌표계를 변경할 경우가 있다. coord_fixed(), coord_polar(), coord_flip() 등을 사용해서 특정 영역 확대, 데카르트 좌표계에서 극좌표계, X-Y 축 변경 등의 작업을 수행할 수 있다.\n\n\n\n\n\n그림 9.8: 좌표계 변환\n\n\n\n9.2.8 분위기 풍(theme)\n분위기 풍(theme)은 앞선 깔끔한 데이터를 시각화 객체로 변환시키는 과정과 아무런 연관이 없다. 대신 외양을 보기좋게 하는 역할을 수행한다. 즉, 시각화에 알맞는 색상 팔레트를 적용시키고, 글꼴을 바꾸는 작업이 여기에 해당된다.\n\n\n\n\n\n그림 9.9: tufte 분위기 풍(theme)\n\n\n\nlibrary(ggthemes)\n\ndefault_g &lt;- mtcars %&gt;% \n  ggplot(aes(x=disp, y=mpg)) +\n    geom_point()\n\ntufte_g &lt;- mtcars %&gt;% \n  ggplot(aes(x=disp, y=mpg)) +\n    geom_point() +\n    theme_tufte()\n\ngridExtra::grid.arrange(default_g, tufte_g, nrow=1)",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>그래프 문법</span>"
    ]
  },
  {
    "objectID": "ggplot2_concept.html#ggplot2-steps-extension",
    "href": "ggplot2_concept.html#ggplot2-steps-extension",
    "title": "9  그래프 문법",
    "section": "\n9.3 ggplot 확장",
    "text": "9.3 ggplot 확장\n기본 ggplot 그래프 객체를 갖게 되면 2개 이상의 그래프를 합치거나 배열을 달리하고, 강조를 하고 주석(annotation)을 달고 애니메이션과 인터랙티브 기능을 추가하여 확장시킬 수 있다.\n\n\n\n\n\n그림 9.10: ggplot2 패키지 확장\n\n\n\n\n\n그림 9.1: ggplot이 필요한 이유\n그림 9.2: 깔끔한 데이터 wide, long 데이터\n그림 9.3: 데이터를 그래프에 매핑\n그림 9.4: 그래프별 기본설정 통계량\n그림 9.5: aes 매핑된 축 척도지정\n그림 9.6: 기하형상 지정\n그림 9.7: 그룹으로 쪼갠 작은 창(facet)\n그림 9.8: 좌표계 변환\n그림 9.9: tufte 분위기 풍(theme)\n그림 9.10: ggplot2 패키지 확장",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>그래프 문법</span>"
    ]
  },
  {
    "objectID": "ggplot2.html",
    "href": "ggplot2.html",
    "title": "10  논문 품질 그래프 생성",
    "section": "",
    "text": "10.1 계층\n산점도는 시간에 따른 정보를 시각화하는 데 가장 적합한 방법은 아닐 수 있다. 대신 ggplot에게 선 그래프로 데이터를 표현하도록 지시할 수 있다.\ngeom_point 계층 대신 geom_line 계층을 추가했다. aes에 by를 추가하여 ggplot이 각 국가를 선으로 연결하여 표현하도록 했다.\n그런데 선과 점을 모두 표시하려면 어떻게 해야 할까? 간단히 그래프에 또 다른 계층을 추가하면 된다.\n각 계층이 이전 계층 위에 그려진다는 점에 주목하자. 이번 예제에서는 점이 선 위에 표시되었다. 다음에 표시된 그래프를 보자.\n이 예제에서 aesthetic color 매핑이 ggplot의 전역 설정에서 geom_line 계층으로 이동했다. 따라서 더 이상 점에는 적용되지 않는다. 이제 선이 점 위에 그려진 것을 명확히 볼 수 있다.",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#계층",
    "href": "ggplot2.html#계층",
    "title": "10  논문 품질 그래프 생성",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\naesthetic에 값 설정하기\n\n\n\n지금까지 (색상같은) aesthetic 를 데이터의 변수로 매핑(mapping)해서 사용하는 법을 살펴봤다. 예를 들어, geom_line(aes(color=continent))을 사용하면, ggplot에서 자동으로 각 대륙별로 다른 색상을 입힌다. 그런데, 모든 선을 파란색으로 바꾸고자 하면 어떨까? geom_line(aes(color=\"blue\")) 명령어가 동작해야 된다고 생각하지만, 사실은 그렇지 않다.특정 변수에 대한 매핑을 생성하지 않았기 대문에, aes() 함수 밖으로 색상을 명세하는 부분을 예를 들어, geom_line(color=\"blue\")와 같이 빼내기만 하면 된다.\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n\n앞선 예제에서 점과 선 계층의 순서를 바꿔보자. 어떻게 되는가?\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n앞선 예제에서 점과 선 계층의 순서를 바꿔보자. 어떻게 되는가?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n선이 점 위에 그려진다!",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#변환과-통계",
    "href": "ggplot2.html#변환과-통계",
    "title": "10  논문 품질 그래프 생성",
    "section": "10.2 변환과 통계",
    "text": "10.2 변환과 통계\nggplot을 사용하면 데이터에 통계 모델을 쉽게 겹칠 수 있다. 이를 시연하기 위해서, 첫번째 예제로 돌아가보자.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n현재 1인당 GDP에 몇 가지 극단적인 이상치가 있어서 점 사이의 내재된 관계를 보기 어렵다. scale 함수를 사용하여 y 축 스케일을 조정할 수 있다. 이를 통해 데이터 값과 aesthetic 시각적 표현 사이의 매핑을 제어할 수 있다. alpha 함수를 사용하여 투명도도 조정할 수 있는데, 이는 특히 많은 데이터가 한 곳에 밀집되어 있을 때 유용하다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n그래프를 렌더링하기 전에 log10 함수가 gdpPercap 열 값을 변환했다. 따라서 변환된 스케일에서 10의 거듭제곱 단위마다 1씩 증가한다. 예를 들어, 1인당 GDP 1,000은 y-축에 3, 10,000은 y-축에 4에 대응된다. 로그 변환은 x 축에 넓게 퍼져 있는 데이터를 시각화하는 데 도움이 된다.\n\n\n\n\n\n\naesthetic에 값 설정하기\n\n\n\ngeom_point(alpha = 0.5)를 사용한 것에 주목하자. 앞서 언급했듯이, aes() 함수 외부에서 설정한 것은 모든 점에 동일한 값이 적용된다. 이 경우 투명도를 지정하는 것은 원하는 바로 문제가 없다. 그러나 다른 aesthetic 설정과 마찬가지로 alpha 투명도를 데이터의 변수에 매핑할 수도 있다. 예를 들어, 각 대륙마다 다른 투명도를 적용하려면 geom_point(aes(alpha = continent))와 같이 코딩하면 된다.\n\n\n또 다른 계층(geom_smooth)을 추가하여 관계를 단순하게 모델링할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n굵은 선의 두께는 geom_smooth 계층의 aesthetic size를 설정하여 조정할 수 있다:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\naesthetic을 지정하는 방법에는 두 가지가 있다. 방금 전에는 geom_smooth 함수에 인수로 전달하여 size에 대한 aesthetic을 설정했다. 앞에서는 aes 함수를 사용하여 데이터 변수와 시각적 표현 사이의 매핑을 정의했다.\n\n\n\n\n\n\n도전과제\n\n\n\n바로 이전 예제에서 점 계층의 점 크기와 색상을 변경해보자.\n힌트: aes 함수를 사용하지 않는다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n바로 이전 예제에서 점 계층의 점 크기와 색상을 변경해보자. 힌트: aes 함수를 사용하지 않는다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\n\n도전과제 4a를 수정하여 점의 모양을 바꾸고 대륙별로 색상을 다르게 하되, 대륙별로 추세선도 표시되도록 해보자. 힌트: 색상 인수를 aesthetic 내부로 이동시킨다.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n도전과제 4a를 수정하여 점의 모양을 바꾸고 대륙별로 색상을 다르게 하되, 대륙별로 추세선도 표시되도록 해보자. 힌트: 색상 인수를 aesthetic 내부로 이동시킨다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#다중-패널-그래프",
    "href": "ggplot2.html#다중-패널-그래프",
    "title": "10  논문 품질 그래프 생성",
    "section": "10.3 다중-패널 그래프",
    "text": "10.3 다중-패널 그래프\n앞에서는 하나의 그래프에 모든 국가에 대한 시간의 흐름에 따른 기대수명 변화를 시각화했다. 대안으로 facet 계층을 추가하여 그래프를 여러 개의 패널로 나눌 수 있다. 이번에는 국가명이 “A” 또는 “Z”로 시작하는 국가에만 초점을 맞춰보자.\n\n\n\n\n\n\n유용한 팁\n\n\n\n데이터의 부분 집합을 추출하는 것으로 시작해보자. substr 함수를 사용하여 문자열의 일부를 추출할 수 있다. 이 경우에는 gapminder$country 벡터의 시작과 끝 위치 문자를 추출한다. %in% 연산자를 사용하면 여러 개의 비교를 간단하게 수행할 수 있다. (이 경우 starts.with %in% c(\"A\", \"Z\")는 starts.with == \"A\" | starts.with == \"Z\"와 동일하다)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nfacet_wrap 계층은 틸드(~)로 표시되는 “공식”을 인수로 받는다. gapminder 데이터셋의 country 열에 있는 각 고유값에 대해 별도의 패널로 그래프를 생성한다.",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#텍스트-수정하기",
    "href": "ggplot2.html#텍스트-수정하기",
    "title": "10  논문 품질 그래프 생성",
    "section": "10.4 텍스트 수정하기",
    "text": "10.4 텍스트 수정하기\n출판용 그래프를 만들기 위해서는 텍스트 요소를 일부 변경해야 할 필요가 있다. x 축이 너무 복잡하고, y 축은 데이터프레임 열 이름이 아닌 “Life expectancy”로 표시되어야 한다.\n몇 가지 다른 계층을 추가하여 텍스트를 수정할 수 있다. theme 계층은 각 축의 텍스트와 전반적인 텍스트 크기를 제어한다. 축, 그래프 제목, 범례는 labs() 함수를 사용하여 설정할 수 있다. 범례 제목은 aes() 함수에서 지정한 것과 동일한 이름을 사용한다. 따라서 색상 범례 제목은 color = \"Continent\"가 되고, 채우기(fill) 범례는 fill = \"MyTitle\"이 된다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#그래프-저장하기",
    "href": "ggplot2.html#그래프-저장하기",
    "title": "10  논문 품질 그래프 생성",
    "section": "10.5 그래프 저장하기",
    "text": "10.5 그래프 저장하기\nggsave() 함수를 사용하여 ggplot으로 생성한 그래프를 로컬 컴퓨터에 저장할 수 있다. 출판을 위한 고품질 그래픽을 생성하기 위해 그래프의 크기와 해상도를 ggsave() 함수의 인수(width, height, dpi)로 전달할 수 있다. 앞에서 생성한 그래프를 저장하려면 먼저 lifeExp_plot 변수에 그래프를 할당한 다음 ggsave() 함수를 사용하여 png 형식으로 results 디렉터리에 저장하도록 지정한다. (작업 디렉터리에 results/ 폴더가 생성되어 있어야 한다.)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nggsave() 함수에는 두 가지 좋은 점이 있다. 첫째, 기본값으로 가장 최근에 생성한 그래프가 저장되므로 plot 인수를 생략하면 ggplot으로 생성한 마지막 그래프가 자동으로 저장된다. 둘째, 저장되는 그래프 이미지 형식이 파일 확장자(예: .png 또는 .pdf)에 따라 결정된다. 필요한 경우 device 인수를 사용하여 명시적으로 파일 형식을 지정할 수도 있다.\n지금까지 ggplot2의 기본을 살펴보았다. RStudio는 다른 계층 사용법에 대한 참고자료로 유용한 치트 시트를 제공하고 있으며, ggplot2 웹사이트에는 추가 기능에 대한 자세한 정보가 있다. 마지막으로, 어떻게 수정해야 할지 모르겠다면 구글 검색을 통해 Stack Overflow에서 재사용 가능한 코드와 함께 관련 질문과 답변을 쉽게 찾을 수 있다.\n\n\n\n\n\n\n도전과제 (심화)\n\n\n\n가용한 연도 기간동안 각 대륙 간 기대수명을 비교하는 상자 그림(boxplot)을 생성한다.\n\ny축의 이름을 “기대수명”으로 변경한다.\nx축 레이블은 제거한다.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n가능한 해법 중 하나는 다음과 같다. xlab()과 ylab()은 각각 x축과 y축 레이블을 설정한다. 축의 제목, 텍스트, 눈금은 테마의 속성이며 theme() 호출 내에서 수정되어야 한다.\n\n10.5.1 R\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>논문 품질 그래프 생성</span>"
    ]
  },
  {
    "objectID": "ggplot2_pattern.html",
    "href": "ggplot2_pattern.html",
    "title": "11  시각화 패턴",
    "section": "",
    "text": "11.1 라벨 붙은 시계열\n시계열 데이터를 제작하게 되면 추세를 파악할 수 있지만 결국 그래서 가장 최근 값이 어떻게 되는지 관심이 많다. 이런 사용자 요구를 맞추는데 시계열 데이터 마지막 시점에 라벨값을 붙이게 되면 가독성도 좋아진다. 기본적인 작업흐름은 데이터셋에서 가장 최근 관측점을 뽑아서 별도 데이터프레임으로 저장하고 이를 geom_text() 혹은 geom_text_repel() 함수를 사용해서 해결한다.\nBLOGR 님이 작성한 Label line ends in time series with ggplot2 코드를 참조하여 ggplot으로 코드를 작성한다.\n## 마지막 관측점\norange_ends &lt;- datasets::Orange %&gt;% \n  group_by(Tree) %&gt;% \n  filter(age == max(age)) %&gt;% \n  ungroup()\n\ndatasets::Orange %&gt;% \n  ## 범례와 그래프 순서 맞추기 위해 범주 순서 조정\n  mutate(Tree = fct_reorder(Tree, -circumference) ) %&gt;%\n  ggplot(aes(age, circumference, color = Tree)) +\n    geom_line(size = 2, alpha = .8) +\n    scale_x_continuous(label=scales::comma, limits = c(0, 1600)) +\n    ## 마지막 관측점 라벨과 큰 점 추가\n    geom_text(data = orange_ends, aes(label = circumference, color = NULL), hjust = -0.5,) +\n    geom_point(data = orange_ends, aes(x=age, y= circumference), size = 3.7) +\n    theme_minimal(base_family = \"MaruBuri\") +\n    labs(title = \"오렌지 나무 성장곡선\",\n         subtitle = \"R 내장된 Orange 데이터셋\",\n         x = \"일별 경과 시간\", y = \"직경 (mm)\", \n         caption = \"제작: 한국 R 사용자회\",\n         color = \"나무\")",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>시각화 패턴</span>"
    ]
  },
  {
    "objectID": "ggplot2_pattern.html#gghighlight-barplot-color",
    "href": "ggplot2_pattern.html#gghighlight-barplot-color",
    "title": "11  시각화 패턴",
    "section": "\n11.4 롤리팝(lolli-pop) 그래프",
    "text": "11.4 롤리팝(lolli-pop) 그래프\n롤리팝(Lollipop) 사탕 그래프는 막대그래프와 클리블랜드 점그래프를 합성한 것으로 한축에는 연속형, 다른 한축에는 범주형을 두고 사용자의 관심을 점그래프로 집중시키는데 효과적이다. 단순히 막대그래프를 제작하는 것과 비교하여 임팩트있는 시각화를 가능하게 한다.\n제작순서는 막대그래프 → 점그래프 → 롤리팝 그래프로 뼈대 골격을 만들어 나간다. 대략 골격이 제작되고 나면 외양과 필요한 경우 값도 텍스트로 넣어 시각화 제품을 완성한다. 롤리팝 사탕 그래프를 작성할 때 geom_point()를 사용해서 롤리팝 사탕을 제작하고, geom_sgement() 함수를 사용해서 사탕 막대를 그린다. 이때 막대 사탕의 시작과 끝을 시작은 x, y에 넣어주고 끝은 xend와 yend에 넣어 마무리한다.\n데이터는 ggplot2에 내장된 midwest 데이터를 사용하자. midwest 데이터셋은 2000년 미국 중서부 센서스 데이터로 인구통계 조사가 담겨있다. percollege 변수는 카운티(우리나라 군에 해당) 별 대학졸업비율을 나타낸다.\n\n# 데이터 -----\n## 롤리팝 사탕 그래프를 위해 상위 15개 군만 추출\nohio_top15 &lt;- ggplot2::midwest %&gt;%\n        filter(state == \"OH\") %&gt;%\n        select(county, percollege) %&gt;%\n        ## 대졸자 비율이 높은 카운티 15개 선정\n        top_n(15, wt = percollege) %&gt;%\n        ## 시각화를 위해 오름차순 정렬\n        arrange(percollege) %&gt;%\n        ## 문자형 자료를 범주형으로 변환\n        mutate(county = factor(county, levels = .$county)) \n\nohio_barplot_g &lt;- ohio_top15 %&gt;% \n  ggplot(aes(county, percollege)) +\n    geom_col(width = 0.5) +\n    coord_flip() +\n    labs(title = \"막대그래프\",\n         y = \"대졸비율(%)\",\n         x = \"\")\n\nohio_dotplot_g &lt;- ohio_top15 %&gt;% \n  ggplot(aes(county, percollege)) +\n    geom_point() +\n    coord_flip() +\n    labs(title = \"점그래프\",\n         y = \"대졸비율(%)\",\n         x = \"\")\n\nohio_lollipop_g &lt;- ohio_top15 %&gt;% \n  ggplot(aes(county, percollege)) +\n    geom_point() +\n    geom_segment(aes(x = county, xend = county, \n                     y = 0, yend = percollege), color = \"grey50\") +\n    coord_flip() +\n    labs(title = \"롤리팝 사탕 그래프\",\n         y = \"대졸비율(%)\",\n         x = \"\")\n\ncowplot::plot_grid(ohio_barplot_g, ohio_dotplot_g, ohio_lollipop_g, nrow=1)\n\n\n\n\n\n\n\n한발더 나아가, 평균값에서 얼마나 차이가 있느냐를 롤리팝 그래프로 시각화하는 패턴이 많이 사용된다. 이를 위해서, 앞서와 마찬가지로 15개 카운티를 뽑아내고 평균을 구하고 평균이상, 평균이하에 대한 요인(factor)도 함께 만들어낸다. 반영한다.\n\n## 평균기준 대졸비율 비교를 위한 데이터셋 준비\nohio &lt;- midwest %&gt;%\n        filter(state == \"OH\") %&gt;%\n        select(county, percollege) %&gt;%\n        top_n(15, wt=percollege) %&gt;%\n        arrange(percollege) %&gt;%\n        mutate(Avg = mean(percollege, na.rm = TRUE),\n               Above = ifelse(percollege - Avg &gt; 0, TRUE, FALSE),\n               county = factor(county, levels = .$county))\n\n## 시각화 기본 골결 제작\ncomparison_lollipop_g &lt;- ohio %&gt;% \n  ggplot( aes(percollege, county, color = Above) ) +\n        geom_segment(aes(x = Avg, y = county, \n                         xend = percollege, yend = county), color = \"grey50\") +\n        geom_point()\n\n## 외양과 설명을 넣어 가시성을 높임\nohio %&gt;% \n  ggplot( aes(percollege, county, color = Above, label=round(percollege,1)) ) +\n    geom_segment(aes(x = Avg, y = county, \n                     xend = percollege, yend = county), color = \"grey50\") +\n    geom_point(size=7) +\n    annotate(\"text\", x = 27.5, y = \"WOOD\", label = \"평균이상\", color = \"#00BFC4\", size = 5, hjust = -0.1, vjust = 5) +\n    annotate(\"text\", x = 27.5, y = \"WOOD\", label = \"평균이하\", color = \"#F8766D\", size = 5, hjust = +1.1, vjust = -1) +\n    geom_text(color=\"black\", size=3) +\n    theme_minimal(base_family = \"MaruBuri\") +\n    labs(x=\"대졸 비율(%)\", y=\"\",\n         title=\"오하이오주 카운티별 대졸비율 비교\")  +\n    geom_segment(aes(x = 27.5, xend = 27.5 , y = \"WOOD\", yend = \"WARREN\"), size=1,\n                 arrow = arrow(length = unit(0.2,\"cm\")), color = \"#00BFC4\") +\n    geom_segment(aes(x = 27.5, xend = 27.5 , y = \"MONTGOMERY\", yend = \"DELAWARE\"), size=1,\n                 arrow = arrow(length = unit(0.2,\"cm\")), color = \"#F8766D\") +\n    theme(legend.position = \"none\")",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>시각화 패턴</span>"
    ]
  },
  {
    "objectID": "ggplot2_pattern.html#gghighlight-trend-line-label",
    "href": "ggplot2_pattern.html#gghighlight-trend-line-label",
    "title": "11  시각화 패턴",
    "section": "\n11.3 추세선 강조 + 라벨",
    "text": "11.3 추세선 강조 + 라벨\n시각화의 백미는 아무래도 대조와 비교를 통해 강한 인상을 주는 것이다. 앞선 ratings TV 시리즈 시청자 평가 데이터를 대상으로 추세선에 강조를 넣고 라벨 텍스트도 넣어 하이라이트 강조 그래프를 작성해보자. geom_line()을 두개 포함시켜 강조하고하는 색상을 별도로 지정하고 선굵기도 달리한다. 라벨도 동일한 방법으로 geom_text()를 두개 포함시켜 강조하고자하는 색상과 글꼴크기도 달리 지정한다.\n\nratings %&gt;% \n  mutate(episode = as.factor(episode)) %&gt;% \n  ggplot(aes(x = episode, y = viewers_7day, group = series)) +\n    geom_line(data = filter(ratings, !series == 8), alpha = .25) +\n    ## 기존 그려진 선에 굵은 선과 색상을 달리하여 차별화한다.\n    geom_line(data = filter(ratings, series == 8), color = \"#CF2154\", size = 1.5) +\n    theme_minimal(base_family = \"MaruBuri\") + \n    labs(x = \"에피소드\", y=\"1주일 시청자수\", title=\"여덟번째 스리즈가 문제네!!!\") +\n    geom_text(data = filter(ratings, episode == 1 & series %in% c(1:7)), color = \"gray50\",\n              aes(label = paste0(series, \" 회차 \")), vjust = -1, family = \"MaruBuri\") +\n    ## 8회차 텍스트 반대위치에 크기를 달리하고 글꼴도 달리하여 라벨 추가\n    geom_text(data = filter(ratings, episode == 10 & series == 8), color = \"#CF2154\",\n              aes(label = glue::glue(\"{series} 회차\")), vjust = -1, family = \"Nanum Pen Script\",\n              size = 7)",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>시각화 패턴</span>"
    ]
  },
  {
    "objectID": "ggplot2_pattern.html#gghighlight-dumbbel-plot",
    "href": "ggplot2_pattern.html#gghighlight-dumbbel-plot",
    "title": "11  시각화 패턴",
    "section": "\n11.5 아령 그래프",
    "text": "11.5 아령 그래프\n두시점을 비교하여 전후를 비교한다던가 두 지역을 비교할 때 아령 그래프는 매우 효과적이다. TV 시리즈별로 회차를 달리하여 첫번째와 가장 마지막 시청자수를 비교하여 시각화하는데 아령(dumbbell) 그래프가 적절한 예시가 될 것으로 보인다. 이를 위해서 ggplot()에 들어가는 자료형을 미리 준비하고 이에 맞춰 geom_line()과 geom_point()를 결합시켜 시각화한다.\n\nratings_dumbbell_df &lt;- ratings %&gt;% \n  select(series, episode, viewers_7day) %&gt;% \n  group_by(series) %&gt;% \n  filter(episode == 1 | episode == max(episode)) %&gt;% \n  mutate(episode = ifelse(episode == 1,\"1회차\", \"최종회\")) %&gt;% \n  ungroup() %&gt;% \n  mutate(series = as.factor(series))\n\n# RColorBrewer::display.brewer.all() 색상\ndumbbell_pal &lt;- RColorBrewer::brewer.pal(n=3, name=\"Set1\")\n\nratings_dumbbell_df %&gt;% \n  ggplot(aes(x = viewers_7day, y = fct_rev(series), color = episode, group = series)) +\n    geom_line(size = .75) +\n    geom_point(size = 2.5) +\n    theme_minimal() +\n    scale_color_manual(values = dumbbell_pal) +\n    labs(title = \"아령 그래프 사례\",\n         subtitle = \"첫번째와 마지막 TV 시리즈를 에피소드별로 비교\",\n         y = \"시즌\", x = \"시청자수 (백만명)\",\n         color = \"회차\") +\n    theme(text = element_text(family = \"MaruBuri\"),\n          legend.position = \"top\")",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>시각화 패턴</span>"
    ]
  },
  {
    "objectID": "ggplot2_pattern.html#gghighlight-slope-plot",
    "href": "ggplot2_pattern.html#gghighlight-slope-plot",
    "title": "11  시각화 패턴",
    "section": "\n11.6 경사 그래프",
    "text": "11.6 경사 그래프\n아령 그래프를 제작한 동일한 데이터를 터프티(tufte) 스타일 경사그래프로 구현하면 시즌별 첫회와 최종회 시청자수 비교를 좀더 직관적으로 만들 수 있다.\nggplot의 기본기능을 활용하여 경사그래프를 시각화하고 강조하고자 하는 시즌을 색상을 달리하여 표현한다. 이를 통해 1~7번째 시즌은 1회차 시청율은 낮으나 최종회는 높게 마무리된 것을 알 수 있고, 더불어 시즌이 진행될 수록 1회차 시청율도 높아지고 있었다. 하지만 8번째 시즌은 다른 시즌과 달리 낮게 시작했고 최종회 시청률도 크게 나아지지 않은 것을 한눈에 파악할 수 있다.\n\nratings_dumbbell_df %&gt;% \n  ggplot(aes(x = episode, y = viewers_7day, group = series)) +\n    geom_point(data = filter(ratings_dumbbell_df, !series == 8), alpha = .25) +\n    geom_point(data = filter(ratings_dumbbell_df, series == 8), color = \"#CF2154\") +\n    geom_line(data = filter(ratings_dumbbell_df, !series == 8), alpha = .25) +\n    geom_line(data = filter(ratings_dumbbell_df, series == 8),color = \"#CF2154\") +\n    theme_minimal(base_family = \"MaruBuri\") +\n    labs(title = \"경사그래프(Slopegraph) 사례\", x=\"에피소드\", y=\"주별 시청자수 (단위: 백만)\") +\n    ## 8번째 시즌\n    geom_text(data = filter(ratings_dumbbell_df, episode == \"최종회\" & series %in% c(1:7)), color = \"gray\",\n              aes(label = series), vjust = -1, family = \"Nanum Pen Script\", hjust = .5) +\n    geom_text(data = filter(ratings_dumbbell_df, episode == \"최종회\" & series == 8), color = \"#CF2154\",\n              aes(label = paste0(series, \" 시즌\")), vjust = -1, family = \"Nanum Pen Script\", size=5) +\n    ## 1~7번째 시즌\n    geom_text(data = filter(ratings_dumbbell_df, episode == \"1회차\" & series %in% c(1:7)), color = \"gray50\",\n              aes(label = paste0(series, \" 시즌\")), family = \"MaruBuri\", hjust = 1.2) \n\n\n\n\n\n\n\n경사그래프를 제작하고는 싶으나 전반적으로 시간이 더 필요하신 분을 위해 slopegraph 패키지가 있다.\nslopegraph는 Base 그래픽을 기본으로 삼고 있어 자료구조도 rownames를 갖는 전통적인 데이터프레임이다. 기본 Base 그래픽을 염두에 두고 상기 TV 연속물 경사그래프를 다음과 같이 작성할 수 있다.\n\n# devtools::install_github(\"leeper/slopegraph\")\nlibrary(slopegraph)\n\nseries_cols &lt;- c(rep(\"darkgray\", 7), \"red\")\n\nratings_dumbbell_df %&gt;% \n  spread(episode, viewers_7day) %&gt;% \n  as.data.frame() %&gt;% \n  column_to_rownames(var=\"series\") %&gt;% \n  slopegraph(., col.lines = series_cols, col.lab = series_cols, \n            cex.lab = 1.5, cex.num = 1.0,\n            xlim = c(-0.5, 3.5), \n            xlabels = c('첫회','최종회'))\n\n\n\n\n\n\n\nslopegraph() 함수 대신 ggslopegraph() 함수를 사용하게 되면 ggplot()으로도 시각화를 할 수 있다. slopegraph() 함수는 자료구조가 직관적이라 처음 시각화를 하는 분에게 적절한 듯 보인다. 따라서, 앞서 ggplot 기반 경사그래프를 제작하고자 하는 경우 ggslopegraph()을 통해서도 ggplot 나머지 기능을 그대로 적용 가능하다.\n\nratings_dumbbell_df %&gt;% \n  pivot_wider(names_from = episode, values_from = viewers_7day) %&gt;% \n  as.data.frame() %&gt;% \n  column_to_rownames(var=\"series\") %&gt;% \n  ggslopegraph(offset.x = 0.06, yrev = FALSE,\n               col.lines = series_cols, col.lab = series_cols) +\n    theme_minimal(base_family = \"NanumGothic\") +\n    labs(title = \"경사그래프 사례 - ggplot\")",
    "crumbs": [
      "**3부** 시각화",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>시각화 패턴</span>"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "\n12  모형\n",
    "section": "",
    "text": "12.1 회귀분석\n회귀분석은 갤톤(Galton) 부모와 자식의 신장간의 관계를 회귀식으로 표현한 데이터셋이 유명하다. (Caffo 2015) (Friendly 2023) 부모의 신장을 기초로 자녀의 신장을 예측하는 회귀식을 구하기 전에 산점도를 통해 관계를 살펴보면 다음과 같다. 성별에 대한 신장의 차이도 산점도를 통해 시각적으로 확인된다.\nlibrary(tidyverse)\nlibrary(HistData)\ndata(GaltonFamilies)\n\nlibrary(ggplot2)\n\n## 1. 산점도 \n# 성별 색상으로 구분\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight)) +\n    geom_point(aes(colour=gender)) +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\")\n\n\n\n\n\n\n\n# 다른 산점도로 성별 구분\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight, colours=gender)) +\n    geom_point(aes(colour=gender)) +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\") +\n    facet_wrap(~gender)\n\n\n\n\n\n\n\n# 성별 상관없는 회귀직선\nGaltonFamilies |&gt; \n  ggplot(aes(midparentHeight, childHeight)) +\n    geom_point() +\n    stat_smooth(method=\"lm\") +\n    xlab(\"Average Height of the Parents (in inches)\") +\n    ylab(\"Height of the Child (in inches)\")\n선형대수로 회귀계수를 추정하는 문제를 풀면 다음과 같이 정의된다. 한번 미분해서 \\(\\nabla f(\\beta ) = -2Xy + X^t X \\beta =0\\) 0으로 놓고 푼 값은 최소값이 되는데 이유는 \\(\\beta\\)에 대해서 두번 미분하게 되면 \\(2 X^t X\\) 로 양수가 되기 때문이다.\n\\[f(\\beta ) = ||y - \\beta X ||^2 = (y - \\beta X)^t (y - \\beta X) = y^t y - 2 y^t X^t \\beta + \\beta^t X^t X \\beta\\]\n\\[\\nabla f(\\beta ) = -2Xy + X^t X \\beta\\]\n\\[\\beta = (X^t X)^{-1} X^t y \\]\n위에서 정의된 방식으로 수식을 정의하고 이를 R로 코딩하면 회귀계수를 다음과 같이 구할 수 있다.\n## 2. 회귀분석\n# 선형대수 수식으로 계산\n\nx &lt;- GaltonFamilies$midparentHeight\ny &lt;- GaltonFamilies$childHeight\n\nx &lt;- cbind(1, x)\n\nsolve(t(x) %*% x) %*% t(x) %*% y\n#&gt;         [,1]\n#&gt;   22.6362405\n#&gt; x  0.6373609\n이를 lm 함수를 사용해서 다시 풀면 위에서 선형대수 수식으로 계산한 것과 동일함을 확인하게 된다.\n# lm 함수를 통해 계산\n\nlm(childHeight ~ midparentHeight, data=GaltonFamilies) %&gt;% coef()\n#&gt;     (Intercept) midparentHeight \n#&gt;      22.6362405       0.6373609",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>모형</span>"
    ]
  },
  {
    "objectID": "models.html#가내수공업-모형개발",
    "href": "models.html#가내수공업-모형개발",
    "title": "\n12  모형\n",
    "section": "\n12.2 가내수공업 모형개발",
    "text": "12.2 가내수공업 모형개발\n데이터 과학 분야의 제품 개발 방식은 다양하다. 엔지니어링의 관점에서 볼 때, 전통적인 장인의 기술이 제자에게 계승되는 방식에서 시작하여, 포드의 대량 생산 방식을 거치고, 대량 맞춤생산(Mass Customization) 방식으로 발전하여, 현재에는 기계 학습과 딥러닝이 통합된 혁신적인 개발 방식까지 다양한 방법론이 혼재되어 있다.\n전통적인 가내수공업 방식은 개별 주문에 따라 제품을 최적화하여 만드는 방식이다. 이 방식에서는 인간의 경험과 지식이 중요한 역할을 하며, 고객의 특별한 요구사항을 충족하기 위해 맞춤형 모델을 개발한다. 이러한 방식을 간략히 살펴보면, 각 고객의 고유한 필요에 따라 제품이나 서비스를 특별히 설계하고 생산하는 것이 핵심이다.\n\n##========================================================\n## 01. 데이터 준비\n##========================================================\n## 모의시험 데이터 생성\n\nx &lt;- seq(1, 100,1)\ny &lt;- x**2 + jitter(x, 1000)\n\ndf &lt;- tibble(x,y)\nhead(df)\n#&gt; # A tibble: 6 × 2\n#&gt;       x        y\n#&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1  188.   \n#&gt; 2     2  -21.3  \n#&gt; 3     3    0.695\n#&gt; 4     4   53.9  \n#&gt; 5     5   18.8  \n#&gt; 6     6 -133.\n\n##========================================================\n## 02. 탐색적 데이터 분석\n##========================================================\n# 통계량\npsych::describe(df)\n#&gt;   vars   n    mean      sd  median trimmed     mad     min      max    range\n#&gt; x    1 100   50.50   29.01   50.50   50.50   37.06    1.00   100.00    99.00\n#&gt; y    2 100 3425.49 3068.05 2648.97 3131.02 3372.75 -133.05 10124.63 10257.68\n#&gt;   skew kurtosis    se\n#&gt; x  0.0    -1.24   2.9\n#&gt; y  0.6    -0.94 306.8\n\n# 산점도\nplot(x, y)\n\n\n\n\n\n\n\n##========================================================\n## 03. 모형 적합\n##========================================================\n\n#---------------------------------------------------------\n# 3.1. 선형회귀 적합\nlm.m &lt;- lm(y ~ x, data=df)\nsummary(lm.m)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -999.1 -657.3 -174.1  508.3 1839.3 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -1753.916    151.479  -11.58   &lt;2e-16 ***\n#&gt; x             102.562      2.604   39.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 751.7 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.9406, Adjusted R-squared:   0.94 \n#&gt; F-statistic:  1551 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,2))\n# 적합모형 시각화\nplot(x,y, data=df, cex=0.7)\nabline(lm.m, col='blue')\n\n# 잔차 \nplot(resid(lm.m))\nabline(h=0, type='3', col='blue')\n\n\n\n\n\n\n\n#---------------------------------------------------------\n# 3.2. 비선형회귀 적합\n# 비선형회귀적합\ndf$x2 &lt;- df$x**2\n\nnlm.m &lt;- lm(y ~ x + x2, data=df)\nsummary(nlm.m)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x + x2, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -194.635  -90.885    6.228   78.164  241.810 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -57.66682   33.82317  -1.705   0.0914 .  \n#&gt; x             2.78312    1.54581   1.800   0.0749 .  \n#&gt; x2            0.98791    0.01483  66.623   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 110.5 on 97 degrees of freedom\n#&gt; Multiple R-squared:  0.9987, Adjusted R-squared:  0.9987 \n#&gt; F-statistic: 3.811e+04 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,2))\n# 적합모형 시각화\nplot(x, y, data=df, cex=0.7)\nlines(x, fitted(nlm.m), col='blue')\n# 잔차 \nplot(resid(nlm.m), cex=0.7)\nabline(h=0, type='3', col='blue')\n\n\n\n\n\n\n\n데이터 준비단계에서 모의시험 데이터를 생성한다. 독립변수 \\(x\\)는 1부터 100까지의 연속적인 수열이며, 종속변수 \\(y\\)는 \\(x^2\\)에 노이즈를 추가한 값으로 정의한다. 생성데이터는 df라는 데이터프레임에 저장하며, 데이터 처음 몇 행을 출력하여 확인한다.\n탐색적 데이터 분석 (EDA) 단계에서 데이터의 기술통계량을 psych 패키지 describe 함수를 활용하여 출력하고, 데이터 분포와 관계를 확인하기 위해 \\(x\\)와 \\(y\\)의 산점도로 시각화한다.\n모형 적합 단계에선 먼저, 선형회귀 모델을 사용하여 \\(y\\)를 \\(x\\)로 예측한다. 적합된 선형 모형 결과를 출력하고, 적합된 모델과 잔차를 시각적으로 확인한다. \\(y = \\beta_0 + \\beta_1 x + \\beta_1 x^2\\) 형태를 가진 2차 회귀 모형을 적합시킨다. \\(x^2\\) 항을 데이터프레임에 추가해서 2차 회귀 모델을 적합한 후, 결과를 출력하고, 적합된 모형과 잔차를 시각적으로 확인한다.\n\n\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science. Leanpub.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of Statistics and Data Visualization.",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>모형</span>"
    ]
  },
  {
    "objectID": "models_building.html",
    "href": "models_building.html",
    "title": "\n13  모형 개발\n",
    "section": "",
    "text": "13.1 기계학습/회귀모형 구성요소\n일반 모형을 “신호 + 잡음(signal + noise)”로 가정하고 다음과 같은 수식으로 표현할 수 있다.\n\\[y = f(x) + \\epsilon\\]\n결국, 잡음이 낀 데이터에서 잡음을 제거하고 신호만 뽑아내는 것이 회귀모형, 기계학습 모형이라고 볼 수 있다. 회귀모형과 기계학습 모형은 회귀모형이 특정 함수형태를 가정하고 데이터에서 신호와 잡음을 구부하는데 초점이 과거 맞춰졌다면, 기계학습모형은 \\(x\\)는 \\(y\\)의 인과관계를 가정으로 놓고 신호와 잡음을 가장 잘 발라낼 수 있는 함수를 찾아내는데 초점을 두고 있다.",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#ml-basic-elements",
    "href": "models_building.html#ml-basic-elements",
    "title": "\n13  모형 개발\n",
    "section": "",
    "text": "출력 : \\(y\\), 관심갖고 있는 결과변수\n입력 : \\(x\\), 설명/예측 변수\n\n\n\\(y\\)의 변동성을 설명하는 목적의 모형을 구축하는 경우 \\(x\\)는 설명변수\n\n\\(y\\)의 변동성을 예측하는 목적의 모형을 구축하는 경우 \\(x\\)는 예측변수\n\n\n가설: : \\(g: x \\rightarrow y\\), \\(x\\)는 \\(y\\)에 영향을 주는 인과관계가 존재한다.\n목적함수 : \\(f: x \\rightarrow y\\), \\(y\\)와 \\(x\\)를 연관시켜주는 함수\n데이터: \\((x_1 , y_1 ), (x_2 , y_2 ), \\dots, (x_n , y_n )\\)\n\n오차: \\(\\epsilon\\), \\(f: x \\rightarrow y\\)으로 설명되지 않는 부분\n\n\n\n\n기계학습 도해",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#대-기계학습-원리",
    "href": "models_building.html#대-기계학습-원리",
    "title": "\n13  모형 개발\n",
    "section": "\n13.2 3대 기계학습 원리",
    "text": "13.2 3대 기계학습 원리\n기계학습 알고리즘 개발자가 데이터를 학습시켜 기계학습 알고리즘을 뽑아내는 과정에 3대 기계학습 원리가 적용된다. 1\n\n\n오컴의 면도날(Occam’s Razor): 사고 절약의 원리(Principle of Parsimony)라고도 불리며, 같은 현상을 설명하는 두가지 모형이 있다면, 단순한 모형을 선택한다.\n표집 편향(Sampling Bias): 모집단을 대표성의 원리에 따라 표본을 추출하지 못할 때, 기계학습 알고리즘도 편향된 표본을 학습하여 결과를 왜곡시킨다.\n데이터 염탐 편향(Data Snooping Bias): 데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습 알고리즘을 선정해야 된다.\n\n\n\n3대 기계학습 원리\n\n\n오컴의 면도날\n\n동일한 조건이면 더 단순한 것을 선택하는 것으로, 가장 큰 이유는 갖고 있는 데이터를 벗어나 새로운 데이터를 갖게 될 경우 학습시킨 기계학습 알고리즘이 더 좋은 성능을 보인다는 것이다. 결국 수많은 가능한 모형중에서 하나를 선택하는 기준이 된다.\n\nAn explanation of the data should be made as simple as possible, but no simpler – Albert Einstein\n\n\n표집 편향\n\n1948년 미국 대통령선거에서 트루먼이 듀이 후보를 물리치고 대통령이 된 것은 알려진 사실이다. 하지만, 대부분의 여론조사에서 듀이의 승리를 예상했지만, 사실은 그 반대로 나타났다. 그 당시 여론조사를 전화기를 사용하였는데, 문제는 전화기가 부유층이 많이 소유하고 있어 미국 대통령선거 모집단을 대표하는 대표성에 문제가 있어 왜곡된 결과가 도출된 것이다.\n상업적으로 개인금융의 신용카드발급, 신용평가에도 동일한 문제가 발생한다. 사실 수익성은 저신용자가 높아 이를 살펴보면, 신용평가에 사용될 데이터는 저신용자는 카드를 발급받을 수 없어 데이터베이스에는 표집편향된 고객정보만 존재하는 것을 어렵지 않게 볼 수 있다.\n\nIf the data is sampled in a biased way, learning will produce a similarly biased outome.\n\n\n데이터 염탐 편향\n\n데이터를 본 후에 기계학습 알고리즘을 결정하는 것으로, 사실 데이터를 보기 전에 기계학습 알고리즘을 선정해야 하지만, 현실적으로 현업에서 작업하는 사람들이 흔히 범하는 실수다. 동일한 데이터에 대해 갖가지 기계학습 알고리즘을 적용해서 가장 좋은 성능이 나오는 알고리즘을 선정한다. 문제는 데이터가 바뀌면 어떨까? 아마 기대했던 성능이 나오지 못할 가능성이 크다.\n\nIf you torture the data long enough, it will confess",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#모형-개발과정",
    "href": "models_building.html#모형-개발과정",
    "title": "\n13  모형 개발\n",
    "section": "\n13.3 모형 개발과정",
    "text": "13.3 모형 개발과정\n통계모형 개발과정은 데이터 과학 프로세스에서 크게 차이가 나지 않는다. 다만, 일반적인 통계모형을 개발할 경우 다음과 같은 과정을 거치게 되고, 지난한 과정이 될 수도 있다.\n\n데이터를 정제하고, 모형에 적합한 데이터(R/파이썬과 모형 팩키지와 소통이 될 수 있는 데이터형태)가 되도록 준비한다.\n변수에 대한 분포를 분석하고 기울어짐이 심한 경우 변수변환도 적용한다.\n변수와 변수간에, 종속변수와 설명변수간에 산점도와 상관계수를 계산한다. 특히 변수간 상관관계가 \\(r &gt; 0.9\\) 혹은 근처인 경우 변수를 빼거나 다른 방법을 강구한다.\n동일한 척도로 회귀계수를 추정하고 평가하려는 경우, scale() 함수로 척도로 표준화한다.\n모형을 적합시킨 후에 잔차를 보고, 백색잡음(whitenoise)인지 확인한다. 만약, 잔차에 특정한 패턴이 보이는 경우 패턴을 잡아내는 모형을 새로 개발한다.\n\n\nplot() 함수를 사용해서 이상점이 있는지, 비선형관계를 잘 잡아냈는지 시각적으로 확인한다.\n다양한 모형을 적합시키고 R^2 와 RMSE, 정확도 등 모형평가 결과가 가장 좋은 것을 선정한다.\n절약성의 원리(principle of parsimony)를 필히 준수하여 가장 간결한 모형이 되도록 노력한다.2\n\n\n\n최종 모형을 선택하고 모형에 대한 해석결과와 더불어 신뢰구간 정보를 넣어 마무리한다.\n\n\n\n\n\n\n\n키보드로 통계모형 표현법\n\n\n\n수학적 표현을 프로그래밍 언어(R, 파이썬 등)로 전환하는 필요성은 키보드 입력의 제한성 때문에 발생한다. 키보드 특수문자를 최적으로 활용하여, R에서의 구현은 아래와 같이 가장 읽기 쉽고 입력하기 편리한 형식으로 다음과 같이 정리할 수 있다.\n\n주효과에 대해 변수를 입력으로 넣을 +를 사용한다.\n교호작용을 변수간에 표현할 때 :을 사용한다. 예를 들어 x*y는 x+y+x:z와 같다.\n모든 변수를 표기할 때 .을 사용한다.\n종속변수와 예측변수를 구분할 때 ~을 사용한다. y ~ .은 데이터프레임에 있는 모든 변수를 사용한다는 의미다.\n특정변수를 제거할 때는 -를 사용한다. y ~ . -x는 모든 예측변수를 사용하고, 특정 변수 x를 제거한다는 의미다.\n상수항을 제거할 때는 -1을 사용한다.\n\n\n\n\n\n\n\n\nR 공식구문\n수학적 표현\n설명\n\n\n\ny~x\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시키는 1차 선형회귀식\n\n\ny~x -1\n\\(y_i = \\beta_1 x_i + \\epsilon_i\\)\n\nx를 y에 적합시 절편 없는 1차 선형회귀식\n\n\ny~x+z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i +\\epsilon_i\\)\n\nx와 z를 y에 적합시키는 1차 선형회귀식\n\n\ny~x:z\n\\(y_i = \\beta_0 + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z 교호작용 항을 y에 적합시키는 1차 선형회귀식\n\n\ny~x*z\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\beta_1 x_i \\times z_i +\\epsilon_i\\)\n\nx와 z, 교호작용항을 y에 적합시키는 1차 선형회귀식",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#과대적합-사례",
    "href": "models_building.html#과대적합-사례",
    "title": "\n13  모형 개발\n",
    "section": "\n13.4 과대적합 사례",
    "text": "13.4 과대적합 사례\n\\(y=x^2 + \\epsilon\\) 오차는 정규분포 평균 0, 표준편차 0.2를 갖는 모형을 따른다고 가정하고, 이를 차수가 높은 다항식을 사용하여 적합시킨 결과를 확인하는 절차는 다음과 같다.\n\n\ntidyr, modelr, ggplot2 팩키지를 불러와서 환경을 설정한다.\n\n\\(y=x^2 + \\epsilon\\), 오차는 \\(N(0, 0.25)\\)을 따르는 모형을 생성하고, df 데이터프레임에 결과를 저장한다.\n\npoly_fit_model 함수를 통해 7차 다항식으로 적합시킨다.\n적합결과를 ggplot을 통해 시각화한다.\n\n1차에서 10차까지 차수를 달리하여 적합시켜 시각적으로 적합도를 확인할 수 있다.\n\n#--------------------------------------------------------------------------------\n# 01. 환경설정\n#--------------------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(modelr)\n\n#--------------------------------------------------------------------------------\n# 02. 참모형 데이터 생성: y = x**2\n#--------------------------------------------------------------------------------\n\ntrue_model &lt;- function(x) {\n  y = x ** 2 + rnorm(length(x), sd=0.25)\n  return(y)\n}\n\nx &lt;- seq(-1, 1, length=20)\ny &lt;- true_model(x)\ndf &lt;- tibble(x,y)\n\n#--------------------------------------------------------------------------------\n# 03. 10차 다항식 적합\n#--------------------------------------------------------------------------------\n\npoly_fit_model &lt;- function(df, order) {\n  lm(y ~ poly(x, order), data=df)\n}\n\n# fitted_mod &lt;- poly_fit_model(df, 10)\n\n#--------------------------------------------------------------------------------\n# 04. 적합결과 시각화\n#--------------------------------------------------------------------------------\n\nfitted_tbl &lt;- tibble(차수 = 1:10) |&gt; \n  mutate(데이터 = list(df)) |&gt; \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |&gt; \n  mutate(예측  = map(적합된모형, fitted)) |&gt; \n  mutate(x = list(seq(-1, 1, length=20))) |&gt;\n  mutate(fitted_df = map2(x, 예측, ~tibble(x_value = .x, \n                                         prediction = .y, \n                                         .name_repair = \"unique\"))) |&gt; \n  select(차수, fitted_df) |&gt; \n  mutate(차수 = as.factor(차수)) |&gt;   \n  unnest(fitted_df) \n\n\ndf %&gt;% \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = fitted_tbl, aes(x = x_value, y = prediction, color = 차수, group = 차수))\n\n\n\n\n\n\n\n다항식 차수를 달리하여 데이터에 1~10차 다항식을 적합시킨 후에 오차를 계산했다. 1차보다 2차 다항식으로 적합시킬 때 오차가 확연히 줄어들지만 그 이후에는 오차의 감소폭이 크지는 않다. 물론 오차는 다항식 차수를 높일수록 낮아지지만 절약성의 원칙을 생각하면 이와 같은 고차 다항식 함수가 필요한지는 의문이다.\n\ntibble(차수 = 1:10) |&gt; \n  mutate(데이터 = list(df)) |&gt; \n  mutate(적합된모형 = map2(데이터, 차수, poly_fit_model)) |&gt; \n  mutate(예측  = map(적합된모형, fitted)) |&gt; \n  mutate(true_y = map(데이터, 2)) |&gt;\n  # 참값과 모형예측값\n  mutate(error_df = map2(true_y, 예측, ~tibble(true_y = .x,  fitted_y = .y))) |&gt; \n  # 오차\n  mutate(rmse = map_dbl(error_df, ~ sqrt(mean((.x$true_y - .x$fitted_y)^2)))) |&gt; \n  mutate(차수 = factor(차수)) |&gt; \n  mutate(color = c(\"gray30\", \"blue\", rep(\"gray30\", 8))) |&gt; \n  # 시각화\n  ggplot(aes(x = fct_rev(차수), y = rmse, fill = color)) +\n    geom_col(width = 0.3) +\n    coord_flip() +\n    labs( x = \"차수\",\n          y = \"RMSE 오차\",\n          title = \"다항식 차수를 달리하여 계산한 RMSE 오차\") +\n    scale_fill_manual(values = c(\"blue\", \"gray30\", rep(\"blue\", 8))) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n기계학습 지도\n기계학습 도해\n3대 기계학습 원리\n\n\n\nAbu-Mostafa, Yaser S, Malik Magdon-Ismail, 와/과 Hsuan-Tien Lin. 2012. Learning from data. Vol 4. AMLBook New York.",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_building.html#footnotes",
    "href": "models_building.html#footnotes",
    "title": "\n13  모형 개발\n",
    "section": "",
    "text": "Caltech MOOC, Yaser Abu-Mostafa, Introductory Machine Learning, 2012↩︎\n간결성 원칙으로도 번역되며, 오캄의 면도날(“Occam’s razor”)라는 이름으로도 알려져 있으며, 연구나 문제 해결의 맥락에서 가장 단순한 설명이나 가설을 우선적으로 고려해야 한다는 의미로, 두 개 이상의 설명이 관찰된 현상을 동등하게 설명할 수 있을 때, 더 적은 가정이 필요한 또는 더 단순한 설명을 선호해야 한다는 것이다.↩︎",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>모형 개발</span>"
    ]
  },
  {
    "objectID": "models_many.html",
    "href": "models_many.html",
    "title": "14  많은 회귀모형",
    "section": "",
    "text": "14.1 기대수명 데이터\ngapminder 데이터를 가지고 회귀모형을 구축하고 모형을 활용하여 종속변수(기대수명, lifeExp)가 늘어나지 못한 국가를 뽑아내고 이를 시각적으로 확인해보자.\n# 0. 환경설정 -----\n# 데이터\nlibrary(gapminder)\n\n# Tidyverse\nlibrary(tidyverse)\n\n# 모형\nlibrary(broom)\n\n# 1. 데이터 -----\ngapminder\n#&gt; # A tibble: 1,704 × 6\n#&gt;    country     continent  year lifeExp      pop gdpPercap\n#&gt;    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n#&gt;  1 Afghanistan Asia       1952    28.8  8425333      779.\n#&gt;  2 Afghanistan Asia       1957    30.3  9240934      821.\n#&gt;  3 Afghanistan Asia       1962    32.0 10267083      853.\n#&gt;  4 Afghanistan Asia       1967    34.0 11537966      836.\n#&gt;  5 Afghanistan Asia       1972    36.1 13079460      740.\n#&gt;  6 Afghanistan Asia       1977    38.4 14880372      786.\n#&gt;  7 Afghanistan Asia       1982    39.9 12881816      978.\n#&gt;  8 Afghanistan Asia       1987    40.8 13867957      852.\n#&gt;  9 Afghanistan Asia       1992    41.7 16317921      649.\n#&gt; 10 Afghanistan Asia       1997    41.8 22227415      635.\n#&gt; # ℹ 1,694 more rows",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>많은 회귀모형</span>"
    ]
  },
  {
    "objectID": "models_many.html#model-reg-viz-data",
    "href": "models_many.html#model-reg-viz-data",
    "title": "14  많은 회귀모형",
    "section": "",
    "text": "14.1.1 기대수명 회귀분석\n대륙과 국가를 그룹으로 잡아 회귀분석을 각각에 대해서 돌리고 나서, 모형 결과값을 데이터와 모형이 함께 위치하도록 티블(tibble)에 저장시켜 놓은다. 그리고 나서, 주요한 회귀모형 성능지표인 결정계수(\\(R^2\\))를 기준으로 정렬시킨다.\n\n# 2. 모형 -----\ncountry_model &lt;- function(df)\n  lm(lifeExp ~ year, data=df)\n\nby_country &lt;- gapminder %&gt;% \n  group_by(country, continent) %&gt;% \n  nest() %&gt;% \n  mutate(model = map(data, country_model),\n         model_glance = map(model, glance),\n         rsquare = map_dbl(model_glance, ~.$r.squared)) |&gt; \n  ungroup()\n\nby_country %&gt;% \n  arrange(rsquare)\n#&gt; # A tibble: 142 × 6\n#&gt;    country          continent data              model  model_glance      rsquare\n#&gt;    &lt;fct&gt;            &lt;fct&gt;     &lt;list&gt;            &lt;list&gt; &lt;list&gt;              &lt;dbl&gt;\n#&gt;  1 Rwanda           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0172\n#&gt;  2 Botswana         Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0340\n#&gt;  3 Zimbabwe         Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0562\n#&gt;  4 Zambia           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0598\n#&gt;  5 Swaziland        Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0682\n#&gt;  6 Lesotho          Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.0849\n#&gt;  7 Cote d'Ivoire    Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.283 \n#&gt;  8 South Africa     Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.312 \n#&gt;  9 Uganda           Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.342 \n#&gt; 10 Congo, Dem. Rep. Africa    &lt;tibble [12 × 4]&gt; &lt;lm&gt;   &lt;tibble [1 × 12]&gt;  0.348 \n#&gt; # ℹ 132 more rows\n\n\n14.1.2 회귀모형 시각화\n데이터셋 by_country를 이용하여 각 나라별로 회귀 모델의 $ R^2 $ (결정 계수) 값을 기반으로 “적합국”과 “비적합국”을 분류하고, 그 결과를 시각화한다. $ R^2 $ 값이 큰 나라 5개와 작은 나라 5개를 추출하여 기대수명 변화를 그래프로 대조하여 국가별 차이를 명확히 한다.\n\n\n최대 $ R^2 $ 값의 국가 추출:\n\n\nby_country 데이터셋에서 rsquare 값이 가장 큰 5개의 국가를 추출한다.\n해당 국가 이름들을 rsquare_max_countries에 저장한다.\n\n\n\n최소 $ R^2 $ 값의 국가 추출:\n\n\nby_country 데이터셋에서 rsquare 값이 가장 작은 5개의 국가를 추출한다.\n해당 국가 이름들을 rsquare_min_countries에 저장한다.\n\n\n\n데이터 필터링 및 시각화:\n\n총 10개 국가에 해당하는 데이터만 by_country에서 추출한다.\n추출된 데이터에서 나라명, 대륙명, $ R^2 $ 값, 그리고 원데이터를 추출한다.\n결과를 $ R^2 $ 값의 내림차순으로 정렬한다.\n데이터를 정리하여 각 나라의 연도별 기대수명을 나타내는 선그래프를 생성하고, 그래프에서는 $ R^2 $ 값이 높은 국가들을 “발전된국가”로, 낮은 값을 가진 국가들을 “개발국”으로 분류하여 색상을 달리하여 시각화할 재료로 준비한다.\n\n\n\n최종적으로는 각 국가별로 연도에 따른 기대수명 변화를 보여주는 그래프를 생성한다. $ R^2 $ 값이 높은 국가들은 빨간색으로, 낮은 국가들은 파란색으로 표시한고, 마지막으로 회귀모형이 얼마나 잘 적합되었는지에 따라 각 국가의 기대수명 변화 패턴을 비교한다. 파란색으로 회귀모형이 잘 적합된 경우에도 서로 다른 패턴이 확인된다. 즉, 선진국과 개발도상국 모두 제2차 세계대전 이후 기대수명이 증대했으나 선진국은 높은 기대수명에서 개도국은 낮은 기대수명에서 시작해서 모두 기대수명이 높아진 것이 눈에 띈다. 하지만, 빨간색으로 회귀계수가 낮은 나라는 기대수명이 높아지다가 특정 사건으로 인해 기대수명이 제자리로 돌아온 이후 다시 기대수명이 높아지는 추세를 보여 비선형적 관계를 보여주고 있어 회귀계수가 전반적으로 낮게 나타났다.\n\nrsquare_max_countries &lt;- by_country |&gt; \n  slice_max(order_by = rsquare, n = 5)  |&gt; \n  pull(country) |&gt; \n  droplevels()\n\nrsquare_min_countries &lt;- by_country |&gt; \n  slice_min(order_by = rsquare, n = 5) |&gt; \n  pull(country) |&gt; \n  droplevels()\n\nby_country |&gt; \n  filter(country %in% rsquare_max_countries | country %in% rsquare_min_countries) |&gt; \n  select(country, continent, rsquare, data) |&gt; \n  arrange(desc(rsquare)) |&gt; \n  unnest(data) |&gt; \n  mutate(country = factor(country, levels = c(rsquare_max_countries, rsquare_min_countries))) |&gt; \n  mutate(class = if_else(country %in% rsquare_max_countries, \"발전된국가\", \"개발국\")) |&gt; \n  ggplot(aes(x = year, y = lifeExp, color = class)) +\n    geom_line() +\n    facet_wrap(~country, nrow = 2) +\n    scale_color_manual(values = c(\"red\", \"blue\")) +\n    theme_minimal() +\n    theme(legend.position = \"none\",\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    labs(title =\"회귀모형 기대수명 적합국과 비적합국\",\n         x = \"\",\n         y = \"기대수명\",\n         caption = \"자료출처: gapminder\")",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>많은 회귀모형</span>"
    ]
  },
  {
    "objectID": "models_many.html#모형식별",
    "href": "models_many.html#모형식별",
    "title": "14  많은 회귀모형",
    "section": "\n14.2 모형식별",
    "text": "14.2 모형식별\npurrr 팩키지를 활용하여 원본 모형 데이터와 모형을 하나의 데이터프레임(tibble)에 담을 수가 있다. 즉, 6가지 서로 다른 회귀모형을 일괄적으로 적합시키고 가장 AIC 값이 적은 회귀모형을 선택하는 코드를 다음과 같이 작성한다. 1\n\n\nreg_models: 다양한 회귀모형을 정의한다.\n\nmutate(map()): 정의한 회귀모형 각각을 적합시키고 모형성능 지표를 추출한다.\nAIC 기준으로 가장 낮은 모형을 선정한다.\n\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n## 데이터셋 준비 -----\ngapminder &lt;- gapminder %&gt;%\n  set_names(colnames(.) %&gt;% tolower())\n\n## 다양한 회귀모형 -----\nreg_models &lt;- list(\n  `01_pop` = 'lifeexp ~ pop',\n  `02_gdppercap` = 'lifeexp ~ gdppercap',\n  `03_simple` = 'lifeexp ~ pop + gdppercap',\n  `04_medium` = 'lifeexp ~ pop + gdppercap + continent + year',\n  `05_more`   = 'lifeexp ~ pop + gdppercap + country + year',\n  `06_full`   = 'lifeexp ~ pop + gdppercap + year*country')\n\nmodel_tbl &lt;- tibble(reg_formula = reg_models) %&gt;%\n  mutate(model_name = names(reg_formula)) %&gt;% \n  select(model_name, reg_formula) %&gt;% \n  mutate(reg_formula = map(reg_formula, as.formula))\n\nmodel_tbl\n#&gt; # A tibble: 6 × 2\n#&gt;   model_name   reg_formula \n#&gt;   &lt;chr&gt;        &lt;named list&gt;\n#&gt; 1 01_pop       &lt;formula&gt;   \n#&gt; 2 02_gdppercap &lt;formula&gt;   \n#&gt; 3 03_simple    &lt;formula&gt;   \n#&gt; 4 04_medium    &lt;formula&gt;   \n#&gt; 5 05_more      &lt;formula&gt;   \n#&gt; 6 06_full      &lt;formula&gt;\n\n## 회귀모형 적합 및 모형 성능 지표 -----\nmodel_tbl &lt;- model_tbl %&gt;%\n  mutate(fit = map(reg_formula, ~lm(., data = gapminder))) %&gt;% \n  mutate(model_glance = map(fit, broom::glance),\n         rsquare      = map_dbl(model_glance, ~.$r.squared),\n         AIC          = map_dbl(model_glance, ~.$AIC)) %&gt;% \n  arrange(AIC)\n\nmodel_tbl\n#&gt; # A tibble: 6 × 6\n#&gt;   model_name   reg_formula  fit          model_glance      rsquare    AIC\n#&gt;   &lt;chr&gt;        &lt;named list&gt; &lt;named list&gt; &lt;named list&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 06_full      &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.976    7752.\n#&gt; 2 05_more      &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.932    9268.\n#&gt; 3 04_medium    &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.717   11420.\n#&gt; 4 03_simple    &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.347   12836.\n#&gt; 5 02_gdppercap &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.341   12850.\n#&gt; 6 01_pop       &lt;formula&gt;    &lt;lm&gt;         &lt;tibble [1 × 12]&gt; 0.00422 13553.\n\n\n14.2.1 교차검증 CV\n\n데이터를 10조각내서 교차검정을 통해 RMSE가 가장 작은 회귀모형이 어떤 것인지 살펴보자. cross_df() 함수로 교차검증 splits 데이터와 모형을 준비한다. 다음으로 analysis() 함수로 교차검증 데이터에 대해서 회귀모형 각각을 적합시키고, assessment() 함수로 적합시킨 모형에 대해 모형성능을 살펴본다. 마지막으로 RMSE 회귀모형 성능지표를 통해 모형선택을 한다.\n\n## 교차검정 -----\nvalid_tbl &lt;- gapminder %&gt;%\n  rsample::vfold_cv(10)\n\ncv_tbl &lt;- list(test_training = list(valid_tbl), \n               model_name = model_tbl$model_name)  \n  \ncv_tbl &lt;- tidyr::expand_grid(test_training = list(valid_tbl), \n                             model_name = model_tbl$model_name)\n\ncv_tbl &lt;- cv_tbl %&gt;%\n  mutate(model_number = row_number()) %&gt;%  # Manually creating the model_number column\n  left_join(model_tbl %&gt;% select(model_name, reg_formula), by = \"model_name\") %&gt;% \n  unnest(cols = c(test_training))\n\ncv_tbl\n#&gt; # A tibble: 60 × 5\n#&gt;    splits             id     model_name model_number reg_formula \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;named list&gt;\n#&gt;  1 &lt;split [1533/171]&gt; Fold01 06_full               1 &lt;formula&gt;   \n#&gt;  2 &lt;split [1533/171]&gt; Fold02 06_full               1 &lt;formula&gt;   \n#&gt;  3 &lt;split [1533/171]&gt; Fold03 06_full               1 &lt;formula&gt;   \n#&gt;  4 &lt;split [1533/171]&gt; Fold04 06_full               1 &lt;formula&gt;   \n#&gt;  5 &lt;split [1534/170]&gt; Fold05 06_full               1 &lt;formula&gt;   \n#&gt;  6 &lt;split [1534/170]&gt; Fold06 06_full               1 &lt;formula&gt;   \n#&gt;  7 &lt;split [1534/170]&gt; Fold07 06_full               1 &lt;formula&gt;   \n#&gt;  8 &lt;split [1534/170]&gt; Fold08 06_full               1 &lt;formula&gt;   \n#&gt;  9 &lt;split [1534/170]&gt; Fold09 06_full               1 &lt;formula&gt;   \n#&gt; 10 &lt;split [1534/170]&gt; Fold10 06_full               1 &lt;formula&gt;   \n#&gt; # ℹ 50 more rows\n\n## 교차검정 analysis, assessment -----\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y)))) %&gt;%\n  mutate(RMSE = map2_dbl(fit, splits, ~modelr::rmse(.x, rsample::assessment(.y))))\n\ncv_fit_tbl\n#&gt; # A tibble: 60 × 7\n#&gt;    splits             id     model_name model_number reg_formula  fit       RMSE\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;named list&gt; &lt;named &gt; &lt;dbl&gt;\n#&gt;  1 &lt;split [1533/171]&gt; Fold01 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.85\n#&gt;  2 &lt;split [1533/171]&gt; Fold02 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.61\n#&gt;  3 &lt;split [1533/171]&gt; Fold03 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.74\n#&gt;  4 &lt;split [1533/171]&gt; Fold04 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.27\n#&gt;  5 &lt;split [1534/170]&gt; Fold05 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.34\n#&gt;  6 &lt;split [1534/170]&gt; Fold06 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.42\n#&gt;  7 &lt;split [1534/170]&gt; Fold07 06_full               1 &lt;formula&gt;    &lt;lm&gt;      3.83\n#&gt;  8 &lt;split [1534/170]&gt; Fold08 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.19\n#&gt;  9 &lt;split [1534/170]&gt; Fold09 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.13\n#&gt; 10 &lt;split [1534/170]&gt; Fold10 06_full               1 &lt;formula&gt;    &lt;lm&gt;      2.77\n#&gt; # ℹ 50 more rows\n\n## 시각화 -----\ncv_fit_tbl %&gt;%\n  ggplot(aes(RMSE, fill = model_name)) +\n  geom_density(alpha = 0.75) +\n  labs(x = \"RMSE\", title = \"gapminder 회귀모형별 교차검정 분포\")\n\n\n\n\n\n\n\n\n14.2.2 병렬처리 - furrr\n\nparallel::detectCores()을 통해 전체 코어 숫자를 확인하고 이를 병렬처리를 통해 교차검증에 따른 시간을 대폭 절감시킨다. 이를 위해서 future 팩키지를 사용하고 절약되는 시간을 측정하기 위해서 tictoc 팩키지를 동원한다.\n\nlibrary(furrr)\nlibrary(tictoc)\n\nplan(multisession, workers = parallel::detectCores() - 1)\n\n\n\npurrr 순차처리\n\n## purrr 순차처리 -----\ntic()\n\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y))))\n\ntoc()\n#&gt; 1.21 sec elapsed\n\n\nfurrr 병렬처리\n\n## furrr 병렬처리 ----\ntic()\n\ncv_fit_tbl &lt;- cv_tbl %&gt;%\n  mutate(fit = future_map2(reg_formula, splits, ~lm(.x, data = rsample::analysis(.y)), .progress=TRUE)) \n\ntoc()\n#&gt; 20.93 sec elapsed",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>많은 회귀모형</span>"
    ]
  },
  {
    "objectID": "models_many.html#footnotes",
    "href": "models_many.html#footnotes",
    "title": "14  많은 회귀모형",
    "section": "",
    "text": "DAN OVANDO(FEBRUARY 20, 2018), “DATA WRANGLING AND MODEL FITTING USING PURRR”↩︎",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>많은 회귀모형</span>"
    ]
  },
  {
    "objectID": "models_case_study.html",
    "href": "models_case_study.html",
    "title": "15  사례: 시군 인구증가",
    "section": "",
    "text": "15.1 인구변동이 많은 시군",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>사례: 시군 인구증가</span>"
    ]
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun",
    "href": "models_case_study.html#r-population-by-sigun",
    "title": "15  사례: 시군 인구증가",
    "section": "",
    "text": "15.1.1 데이터 가져오기 및 데이터 전처리\n인구변동이 많은 시군 통계 분석을 위해 필요한 팩키지를 불러 읽어온다. 통계청 KOSIS에서 다운로드 받은 파일을 data 폴더 아래 저장하고 나서, 전처리 작업을 수행한다. 서울특별시를 포함한 광역시에 포함된 군은 데이터 분석에 제외할 것이기 때문에 stringr 정규표현식 기능을 활용하여 깔끔하게 향후 데이터분석을 위한 데이터프레임으로 정리한다.\n\n# 0. 환경설정 -----------\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggpubr)\nextrafont::loadfonts()\n\n# 1. 데이터 가져오기 ---------\n\nkor_dat &lt;- read_excel(\"data/korea-pop-zipf-law.xlsx\", sheet=\"데이터\", skip=1) \n\nkor_dat &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` !=\"전국\") %&gt;% \n  filter(`인구현황별` ==\"전체인구(A)\")\n\n# 2. 데이터 전처리 ---------\n## 2.1. 시군 뽑아내기 -------\n\nsigungu_v &lt;- kor_dat %&gt;% count(`소재지(시군구)별`) %&gt;% \n  pull(`소재지(시군구)별`)\n\nsigungu_v &lt;- sigungu_v[str_detect(sigungu_v, \"시$|군$\")]\n\n## 2.2. 시군 데이터 전처리 -------\n\nkor_df &lt;- kor_dat %&gt;% \n  filter(`소재지(시군구)별` %in% sigungu_v) %&gt;% \n  filter(!is.na(`2016 년`)) %&gt;% group_by(`소재지(시군구)별`) %&gt;% \n  summarise_if(is.numeric, mean) %&gt;% \n  rename(시군 = `소재지(시군구)별`)\n\n## 2.3. 시각화 데이터 변환  -------\nkor_lng_df &lt;- kor_df %&gt;% \n  gather(연도, 인구수, -시군) %&gt;% \n  mutate(연도 = as.numeric(str_extract(연도, \"[0-9]+\")))",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>사례: 시군 인구증가</span>"
    ]
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun-problem",
    "href": "models_case_study.html#r-population-by-sigun-problem",
    "title": "15  사례: 시군 인구증가",
    "section": "\n15.2 문제점",
    "text": "15.2 문제점\n대한민국 시군이 166 이기 때문에 인구변동이 많은 시군을 추출하기 위해서 서울시는 천만명 근처이고 가장 작은 울릉군은 2016년 기준 10,001 명이라 편차가 매우 크다. 따라서, 이를 시각화를 하게 되면 문제점이 한눈에 파악된다.\n\n# 3. 시각화 ----------------\n## 3.1. 시군별 연도별 인구수 변화\nkor_lng_df %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=시군, group=시군)) +\n    geom_point() +\n    geom_line() +\n    theme_pubclean(base_family=\"NanumGothic\") +\n    scale_y_log10(labels=scales::comma) +\n    scale_x_date(date_labels = \"%Y\") +\n    theme(legend.position = \"none\") +\n    labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\")",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>사례: 시군 인구증가</span>"
    ]
  },
  {
    "objectID": "models_case_study.html#r-population-by-sigun-solution",
    "href": "models_case_study.html#r-population-by-sigun-solution",
    "title": "15  사례: 시군 인구증가",
    "section": "\n15.3 FP 통한 문제해결",
    "text": "15.3 FP 통한 문제해결\n이러한 문제점에 대해 가장 많이 활용되는 기법이 자료구조로 티블(tibble)을 도입하고, 데이터 분석을 위한 방법으로 함수형 프로그래밍을 조합하는 것이다.\n\n15.3.1 티블 자료구조\n데이터프레임을 기존 폭넓은(wide) 형태를 긴(long) 형태로 변환하고 이를 nest()를 적용시키면 함수형 프로그램을 적용시킬 수 있는 자료구조가 된다. 더불어, 선형회귀모형을 각 시군별로 적용시킬 예정이라 회귀모형 함수도 생성시켜 둔다.\n그리고 나서 전체 시군별로 연도별 인구변화를 회귀분석으로 수행하여 수행결과를 broom 팩키지의 tidy, glance, augment 함수를 활용하여 데이터와 모형분석결과를 결합시킨다.\n\n# 4. 인구변화 심한 시군 추출 ----------------\n## 4.1. 데이터 \nkor_sigun_tb &lt;- kor_lng_df %&gt;% group_by(시군) %&gt;% \n  nest()\n\n## 4.2. 모형 - 선형회귀모형\nsigun_model &lt;- function(df) {\n  lm(인구수 ~ 연도, data=df)\n}\n\n## 4.3. 데이터 + 모형 결합\nkor_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  mutate(model = map(data, sigun_model)) %&gt;% \n  mutate(\n    tidy    = map(model, broom::tidy),\n    glance  = map(model, broom::glance),\n    결정계수 = glance %&gt;% map_dbl(\"r.squared\"),\n    augment = map(model, broom::augment)\n  )\n\nDT::datatable(kor_sigun_tb)\n\n\n\n\n\n\n15.3.2 급성장 시군 - 결정계수(\\(R^2\\))\n회귀분석 결정계수(\\(R^2\\))기준 인구가 연도별로 높은 상관관계를 갖는 시군과 그렇지 않는 상위 하위 5개 시군을 뽑아 시각화 해보자.\n\n## 4.4. 고성장 및 정체 시군 추출\n\nkor_high_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"고성장\")\n\nkor_low_growth_sigun_tb &lt;- kor_sigun_tb %&gt;% \n  top_n(-5, 결정계수) %&gt;% \n  unnest(data) %&gt;% \n  mutate(구분=\"저성장\")\n\nkor_growth_sigun_tb &lt;- bind_rows(kor_high_growth_sigun_tb, kor_low_growth_sigun_tb)\n\n## 4.5. 고성장 및 정체 시군 시각화\n\nkor_growth_sigun_tb %&gt;% \n  mutate(구분 = factor(구분),\n         시군 = fct_reorder(시군, 결정계수)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=시군)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_log10(labels=scales::comma) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~시군, nrow=2, scale=\"free\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  scale_x_date(date_labels = \"%Y\")\n\n\n\n\n\n\n\n\n15.3.3 급성장 시군 - 회귀계수\n회귀분석 결정계수(\\(R^2\\))기준이 아닌 회귀계수(\\(\\beta_1\\))를 기준으로 상위 5개, 하위 5개 시군을 뽑아 연도별 인구변화를 시각화해보자.\n\n\n# 5. 인구 증가 및 인구감소 ----------------\nkor_sigun_reg_df &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0)\n\ntop10_plus_sigun &lt;- kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(5, 증감) %&gt;% \n  pull(시군)\n\ntop10_minus_sigun &lt;-kor_sigun_tb %&gt;% \n  mutate(증감 = map(tidy, \"estimate\")) %&gt;% \n  unnest(증감) %&gt;% \n  filter(row_number() %% 2 == 0) %&gt;% \n  top_n(-5, 증감) %&gt;% \n  pull(시군)\n\nkor_lng_df %&gt;% \n  left_join(kor_sigun_reg_df, by=\"시군\") %&gt;% \n  filter(시군 %in% c(top10_plus_sigun, top10_minus_sigun)) %&gt;%\n  mutate(구분 = ifelse(시군 %in% top10_plus_sigun, \"성장\", \"역성장\"),\n         시군 = fct_reorder(시군, -증감)) %&gt;% \n  mutate(연도 = make_date(year=연도)) %&gt;% \n  ggplot(aes(x=연도, y=인구수, color=구분, group=구분)) +\n  geom_point() +\n  geom_line() +\n  theme_pubclean(base_family=\"NanumGothic\") +\n  scale_y_continuous(labels=scales::comma) +\n  scale_x_date(date_labels = \"%Y\") +\n  theme(legend.position = \"none\") +\n  labs(x=\"\", y=\"인구수\", title=\"시군 인구수 년도별 변화(2005 - 2016)\") +\n  facet_wrap(~시군, scale=\"free\", nrow=2)",
    "crumbs": [
      "**4부** 모형",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>사례: 시군 인구증가</span>"
    ]
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "\n16  함수\n",
    "section": "",
    "text": "16.1 함수 기본 지식\n함수는 입력값(x)를 넣어 어떤 작업(f)을 수행한 결과를 반환(y) 과정으로 이해할 수 있는데, 인자로 다양한 값을 함수에 넣을 수 있고, 물론 함수가 뭔가 유용한 작업을 수행하기 위한 전제조건을 만족시키는지 확인하는 과정을 assert 개념을 넣어 확인하고 기술된 작업을 수행한 후에 출력값을 변환시키게 된다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#all-about-function",
    "href": "functions.html#all-about-function",
    "title": "\n16  함수\n",
    "section": "",
    "text": "데이터 과학 함수 개념",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#how-to-use-function",
    "href": "functions.html#how-to-use-function",
    "title": "\n16  함수\n",
    "section": "\n16.2 함수 사용법",
    "text": "16.2 함수 사용법\n본격적으로 함수를 작성하기 전에 먼저, 함수를 사용하는 방법을 익히는 것이 필요하다. 함수는 함수명, 인자(argument), 함수 몸통(body), 반환값(return value)으로 구성된다.\n데이터 과학 대표 언어 R과 파이썬으로 4칙연산을 구현하는 함수를 작성하여 자세히 살펴보자.\n\n16.2.1 R 함수\n\n함수명: 함수명을 먼저 적고 &lt;-, function(), {, } 순으로 R이 함수임을 알 수 있도록 전개한다.\n함수 인자: 함수에 넣을 인자를 정의하여 넣어 둔다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 명시할 수도 있고, 그냥 놔두면 마지막 객체가 자동으로 반환된다.\n\n\nbasic_operation &lt;- function(first, second) {\n  sum_number &lt;- first + second\n  minus_number &lt;- first - second\n  multiply_number &lt;- first * second\n  divide_number &lt;- first / second\n  \n  result &lt;- list(sum_number, minus_number, multiply_number, divide_number)\n  \n  return(result)\n}\n\nbasic_operation(7, 3)\n#&gt; [[1]]\n#&gt; [1] 10\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 21\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 2.333333\n\n\n16.2.2 파이썬 함수\n\n함수 머리(header): def로 함수임을 선언하고, 함수명과 함수인자를 기술, 마지막을 :으로 마무리.\n함수 설명: docstring으로 ““” … ““” 으로 함수에 대한 도움말을 기술한다. 함수가 하는 역할, 매개변수, 반환되는 값, 예제 등을 넣어 개발자가 봤을 때 피로도가 없도록 작성한다.\n함수 몸통(body): 앞서 사칙연산처럼 함수가 수행해야 되는 작업을 기술한다.\n반환값(return): return 예약어로 함수작업결과 반환되는 값을 지정한다.\n\n\ndef basic_operation(first, second):\n    \"\"\"\n    숫자 두개를 받아 사칙연산을 수행하는 함수.\n    \n    예제\n        basic_operation(10, 20)\n    매개변수(args)\n        first(int): 정수형 숫자\n        second(int): 정수형 숫자\n    반환값(return)\n        리스트: +-*/ 사칙연산 결과\n    \"\"\"\n    sum_number = first + second\n    minus_number = first - second\n    multiply_number = first * second\n    divide_number = first / second\n    \n    result = [sum_number, minus_number, multiply_number, divide_number]\n    \n    return result\n    \nbasic_operation(7, 3)    \n#&gt; [10, 4, 21, 2.3333333333333335]\n\n다른 사람이 작성한 함수를 사용한다는 것은 좀더 엄밀한 의미로 함수를 호출(call)한다고 한다. 함수를 호출해서 다른 사람이 작성한 함수를 사용하기 위해서 먼저 함수명을 알아야 하고, 그 다음으로 함수에서 사용되는 인자(arugment)를 파악해서 올바르게 전달해야 원하는 결과를 얻을 수 있다.\n표준편차(sd)를 계산하는 sd 함수의 경우 전달되는 인자는 두개 x, na.rm = FALSE인데 이를 확인할 수 있는 명령어가 args() 함수다.\n\nargs(sd)\n#&gt; function (x, na.rm = FALSE) \n#&gt; NULL\n\nx는 ? sd 명령어를 통해서 숫자 벡터를 전달해 주어야만 표준편차를 계산할 수 있다. 예를 들어, 데이터프레임(penguins)의 변수 하나(bill_length_mm)를 지정하여 전달하고 na.rm = TRUE도 명세하여 인자로 전달한다. 인자값이 기본디폴트 값으로 설정된 경우 타이핑을 줄일 수 있고, 결측값이 포함된 경우에 따라서 다른 인자를 넣어 전달하는 방식으로 함수를 사용한다.\n\nlibrary(palmerpenguins)\n\nsd(penguins$bill_length_mm, na.rm = TRUE)\n#&gt; [1] 5.459584",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#convert-scripts-to-function",
    "href": "functions.html#convert-scripts-to-function",
    "title": "\n16  함수\n",
    "section": "\n16.3 스크립트 → 함수",
    "text": "16.3 스크립트 → 함수\n함수를 작성하는 경우는 먼저 데이터를 가져와서 정제하고 EDA과정을 거치며 모형과 시각화 산출물을 제작하는 과정을 거친다. 그리고 나서 이런 작업이 몇번 반복하게 되면 함수작성을 고려하게 된다. 즉, 스크립트에서 함수로 변환하는 과정을 설명하면 다음과 같다.\n\nR 함수 템플릿을 제작한다.\n\n함수명 &lt;- function() { }\n\n\n스크립트를 함수 몸통에 복사하여 붙인다.\n반복작업되는 인자를 찾아내 이를 인자로 넣어둔다.\n인자값과 연동되는 부분을 찾아 맞춰준다.\n함수명을 적절한 동사를 갖춘 이름으로 작명한다.\n\nreturn이 불필요하기 때문에 R 언어 특성을 반영하여 필요한 경우 제거한다.\n\n\n16.3.1 주사위\n먼저 주사위를 모사하여 보자. 즉, 주사위를 물리적으로 만드는 대신 주사위를 던진 것과 동일한 효과가 나타나도록 이를 구현해 본다.\n\n주사위 던지는 스크립트\n\n먼저 주사위 눈을 1,2,3,4,5,6 숫자 벡터로 정의하고 나서 sample() 함수로 size=1을 지정한다. 즉, 주사위 눈 6개중 임의로 하나를 선택한다.\n\ndice &lt;- c(1,2,3,4,5,6)\n\nsample(dice, size=1)\n#&gt; [1] 2\n\n\n함수 템플릿\n\n“함수명 &lt;- function() { }”으로 구성되는 함수 템플릿을 작성한다.\n\ndraw_dice &lt;- function() {\n  \n}\n\n\n함수 몸통으로 복사하여 붙여넣기\n\n함수 몸통내부에 dice &lt;- c(1,2,3,4,5,6)을 함수를 매번 호출할 때마다 실행시킬 필요는 없기 때문에 외부로 빼내고 실제 주사위 던지는 과정을 모사하는 코드만 복사하여 붙여넣는다.\n\ndice &lt;- c(1, 2, 3, 4, 5, 6)\n\ndraw_dice &lt;- function() {\n  sample(dice, size=1)\n}\n\ndraw_dice()\n#&gt; [1] 6\n\n\n함수명, 함수 인자 등 마무리\n\n함수명을 draw_dice 말고 다른 더 기억하기 좋고 짧고 간결한 형태로 필요한 경우 변경시키고, 인자도 없는 것에서 횟수를 지정할 수 있도록 변경시키고, 필요한 경우 return 함수를 지정하여 반환값을 명시적으로 적어 둔다.\n\ndraw_dice &lt;- function(num_try) {\n  face &lt;- sample(dice, size=num_try)\n  return(face) # 불필요함.\n}\n\ndraw_dice(3)\n#&gt; [1] 4 6 2",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#why-write-function",
    "href": "functions.html#why-write-function",
    "title": "\n16  함수\n",
    "section": "\n16.4 왜 함수가 필요한가?",
    "text": "16.4 왜 함수가 필요한가?\n왜 함수가 필요한지를 데이터를 분석할 때 자주 나오는 변수 정규화 사례를 바탕으로 살펴보자. 데이터프레임에 담긴 변수의 측도가 상이하여 변수를 상대적으로 비교하기 위해 측도를 재조정하여 표준화할 필요가 있다. 변수에서 평균을 빼고 표준편차로 나누는 정규화도 있지만, 최대값에서 최소값을 빼서 분모에 두고 분자에 최소값을 빼서 나누면 모든 변수가 0–1 사이 값으로 척도가 조정된다.\n\\[ f(x)_{\\text{척도조정}} = \\frac{x-min(x)}{max(x)-min(x)} \\]\n\ndf &lt;- data.frame(a=c(1,2,3,4,5),\n                         b=c(10,20,30,40,50),\n                         c=c(7,8,6,1,3),\n                         d=c(5,4,6,5,2))\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) /\n        (max(df$a, na.rm = TRUE) - min(df$b, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) /\n        (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) /\n        (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\ndf        \n#&gt;      a         b         c    d\n#&gt; 1 0.00  0.000000 0.8571429 0.75\n#&gt; 2 0.25 -1.111111 1.0000000 0.50\n#&gt; 3 0.50 -2.222222 0.7142857 1.00\n#&gt; 4 0.75 -3.333333 0.0000000 0.75\n#&gt; 5 1.00 -4.444444 0.2857143 0.00\n\n상기 R 코드는 측도를 모두 맞춰서 변수 4개(a, b, c, d)를 비교하거나 향후 분석을 위한 것이다. 하지만, 읽어야 하는 코드중복이 심하고 길어 코드를 작성한 개발자의 의도 가 본의 아니게 숨겨져 있다. 작성한 R 코드에 실수한 것이 있는 경우, 다음 프로그램 실행에서 버그(특히, 구문론이 아닌 의미론적 버그)가 숨겨지게 된다. 상기 코드가 작성되는 과정을 살펴보면 본의 아니게 의도가 숨겨진다는 의미가 어떤 것인지 명확해진다.\n\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) 코드를 작성한 후, 정상적으로 돌아가는지 확인한다.\n1번 코드가 잘 동작하게 되면 다음 복사하여 붙여넣기 신공을 사용하여 다른 칼럼 작업을 확장해 나간다. df$b, df$c, df$d를 생성하게 된다.\n즉, 복사해서 붙여넣은 것을 변수명을 편집해서 df$b, df$c, df$d 변수를 순차적으로 생성해 낸다.\n\n\n\n\n\n\n\n해들리 위캠 어록\n\n\n\n\n중복은 의도를 숨기게 되고, 복사하여 붙여넣기 두번하면 함수를 작성할 시점이 되었다. (Duplication hides the intent. If you have copied-and-pasted twice, it is time to write a function.)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#funciton-component",
    "href": "functions.html#funciton-component",
    "title": "\n16  함수\n",
    "section": "\n16.5 함수 구성요소",
    "text": "16.5 함수 구성요소\n\n16.5.1 인자(argument)\n함수 구성요소 중 중요한 요소로 인자(argument)를 꼽을 수 있다. 인자는 크게 두가지로 나뉜다.\n\n데이터 인자(data argumnets): 대다수 함수는 기본적으로 데이터에 대한 연산을 가정하고 있다. 따라서 데이터를 함수 인자로 지정하여 이를 함수몸통에서 처리하고 결과를 반환시키는 것은 당연한 귀결이다.\n동작방식 지정 인자(detail arguments): 함수가 동작하는 방식에 대해서 세부적으로 동작하는 방식에 대해서 지정할 필요가 있는데 이때 필요한 것이 동작방식 지정 인자가 된다.\n\n예를 들어 t.test() 함수를 살펴보면 x가 데이터 인자가 되며, 기타 alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...은 함수가 구체적으로 어떻게 동작하는지 명세한 인자값이다.\n\n? t.test\n\n## Default S3 method:\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE,\n       conf.level = 0.95, ...)\n\n\n16.5.2 인자값 확인 - assert\n\n인자값이 제대로 입력되어야 함수몸통에서 기술한 연산작업이 제대로 수행될 수 있다. 이를 위해서 testthat, assertive, assertr, assertthat 등 수많은 팩키지가 존재한다. stopifnot(), stop() 등 Base R 함수를 사용해도 문제는 없다.\n다음과 같이 입력값에 NA가 포함된 경우 벡터의 합계를 구하는 함수가 동작하지 않거나 아무 의미없는 값을 반환시키곤 한다. 그리고 앞서 인자값을 잘 제어하지 않게 되면 귀중한 컴퓨팅 자원을 낭비하기도 한다. 이를 방기하기 위해서 stopifnot()함수로 함수 몸통을 수호하는 보호자처럼 앞서 인자값의 적절성에 대해서 검정을 먼저 수행한다. 그리고 나서 사전 유효성 검사를 통과한 인자값에 대해서만 함수 몸통에 기술된 연산작업을 수행하고 결과값을 반환시킨다.\n\nlibrary(testthat)\n\nnum_vector &lt;- c(1,2,3,4, 5)\nna_vector &lt;- c(1,2,3,NA, 5)\n\nsum_numbers &lt;- function(vec) {\n  \n  stopifnot(!any(is.na(vec)))\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\nsum_numbers(num_vector)\n#&gt; [1] 15\nsum_numbers(na_vector)\n#&gt; Error in sum_numbers(na_vector): !any(is.na(vec)) is not TRUE\n\n상기 코드의 문제점은 stopifnot() 함수가 잘못된 입력값에 대해서 문제가 무엇이고, 어떤 행동을 취해야 하는지 친절하지 않다는데 있다. 이를 assertive 팩키지를 활용해서 극복하는 방안을 살펴보자. asserive 팩키지를 설치하면 R 함수 작성에 걸림돌이 될 수 있는 거의 모든 사전 점검작업을 수행할 수 있다는 것이 매력적이다. install.packages(\"assertive\")를 실행하게 되면 함께 설치되는 팩키지는 다음과 같다.\n‘assertive.base’, ‘assertive.properties’, ‘assertive.types’, ‘assertive.numbers’, ‘assertive.strings’, ‘assertive.datetimes’, ‘assertive.files’, ‘assertive.sets’, ‘assertive.matrices’, ‘assertive.models’, ‘assertive.data’, ‘assertive.data.uk’, ‘assertive.data.us’, ‘assertive.reflection’, ‘assertive.code’\n\nlibrary(assertive)\n\nsum_numbers_assertive &lt;- function(vec) {\n  \n  assert_is_numeric(vec)\n  \n  if(assert_any_are_na(vec)) {\n      stop(\"벡터 x는 NA 값이 있어요. 그래서 총합을 구하는게 의미가 없네요\")\n  }\n  \n  total &lt;- 0\n\n  for(i in 1:length(vec)) {\n    total &lt;- total + vec[i]\n  }\n  total\n}\n\n# sum_numbers_assertive(num_vector)\nsum_numbers_assertive(na_vector)\n#&gt; Error in if (assert_any_are_na(vec)) {: the condition has length &gt; 1\n\n\n16.5.3 반환값 확인\nR은 파이썬과 달리 return()이 꼭 필요하지는 않다. 왜냐하면 마지막 객체가 자동으로 함수 반환값으로 정의되기 때문이다. 함수 반환값 관련하여 몇가지 사항을 알아두면 도움이 많이 된다.\n먼저 함수에서 반환되는 값이 하나가 아닌 경우 이를 담아내는 방법을 살펴보자. list()로 감싸 이를 반환하는 경우가 많이 사용되었지만, 최근 zeallot 팩키지가 도입되어 함수 출력값을 받아내는데 간결하고 깔끔하게 작업할 수 있게 되었다. zeallot vignette에 다양한 사례가 나와 있다.\n예를 들어 단변량 회귀모형의 경우 lm() 함수로 회귀식을 적합시킨다. 그리고 나서 coef() 함수로 절편과 회귀계수를 추출할 때 %&lt;-% 연산자를 사용하게 되면 해당값을 벡터객체에 할당시킬 수 있다.\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\nc(inter, slope) %&lt;-% coef(lm(mpg ~ cyl, data = mtcars))\n\ncat(\"절편: \", inter, \"\\n기울기: \", slope)\n#&gt; 절편:  37.88458 \n#&gt; 기울기:  -2.87579\n\niris 데이터셋을 훈련/시험 데이터셋으로 쪼갠다. 이를 위해서 일양균등분포에서 난수를 생성시켜 8:2 비율로 훈련/시험 데이터를 나눈다. 그리고 나서, %&lt;-% 연산자로 훈련/시험 데이터로 나누어 할당하는 것도 가능하다. 각 붓꽃마다 0~1 사이 난수를 생성하여 할당한다. 그리고 난수값이 0.2 이상이면 훈련, 그렇지 않으면 시험 데이터로 구분한다.\n\n\niris_df &lt;- iris %&gt;% \n  mutate(runif = runif(n())) %&gt;% \n  mutate(train_test = ifelse(runif &gt; 0.2, \"train\", \"test\")) \n\nc(test, train) %&lt;-%  split(iris_df, iris_df$train_test)\n\ncat(\"총 관측점: \", nrow(iris), \"\\n훈련: \", nrow(train), \"\\n시험: \", nrow(test))\n#&gt; 총 관측점:  150 \n#&gt; 훈련:  128 \n#&gt; 시험:  22\n\n혹은, 회귀분석 결과를 list() 함수로 결합시켜 리스트로 반환시킨다. 이런 경우 결과값이 하나가 아니더라도 추후 리스트 객체를 풀어 활용하는 것이 가능하다.\n\nget_lm_statistics &lt;- function(df) {\n  mtcars_lm &lt;- lm(mpg ~ cyl, data=df)\n  \n  intercept &lt;- coef(mtcars_lm)[1]\n  beta      &lt;- coef(mtcars_lm)[2]\n  \n  lm_stats &lt;- list(intercept = intercept, \n                   beta = beta)\n  \n  return(lm_stats)\n}\n\nmtcars_list &lt;- get_lm_statistics(mtcars)\n\nmtcars_list\n#&gt; $intercept\n#&gt; (Intercept) \n#&gt;    37.88458 \n#&gt; \n#&gt; $beta\n#&gt;      cyl \n#&gt; -2.87579",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#time-to-write-function",
    "href": "functions.html#time-to-write-function",
    "title": "\n16  함수\n",
    "section": "\n16.6 함수를 작성하는 시점",
    "text": "16.6 함수를 작성하는 시점\n복사해서 붙여넣는 것을 두번 하게 되면, 함수를 작성할 시점이다. 중복을 제거하는 한 방법이 함수를 작성하는 것이고, 함수를 작성하게 되면 의도가 명확해진다. 함수명을 rescale로 붙이고 이를 실행하게 되면, 의도가 명확하게 드러나게 되고, 복사해서 붙여넣게 되면서 생겨나는 중복과 반복에 의한 실수를 줄일 수 있게 되고, 향후 코드를 갱신할 때도 도움이 된다.\n\nrescale &lt;- function(x){\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf$a &lt;- rescale(df$a)\ndf$b &lt;- rescale(df$b)\ndf$c &lt;- rescale(df$c)\ndf$d &lt;- rescale(df$d)\n\nrescale() 함수를 사용해서 복사하여 붙여넣는 중복을 크게 줄였으나, 여전히 함수명을 반복해서 복사하여 붙여넣기를 통해 코드를 작성했다. 함수형 프로그래밍을 사용하는 것으로 함수명을 반복적으로 사용하는 것조차도 피할 수 있다.\n\nlibrary(purrr)\ndf &lt;- map_df(df, rescale)\ndf\n#&gt; # A tibble: 5 × 4\n#&gt;       a     b     c     d\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0     1    0.857  0.75\n#&gt; 2  0.25  0.75 1      0.5 \n#&gt; 3  0.5   0.5  0.714  1   \n#&gt; 4  0.75  0.25 0      0.75\n#&gt; 5  1     0    0.286  0\n\n함수를 사용하지 않고 복사하여 붙여넣기 방식으로 코드를 작성한 경우 의도하지 않은 실수가 있어 함수를 도입하여 작성한 코드와 결과가 다른 것이 존재한다. 코드를 읽어 찾아보거나 실행한 후 결과를 통해 버그를 찾아보는 것도 함수의 의미와 중요성을 파악하는데 도움이 된다.\n\n\n\n\n\n\n좋은 함수란?\n\n\n\n척도를 일치시키는 기능을 함수로 구현했지만, 기능을 구현했다고 좋은 함수가 되지는 않는다. 좋은 함수가 되는 조건은 다음과 같다.\n\n\nCorrect: 기능이 잘 구현되어 올바르게 동작할 것\n\nUnderstandable: 사람이 이해할 수 있어야 함. 즉, 함수는 컴퓨터를 위해 기능이 올바르게 구현되고, 사람도 이해할 수 있도록 작성되어야 한다.\n즉, Correct + Understandable: 컴퓨터와 사람을 위해 적성될 것.\n\n한걸음 더 들어가 구체적으로 좋은 함수는 다음과 같은 특성을 지니고 있다.\n\n함수와 인자에 대해 유의미한 명칭을 사용한다.\n\n함수명에 적절한 동사명을 사용한다.\n\n\n직관적으로 인자를 배치하고 기본디폴트값에도 추론가능한 값을 사용한다.\n함수가 인자로 받아 반환하는 것을 명확히 한다.\n함수 내부 몸통부문에 일관된 스타일을 잘 사용한다.\n\n좋은 함수 작성과 연계하여 깨끗한 코드(Clean code)는 다음과 같은 특성을 갖고 작성된 코드를 뜻한다.\n\n가볍고 빠르다 - Light\n가독성이 좋다 - Readable\n해석가능하다 - Interpretable\n유지보수가 뛰어나다 - Maintainable",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#how-to-write-function",
    "href": "functions.html#how-to-write-function",
    "title": "\n16  함수\n",
    "section": "\n16.7 사례: rescale 함수",
    "text": "16.7 사례: rescale 함수\n함수를 작성할 경우 먼저 매우 단순한 문제에서 출발한다. 척도를 맞추는 상기 과정을 R 함수로 만드는 과정을 통해 앞서 학습한 사례를 실습해 보자.\n\n입력값과 출력값을 정의한다. 즉, 입력값이 c(1,2,3,4,5) 으로 들어오면 출력값은 0.00 0.25 0.50 0.75 1.00 0–1 사이 값으로 나오는 것이 확인되어야 하고, 각 원소값도 출력벡터 원소값에 매칭이 되는지 확인한다.\n기능이 구현되어 동작이 제대로 되는지 확인되는 R코드를 작성한다.\n\n\n(df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\n\n확장가능하게 임시 변수를 사용해서 위에서 구현된 코드를 다시 작성한다.\n\n\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\nx &lt;- df$a\n( x - min( x , na.rm = TRUE)) / (max( x , na.rm = TRUE) - min( x , na.rm = TRUE))\n\n\n함수 작성의도를 명확히 하도록 다시 코드를 작성한다.\n\n\nx &lt;- df$a\nrng &lt;- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n\n\n최종적으로 재작성한 코드를 함수로 변환한다.\n\n\nx &lt;- df$a\n\nrescale &lt;- function(x){\n                rng &lt;- range(x, na.rm = TRUE)\n                (x - rng[1]) / (rng[2] - rng[1])\n            }\n\nrescale(x)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "functions.html#function-is-argument",
    "href": "functions.html#function-is-argument",
    "title": "\n16  함수\n",
    "section": "\n16.8 사례: 요약통계 함수",
    "text": "16.8 사례: 요약통계 함수\n데이터를 분석할 때 가장 먼저 수행하는 작업이 요약통계를 통해 데이터를 이해하는 것이다. 이미 훌륭한 요약통계 패키지와 함수가 있지만, 친숙한 개념을 함수로 다시 제작함으로써 함수에 대한 이해를 높일 수 있다.\n요약통계 기능을 먼저 구현한 다음에 중복 제거하여 요약통계 기능 함수를 제작해보자. 함수도 인자로 넣어 처리할 수 있다는 점이 처음에 이상할 수도 있지만, 함수를 인자로 처리할 경우 코드 중복을 상당히 줄일 수 있다. \\(L_1\\), \\(L_2\\), \\(L_3\\) 값을 구하는 함수를 다음과 같이 작성하는 경우, 숫자 1,2,3 만 차이날 뿐 다른 부분은 동일하기 때문에 함수 코드에 중복이 심하게 관찰된다.\n\n1단계: 중복이 심한 함수, 기능 구현에 초점을 맞춤\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ 1\nf2 &lt;- function(x) abs(x - mean(x)) ^ 2\nf3 &lt;- function(x) abs(x - mean(x)) ^ 3\n\n\n2단계: 임시 변수로 처리할 수 있는 부분을 식별하고 적절한 인자명(power)을 부여한다.\n\n\nf1 &lt;- function(x) abs(x - mean(x)) ^ power\nf2 &lt;- function(x) abs(x - mean(x)) ^ power\nf3 &lt;- function(x) abs(x - mean(x)) ^ power\n\n\n3단계: 식별된 변수명을 함수 인자로 변환한다.\n\n\nf1 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf2 &lt;- function(x, power) abs(x - mean(x)) ^ power\nf3 &lt;- function(x, power) abs(x - mean(x)) ^ power\n\n여기서 요약통계함수 인자로 “데이터”(df)와 기초통계 요약 “함수”(mean, sd 등)도 함께 넘겨 요약통계함수를 간략하고 가독성 높게 작성할 수 있다.\n먼저, 특정 변수의 중위수, 평균, 표준편차를 계산하는 함수를 작성하는 경우를 가정해보자.\n\n1 단계: 각 기능을 구현하는 기능 구현에 초점을 맞춤\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- median(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- mean(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- sd(df[[i]])\n    }\n    output\n  }\n\n\n2 단계: median, mean, sd를 함수 인자 fun 으로 함수명을 통일.\n\n\ncol_median &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n3 단계: 함수 인자 fun 을 넣어 중복을 제거.\n\n\ncol_median &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_mean &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\ncol_sd &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n  }\n\n\n4 단계: 함수를 인자로 갖는 요약통계 함수를 최종적으로 정리하고, 테스트 사례를 통해 검증.\n\n\ncol_summary &lt;- function(df, fun) {\n    output &lt;- numeric(length(df))\n    for (i in seq_along(df)) {\n      output[i] &lt;- fun(df[[i]])\n    }\n    output\n}\n\ncol_summary(df, fun = median)\n#&gt; [1] 0.5000000 0.5000000 0.7142857 0.7500000\ncol_summary(df, fun = mean)\n#&gt; [1] 0.5000000 0.5000000 0.5714286 0.6000000\ncol_summary(df, fun = sd)\n#&gt; [1] 0.3952847 0.3952847 0.4164966 0.3791438\n\n\n\n\n데이터 과학 함수 개념",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>함수</span>"
    ]
  },
  {
    "objectID": "function_plyr.html",
    "href": "function_plyr.html",
    "title": "17  분할-적용-병합 전략",
    "section": "",
    "text": "17.1 gapminder 데이터셋\ngapminder 데이터 팩키지의 각 대륙, 국가, 연도별 인구와 중요 두가지 정보인 평균수명과 일인당GDP 정보를 바탕으로 각 대륙별 평균 GDP를 추출해보자. 이를 위해서 먼저 인당GDP(gdpPercap)와 인구수(pop)를 바탕으로 GDP를 계산하고 이를 평균낸다. 특정 연도를 지칭하지 않는 것이 다소 문제의 소지가 있을 수 있지만, 분할-적용-병합 전략을 살펴보는데 큰 무리는 없어 보인다.\nlibrary(tidyverse)\nlibrary(gapminder)\n\ncalc_GDP &lt;- function(dat, year=NULL, country=NULL) {\n  if(!is.null(year)) {\n    dat &lt;- dat[dat$year %in% year, ]\n  }\n  if (!is.null(country)) {\n    dat &lt;- dat[dat$country %in% country,]\n  }\n  gdp &lt;- dat$pop * dat$gdpPercap\n\n  new &lt;- cbind(dat, gdp=gdp)\n  return(new)\n}\n\ngapminder %&gt;% \n  mutate(GDP = pop * gdpPercap) %&gt;% \n  DT::datatable()",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "function_plyr.html#진화의-역사",
    "href": "function_plyr.html#진화의-역사",
    "title": "17  분할-적용-병합 전략",
    "section": "\n17.2 진화의 역사",
    "text": "17.2 진화의 역사\n\n17.2.1 선사시대 split, lapply, do.call\n\n선사시대에는 대륙별로 split 한 후에 lapply() 함수를 사용해서 앞서 정의한 calc_GDP 함수로 GDP를 계산한 후에 평균을 다시 계산한 뒤에 마지막으로 do.call() 함수로 병합(combine)하여 GDP 대륙별 평균을 구할 수 있다.\n\ncontinent_split_df  &lt;- split(gapminder, gapminder$continent)\nGDP_list_df         &lt;- lapply(continent_split_df, calc_GDP)\nGDP_list_df         &lt;- lapply(GDP_list_df, function(x) mean(x$gdp))\nmean_GDP_df         &lt;- do.call(rbind, GDP_list_df)\n\nmean_GDP_df\n#&gt;                  [,1]\n#&gt; Africa    20904782844\n#&gt; Americas 379262350210\n#&gt; Asia     227233738153\n#&gt; Europe   269442085301\n#&gt; Oceania  188187105354\n\n\n17.2.2 plyr::ddply\n\n석기시대 plyr 팩키지 ddply 함수를 사용해서 각 대륙별로 쪼갠 후에 각 대륙별 평균 GDP를 구할 수 있다.\n\nlibrary(gapminder)\n\nplyr::ddply(\n .data = calc_GDP(gapminder),\n .variables = \"continent\",\n .fun = function(x) mean(x$gdp)\n)\n#&gt;   continent           V1\n#&gt; 1    Africa  20904782844\n#&gt; 2  Americas 379262350210\n#&gt; 3      Asia 227233738153\n#&gt; 4    Europe 269442085301\n#&gt; 5   Oceania 188187105354\n\n\n17.2.3 초기 tidyverse 시대: group_by, do\n\ngroup_by() + do()를 결합하여 임의 연산작업을 각 그룹별로 수행시킬 수 있다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  do(calc_GDP(.)) %&gt;%\n  do(out = mean(.$gdp)) %&gt;% \n  unnest\n#&gt; # A tibble: 5 × 2\n#&gt;   continent           out\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.\n\n\n17.2.4 중기 tidyverse 시대: group_by, by_slice\n\ngroup_by() + by_slice()를 결합하여 분할-적용-병합 전략을 적용시킬 수도 있으나 by_slice() 함수가 dplyr::do() 함수와 같은 작업을 수행했고, purrrlyr에 갔다가… 그후 행방이 묘연해졌다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  purrrlyr::by_slice(~calc_GDP(.x), .collate = 'rows') %&gt;% \n  select(continent, gdp) %&gt;% \n  group_by(continent) %&gt;%\n  purrrlyr::by_slice(~mean(.$gdp), .collate = 'rows')\n#&gt; # A tibble: 5 × 2\n#&gt;   continent          .out\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.\n\n\n17.2.5 현대 tidyverse 시대: group_by, nest, mutate(map())\n\n현대 분할-적용-병합 전략은 group_by + nest()로 그룹별 데이터프레임으로 만들고, mutate(map())을 사용해서 calc_GDP() 함수를 적용시켜 GDP를 계산하고, summarize() 함수를 적용시켜 각 대륙별 평균 GDP를 계산한다. 마지막으로 unnest를 적용시켜 원하는 산출물을 얻는다.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  nest() %&gt;% \n  mutate(data = purrr::map(data, calc_GDP)) %&gt;% \n  mutate(mean_GDP = purrr::map(data, ~ summarize(., mean_GDP = mean(gdp)))) %&gt;% \n  unnest(mean_GDP)\n#&gt; # A tibble: 5 × 3\n#&gt; # Groups:   continent [5]\n#&gt;   continent data                mean_GDP\n#&gt;   &lt;fct&gt;     &lt;list&gt;                 &lt;dbl&gt;\n#&gt; 1 Asia      &lt;df [396 × 6]&gt; 227233738153.\n#&gt; 2 Europe    &lt;df [360 × 6]&gt; 269442085301.\n#&gt; 3 Africa    &lt;df [624 × 6]&gt;  20904782844.\n#&gt; 4 Americas  &lt;df [300 × 6]&gt; 379262350210.\n#&gt; 5 Oceania   &lt;df [24 × 6]&gt;  188187105354.\n\n\n17.2.6 tidyverse/base 하이브리드 조합: split, map_dfr\n\n마지막으로 base R split() 함수와 map_dfr() 함수를 조합해서 원하는 결과를 얻어낼 수도 있다.\n\ngapminder %&gt;%\n  split(.$continent) %&gt;%\n  purrr::map_dfr(calc_GDP) %&gt;% \n  split(.$continent) %&gt;%\n  purrr::map_dfr(~mean(.$gdp)) %&gt;% \n  gather(continent, mean_GDP)\n#&gt; # A tibble: 5 × 2\n#&gt;   continent      mean_GDP\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1 Africa     20904782844.\n#&gt; 2 Americas  379262350210.\n#&gt; 3 Asia      227233738153.\n#&gt; 4 Europe    269442085301.\n#&gt; 5 Oceania   188187105354.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "function_plyr.html#gapminder-실습",
    "href": "function_plyr.html#gapminder-실습",
    "title": "17  분할-적용-병합 전략",
    "section": "\n17.3 gapminder 실습",
    "text": "17.3 gapminder 실습\n앞서, 함수를 사용해서 코드를 단순화하는 방법을 살펴봤다. gapminder 데이터셋을 인자로 받아, pop와 gdpPercap를 곱해 GDP를 계산하는 calcGDP 함수를 정의했다. 추가적인 인자를 정의해서, year별, country별 필터를 적용할 수도 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n데이터 작업을 할 때, 흔히 마주치는 작업이 집단별로 그룹을 묶어 계산하는 것이다. 위에서는 단순히 두 열을 곱해서 GDP를 계산했다. 하지만, 대륙별로 평균 GDP를 계산하고자 한다면 어떻게 해야 할까?\ncalcGDP를 실행하고 나서, 각 대륙별로 평균을 산출해보자. 데이터셋을 가져와서 인구 열과 1인당 GDP 열을 곱합니다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n하지만, 그다지 멋있지는 않다. 그렇다. 함수를 사용해서 반복되는 작업을 상당량 줄일 수 있었다. 함수 사용은 멋있었다. 하지만, 여전히 반복이 있다. 직접 반복하게 되면, 지금은 물론이고 나중에도 시간을 까먹게 되고, 잠재적으로 버그가 스며들 여지를 남기게 된다.\ncalcGDP처럼 유연성 있는 함수를 새로 작성할 수도 있지만, 제대로 동작하는 함수를 개발하기까지 상당한 노력과 테스트가 필요하다.\n여기서 맞닥뜨린 추상화 문제를 “분할-적용-병합(split-apply-combine)” 전략이라고 한다.\n\n\n\n\n\n그림 17.2: 분할-적용-병합 전략 도식화\n\n\n데이터를 집단으로 분할(split)하고, 이번 경우에는 대륙별로 분할한다. 그리고 분할된 집단에 연산을 적용(apply)한 후, 선택적으로 결과를 묶어 병합(combine)한다.\n\n17.3.1 dplyr 패키지\ndplyr 패키지는 데이터 조작을 위한 문법을 제공하며, 이를 통해 “분할-적용-병합” 문제를 해결할 수 있다. 먼저 dplyr 패키지를 로드하고, dplyr을 사용하여 대륙별 평균 GDP를 빠르게 계산한다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n방금 코드에서 일어난 일을 복기해 보자.\n\n\n%&gt;% 파이프 연산자를 사용하여 calcGDP(gapminder) 함수의 결과를 다음 단계로 전달했다.\n\ngroup_by(continent)를 사용하여 데이터를 대륙별로 그룹화했다.\n\nsummarise()를 사용하여 각 그룹에 대해 mean(gdp)를 계산하고, 결과를 mean_gdp라는 새로운 열에 저장했다.\n\n그룹화 변수를 추가하면 어떻게 될까?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ngroup_by()에 year를 추가하여 대륙과 연도별로 그룹화했다.\n\n\n\n\n\n\n도전과제\n\n\n\n대륙별로 평균 기대수명을 계산해보자. 어느 대륙의 기대수명이 가장 길고, 어느 대륙이 가장 짧은가?\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n도전과제\n\n\n\ndplyr의 함수를 사용하여 도전과제 2의 출력 결과로부터, 1952년에서 2007년 사이의 평균 기대수명 차이를 계산해보자.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n이렇게 dplyr 패키지를 사용하면 데이터 조작과 분석을 보다 간결하고 직관적으로 수행할 수 있다. 파이프 연산자(%&gt;%)를 사용하여 연산을 연결하면 코드의 가독성도 향상된다. group_by(), summarise(), mutate() 등의 함수를 조합하여 다양한 분석 작업을 수행할 수 있다.알겠습니다. 계속해서 교정 작업을 진행하겠습니다.\n\n\n\n\n\n\n도전과제\n\n\n\n실제로 실행하지 말고, 다음 중 어떤 코드가 대륙별 평균 기대수명을 계산하는지 살펴보자.\n\n\n\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  group_by(gapminder$continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\ngapminder %&gt;%\n  filter(continent) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp))\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n답안 3은 대륙별 평균 기대 수명을 계산한다.\n\n\n\n\n\n\n\n\n그림 17.1: 분할-적용-병합 작업과정 도식화\n그림 17.2: 분할-적용-병합 전략 도식화\n\n\n\nWickham, Hadley. 2011. “The split-apply-combine strategy for data analysis”. Journal of statistical software 40: 1–29.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>분할-적용-병합 전략</span>"
    ]
  },
  {
    "objectID": "functions_ds.html",
    "href": "functions_ds.html",
    "title": "\n18  데이터 과학 함수\n",
    "section": "",
    "text": "18.1 Defuse-and-Inject 패턴\ntidy evaluation에서 Defuse-and-Inject 패턴을 통해 데이터프레임 dplyr 패키지와 그래프 문법에 따른 시각화 ggplot2 패키지에 함수를 직관적으로 적용시킬 수 있다. 신관제거(defuse)는 기본적으로 표현식의 평가를 지연시켜 바로 실행되는 것을 막는 역할을 수행한다. 이런 기능을 통해 환경의 맥락을 유지하는 역할을 수행한다. 주입(injection)은 포획되거나 신관제거된 표현석을 다른 맥락에서 평가하거나 다른 표현식에 주입하는 개념이다. 신관제거에 enquo()가 사용되었다면 주입에는 !! (뱅-뱅 이라고 읽음) 연산자를 사용하여 으로 다른 함수 내부에서 평가되어 실행되는 역할을 수행한다.\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_na &lt;- function(dataframe, col_name) {\n  \n  col_quo = enquo(col_name) # 신관제거(defuse)\n  \n  dataframe %&gt;%\n    select(species, island, sex, year, body_mass_g) |&gt; \n    filter(is.na(!!col_quo)) # 주입(inject)\n}\n\n# 사용방법\npenguins %&gt;% filter_na(sex)\n#&gt; # A tibble: 11 × 5\n#&gt;    species island    sex    year body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt; &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen &lt;NA&gt;   2007          NA\n#&gt;  2 Adelie  Torgersen &lt;NA&gt;   2007        3475\n#&gt;  3 Adelie  Torgersen &lt;NA&gt;   2007        4250\n#&gt;  4 Adelie  Torgersen &lt;NA&gt;   2007        3300\n#&gt;  5 Adelie  Torgersen &lt;NA&gt;   2007        3700\n#&gt;  6 Adelie  Dream     &lt;NA&gt;   2007        2975\n#&gt;  7 Gentoo  Biscoe    &lt;NA&gt;   2007        4100\n#&gt;  8 Gentoo  Biscoe    &lt;NA&gt;   2008        4650\n#&gt;  9 Gentoo  Biscoe    &lt;NA&gt;   2009        4725\n#&gt; 10 Gentoo  Biscoe    &lt;NA&gt;   2009        4875\n#&gt; 11 Gentoo  Biscoe    &lt;NA&gt;   2009          NA\nfilter_na() 함수는 데이터프레임과 칼럼명을 패러미터로 받아 칼럼명에 결측값이 있는 행만 추출하여 반환하는 역할을 수행한다. 이를 위해서 칼럼명을 신관제거하여 col_quo 표현식으로 지연시킨 후에 !!col_quo에 주입시켜 평가작업을 수행하여 원하는 결과를 반환한다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#역사",
    "href": "functions_ds.html#역사",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.2 역사",
    "text": "18.2 역사\ntidyvserse는 데이터 마스킹(data-masking) 방식을 ggplot2, dplyr 패키지에 도입했지만, 결국 rlang 패키지에 자체 프로그래밍 프레임워크를 장착했다. rlang 패키지 Defuse-and-Inject 패턴에 이르는 과정은 이전 다양한 시도를 통해 학습하는 배움의 과정이였다.\n\nS언어에서 attach() 함수로 데이터 범위 개념을 도입했다. (Becker 2018)\n\nS언어로 모형 함수에 데이터 마스킹 공식을 도입했다. (Chambers 와/과 Hastie 1992)\n\nPeter Delgaard frametools 패키지를 1997년 작성했고 나중에 base::transform(), base::subset() 함수로 Base R에 채택됐다.\nLuke Tierney가 원래 환경을 추적하기 위해 공식을 2000년에 변경했고 R 1.1.0에 반영되었으며 Quosures의 모태가 되었다.\n2001년 Luke Tierney는 base::with()를 소개했다.\n\ndplyr 패키지가 2014년 첫선을 보였고, 2017년 rlang 패키지에 tidy eval이 구현되며 quosure, 암묵적 주입(implicit injection), 데이터 대명사(data pronouns) 개념이 소개됐다.\n2019년 rlang 0.4.0에 Defuse-and-Inject 패턴을 단순화한 {{}}이 도입되어 직관적으로 코드를 작성하게 되었다.\n\n데이터 분석에서 빈도수가 높은 작업을 Base R과 dplyr 패키지를 사용한 사례를 다음과 같이 비교하면 S언어에서 현재까지 이뤄낸 발전이 가시적으로 다가온다.\n\n\n\n\n\n\n\n작업\nBase R\ndplyr\n\n\n\n행 필터링 (Filter)\nsubset(data, condition)\ndata %&gt;% filter(condition)\n\n\n특정 칼럼 선택 (Select)\ndata[, c(\"col1\", \"col2\")]\ndata %&gt;% select(col1, col2)\n\n\n그룹별 집계작업\naggregate(. ~ grouping_var, data, FUN = mean)\ndata %&gt;% group_by(grouping_var) %&gt;% summarize(new_col = mean(col_name))\n\n\n죠인(Join)\nmerge(data1, data2, by = \"key_column\")\ndata1 %&gt;% inner_join(data2, by = \"key_column\")\n\n\n칼럼 추가\ntransform(data, new_col = some_func(existing_col))\ndata %&gt;% mutate(new_col = some_func(existing_col))\n\n\n행 결합\nrbind(data1, data2)\nbind_rows(data1, data2)\n\n\n칼럼 결합\ncbind(data1, data2)\nbind_cols(data1, data2)\n\n\n정렬\ndata[order(data$col_name), ]\ndata %&gt;% arrange(col_name)\n\n\n\n\n18.2.1 attach() 함수\n데이터프레임에 attach() 함수를 사용하면 데이터프레임을 구성하는 칼럼이 벡터로 작업환경에서 바로 접근하여 작업을 수행할 수 있다. penguins 데이터프레임을 attach()한 결과 bill_depth_mm 벡터가 작업환경에서 바로 접근하여 평균값을 계산할 수 있게 되었다. 작업을 완료한 후에 detach() 를 사용해서 작업환경에서 제거한다.\n\nlibrary(palmerpenguins)\n\nbase::attach(penguins)\n\nls(pos = which(search() == \"penguins\")[1])\n#&gt; [1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"      \n#&gt; [4] \"flipper_length_mm\" \"island\"            \"sex\"              \n#&gt; [7] \"species\"           \"year\"\n\nmean(bill_depth_mm, na.rm = TRUE)\n#&gt; [1] 17.15117\n\ndetach(penguins)\n\n\n18.2.2 with() 함수\nattach() 함수는 편리한 장점이 있지만, 데이터프레임 변수명과 함수명, 또 다른 작업에서 나온 객체명과 충돌이 발생할 경우 전혀 생각하지 못한 문제가 발생할 수 있다. 따라서, 격리를 통해 문제를 단순화하는 것이 필요하다. 이를 위해서 with() 함수를 사용하게 되면 데이터프레임에 속한 칼럼명을 명시하지 않더라도 간결하게 데이터 분석 작업을 이어나갈 수 있다.\n\nlibrary(palmerpenguins)\n\nwith(data = penguins,\n     expr = mean(bill_depth_mm, na.rm = TRUE))\n#&gt; [1] 17.15117\n\n\n18.2.3 aggregate() 함수\nBase R 에서 지원되는 aggregate() 함수를 사용해서 동일한 결과를 얻을 수 있다. aggregate() 함수는 with()와 지향점은 유사하지만 구현방식에서 다소 차이가 난다.\n\naggregate(bill_depth_mm ~ 1, \n          data = penguins, \n          FUN = mean, \n          na.rm = TRUE)\n#&gt;   bill_depth_mm\n#&gt; 1      17.15117",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#데이터-마스킹",
    "href": "functions_ds.html#데이터-마스킹",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.3 데이터 마스킹",
    "text": "18.3 데이터 마스킹\nR에서의 데이터 마스킹(Data Masking)은 tidyverse 생태계에서 데이터 문법을 담당하는 dplyr 패키지에서 핵심적인 개념이다. 데이터 마스킹을 사용하면 데이터프레임 칼럼을 $, [[ ]]를 사용하지 않고도 칼럼명으로 직접 참조할 수 있어, 데이터를 조작하고 변환할 때 훨씬 직관적이고 가독성 높은 코드를 작성할 수 있다.\npenguins 데이터프레임의 species를 데이터 마스킹 없이 조작하려면 penguins$species 혹은 penguins[['species']]와 같이 구문을 작성해야 하지만 데이터 마스킹을 사용하면 species 만으로 충분한다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# 데이터 마스킹을 사용하여 펭귄종(species)이 \"Adelie\"만 추출한다.\npenguins %&gt;% filter(species == \"Adelie\")\n#&gt; # A tibble: 152 × 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           NA            NA                  NA          NA\n#&gt;  5 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  6 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  7 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  8 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  9 Adelie  Torgersen           34.1          18.1               193        3475\n#&gt; 10 Adelie  Torgersen           42            20.2               190        4250\n#&gt; # ℹ 142 more rows\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\ndplyr 함수의 데이터 마스킹은 비표준 평가 (Non-standard evaluation, NSE)라는 개념에 기반을 두는데, 표현식을 캡처하고 난 후 바로 바로 실행되지 않고 제공된 데이터의 맥락 내에서 평가가 이루어진다. 데이터 마스킹은 강력하며 깔끔한 구문을 제공하지만, 칼럼명과 충돌할 수 있는 환경의 변수 이름이 있을 때 예기치 않은 방식으로 동작한다. 모호한 상황이 발생할 때 항상 다음과 같은 방식으로 .data$column_name 함으로써 데이터 마스킹 재정의(Overriding)를 통해 명확히 한다.\n\nspecies &lt;- \"Chinstrap\"\n\npenguins %&gt;% \n  filter(.data$species == \"Adelie\")\n#&gt; # A tibble: 152 × 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           NA            NA                  NA          NA\n#&gt;  5 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  6 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  7 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  8 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  9 Adelie  Torgersen           34.1          18.1               193        3475\n#&gt; 10 Adelie  Torgersen           42            20.2               190        4250\n#&gt; # ℹ 142 more rows\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\n\n항목\n데이터 마스킹\nTidy Evaluation\n\n\n\n정의\n- 데이터프레임 칼럼명을 직접적인 변수처럼 다룰 수 있는 능력.  - $나 [[ ]] 없이 칼럼 참조를 단순화.\n- R 메타프로그래밍을 위한 프레임워크, 특히 tidyverse 에서 사용.  - 다양한 맥락에서 표현식을 캡쳐하고 평가하는 도구 제공.\n\n\n사용 사례\n- dplyr 함수에서 직접 데이터 조작.  - 코드 가독성 향상.\n- 따옴표 없는 표현식으로 사용자 정의 함수 생성.  - 표현식을 프로그래밍 방식으로 구성 및 평가.  - 표현식 평가 맥락 제어.\n\n\n구현\n- 기본적으로 tidy evaluation 메커니즘을 사용하여 구현됨.\n- rlang 패키지 enquo(), quo(), !! 등을 사용.  - 표현식과 그 환경을 캡쳐하기 위해 쿼저(Quosure) 의존.\n\n\n복잡성\n- 최종 사용자를 위해 간소화.  - 기본적인 복잡성을 추상화.\n- R 메타프로그래밍 이해 필요.  - 고급 사용자에게 더 많은 유연성 제공.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#깔끔한-평가",
    "href": "functions_ds.html#깔끔한-평가",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.4 깔끔한 평가",
    "text": "18.4 깔끔한 평가\n깔끔한 평가(Tidy evaluation)은 R tidyverse 프레임워크로, 특히 비표준 평가 (Non-Standard Evaluation, NSE)와 관련하여 tidyverse 함수로 프로그래밍하는 방법을 표준화했다. NSE는 R 함수가 표준과는 다른 맥락에서 표현식을 평가할 때 발생한다.\n\n준인용(Quasiquotation): enquo() 함수를 사용하여 표현식을 캡처하고 !!를 사용하여 표현식의 인용제거(Unquoting)을 가능케 한다.\nPronouns (대명사): .data 대명사는 데이터프레임의 칼럼명을 명시적으로 참조하는데 사용되어 모호성을 제거한다.\n함수: enquo()는 표현식을 캡처하고, quo_name()은 표현식을 문자열로 변환하며, !!는 표현식 인용제거 또는 주입작업을 수행한다.\n\n예를 들어, dplyr 패키지 filter 및 select와 같은 동사를 사용하지만 함수에 칼럼명을 작성하려는 경우, 인수로 전달될 때 이러한 동사가 어떤 칼럼을 참조하는지 명확히 하기 위해 깔끔한 평가(tidy evaluation)가 사용된다.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nfilter_and_select &lt;- function(data, col_name, threshold) {\n  \n  # 칼럼명 문자열을 기호로 변환\n  col_sym &lt;- sym(col_name)\n  \n  # 준인용(quasiquotation)을 사용해서 칼럼 표현식을 캡쳐\n  col_expr &lt;- enquo(col_sym)\n  \n  # !! 연산자를 이용하여 인용제거(unquote)하고 표현식을 주입\n  data %&gt;% \n    filter(!!col_expr &gt; threshold) %&gt;% \n    select(!!col_expr)\n}\n\nfilter_and_select(penguins, \"bill_length_mm\", 55)\n#&gt; # A tibble: 5 × 1\n#&gt;   bill_length_mm\n#&gt;            &lt;dbl&gt;\n#&gt; 1           59.6\n#&gt; 2           55.9\n#&gt; 3           55.1\n#&gt; 4           58  \n#&gt; 5           55.8\n\nsym() → enquo() → !!(뱅뱅) 구현방식이 Defuse-and-Inject 패턴으로 내부 동작방식은 동일하지만 사용자 구문은 { 칼럼명 }으로 깔끔해졌다.\n\nfilter_and_select_latest &lt;- function(data, col_name, threshold) {\n  \n  data %&gt;% \n    filter({{ col_name }} &gt; threshold) %&gt;% \n    select({{ col_name }})\n  \n}\n\nfilter_and_select_latest(penguins, bill_length_mm, 55)\n#&gt; # A tibble: 5 × 1\n#&gt;   bill_length_mm\n#&gt;            &lt;dbl&gt;\n#&gt; 1           59.6\n#&gt; 2           55.9\n#&gt; 3           55.1\n#&gt; 4           58  \n#&gt; 5           55.8\n\n\n데이터 마스킹: 변수를 사용하여 계산하는 arrange(), filter(), summarize() 등 함수에 사용.\nTidy-selection: 변수를 선택하는 select(), relocate(), rename()과 같은 함수에 사용.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#그룹별-평균",
    "href": "functions_ds.html#그룹별-평균",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.5 그룹별 평균",
    "text": "18.5 그룹별 평균\n\n데이터프레임 요약통계량을 계산하는 코드를 작성해보자. 펭귄종(species) 별로 부리길이를 계산하는 코드를 작성해보자.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(부리길이_평균 = mean(bill_length_mm, na.rm = TRUE))\n#&gt; # A tibble: 3 × 2\n#&gt;   species   부리길이_평균\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Adelie             38.8\n#&gt; 2 Chinstrap          48.8\n#&gt; 3 Gentoo             47.5\n\n이번에는 그룹변수와 데이터프레임 칼럼명을 달리하여 그룹별로 평균을 계사하는 함수를 작성하여 코드로 작성해보자.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by(group_varname) |&gt; \n    summarise(부리길이_평균 = mean(varname, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n#&gt; Error in `group_by()`:\n#&gt; ! Must group by variables found in `.data`.\n#&gt; ✖ Column `group_varname` is not found.\n\n상기 코드가 동작하지 않는 이유는 함수에 tidyverse 코드를 함수에 단순히 전달해서 넣기 때문에 발생했다. 다음과 같이 칼러명을 포용(embracing)하는 방식으로 { 칼럼명 }과 같이 함수에 사용되는 데이터프레임 변수명을 명시적으로 작성할 경우 문제가 해결된다.\n\nget_group_mean &lt;- function(dataframe, group_varname, varname) {\n  dataframe |&gt; \n    group_by( {{ group_varname }} ) |&gt; \n    summarise(부리길이_평균 = mean( {{ varname }}, na.rm = TRUE))\n}\n\nget_group_mean(penguins, species, bill_length_mm)\n#&gt; # A tibble: 3 × 2\n#&gt;   species   부리길이_평균\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Adelie             38.8\n#&gt; 2 Chinstrap          48.8\n#&gt; 3 Gentoo             47.5",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_ds.html#그래프",
    "href": "functions_ds.html#그래프",
    "title": "\n18  데이터 과학 함수\n",
    "section": "\n18.6 그래프",
    "text": "18.6 그래프\n\n데이터프레임에서 범주형 변수를 하나 선택하여 빈도수를 시각화하는 스크립트를 다음과 같이 작성한다.\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins |&gt; \n  count(island) |&gt; \n  ggplot(aes(x=island, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n범주형 변수를 막대그래프로 시각화하는 함수를 제작해보자.\n\ndraw_bar_chart &lt;- function(dataframe, varname) {\n  dataframe |&gt; \n    count( {{ varname }} ) |&gt; \n    ggplot(aes(x = {{ varname }}, y = n )) + \n      geom_col()\n}\n\npenguins |&gt; draw_bar_chart(year)\n\n\n\n\n\n\n\n함수내에서 새로운 변수 이름을 생성하는 경우 := 연산자를 사용해야 한다. 깔끔한 평가(tidy evaluation)에서 = 와 동일한 역할을 수행하는 것이 := 이기 때문이다. 예를 들어, 펭귄이 서식하고 있는 섬을 기준으로 빈도수를 내림차순 막대그래프를 작성할 경우, 함수 내부에서 범주형 변수를 다시 재정의해야 하는데 이 경우 := 연산자의 도입이 필요하다.\n\nlibrary(forcats)\n\norder_bar_chart &lt;- function(dataframe, varname) {\n  dataframe |&gt; \n    mutate({{ varname }} :=  fct_rev(fct_infreq( {{ varname }} ))) |&gt;\n    ggplot(aes(y = {{ varname }} )) + \n      geom_bar()\n}\n\npenguins |&gt; order_bar_chart(island)\n\n\n\n\n\n\n\n그래프를 작성할 때 거의 항상 등장하는 문제가 그래프 x-축, y-축 라벨을 붙이고 그래프 제목, 범례 등 텍스트를 넣어야 한다. 이런 경우 stringr, glue 패키지의 다양한 함수를 깔끔한 평가(tidy evaluation)에서 지원하는 기능이 rlang 패키지 englue() 함수다.\n\ndraw_bar_chart &lt;- function(dataframe, varname, penguin_species) {\n  \n  title_label &lt;- rlang::englue(\"남극에 서식하고 있는 펭귄 종 ({{ varname }}) {penguin_species} 빈도수\")\n  \n  dataframe |&gt; \n    filter({{ varname }} == penguin_species) |&gt; \n    count( island ) |&gt; \n    ggplot(aes(x = island, y = n )) + \n      geom_col() +\n      labs(title = title_label)\n}\n\npenguins |&gt; draw_bar_chart(species, \"Adelie\")\n\n\n\n\n\n\n\n\n\n\n\nBecker, Richard. 2018. The new S language. CRC Press.\n\n\nChambers, J. M., 와/과 T. J. Hastie. 1992. Statistical Models in S. London: Chapman & Hall.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>데이터 과학 함수</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html",
    "href": "functions_purrr.html",
    "title": "19  함수형 프로그래밍",
    "section": "",
    "text": "19.1 왜 함수형 프로그래밍인가?\n데이터 분석을 아주 추상화해서 간략하게 얘기한다면 데이터프레임을 함수에 넣어 새로운 데이터프레임으로 만들어 내는 것이다.\n데이터 분석, 데이터 전처리, 변수 선택, 모형 개발이 한번에 해결되는 것이 아니라서, 데이터프레임을 함수에 넣어 상태가 변경된 데이터프레임이 생성되고, 이를 다시 함수에 넣어 또다른 변경된 상태 데이터프레임을 얻게 되는 과정을 쭉 반복해 나간다.\n따라서… 데이터 분석에는 함수형 프로그래밍 패러다임을 활용하고, 툴/패키지 개발에는 객체지향 프로그래밍 패러다임 사용이 권장된다.",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#why-functional-programming",
    "href": "functions_purrr.html#why-functional-programming",
    "title": "19  함수형 프로그래밍",
    "section": "",
    "text": "함수로 이해하는 데이터 분석\n\n\n\n\n\n\n\n데이터 분석 작업흐름\n\n\n\n\n\n\n\n데이터 분석과 툴/패키지 도구 개발",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-newton",
    "href": "functions_purrr.html#functional-programming-newton",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.2 뉴튼 방법(Newton’s Method)\n",
    "text": "19.2 뉴튼 방법(Newton’s Method)\n\n뉴튼-랩슨 알고리즘으로도 알려진 뉴튼(Newton Method) 방법은 컴퓨터를 사용해서 수치해석 방법으로 실함수의 근을 찾아내는 방법이다.\n특정 함수 \\(f\\) 의 근을 찾을 경우, 함수 미분값 \\(f'\\), 초기값 \\(x_0\\)가 주어지면 근사적 근에 가까운 값은 다음과 같이 정의된다.\n\\[x_{1} = x_0 - \\frac{f(x_0)}{f'(x_0)}\\]\n이 과정을 반복하게 되면 오차가 매우 적게 근의 값에 도달하게 된다.\n\\[x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\\]\n기하적으로 보면, 파란 선은 함수 \\(f\\) 이고, \\(f\\)를 미분한 \\(f'\\) 빨간 선은 뉴턴방법을 활용하여 근을 구해가는 과정을 시각적으로 보여주고 있다. \\(x_{n-1}\\) 보다 \\(x_n\\)이, \\(x_n\\) 보다 \\(x_{n+1}\\)이 함수 \\(f\\) 근에 더 가깝게 접근해 나가는 것이 확인된다.\n\n\n뉴튼 방법 도식화",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-newton-method",
    "href": "functions_purrr.html#functional-programming-newton-method",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.3 뉴튼 방법 R 코드 1\n",
    "text": "19.3 뉴튼 방법 R 코드 1\n\n뉴튼 방법을 R코들 구현하면 다음과 같이 612의 제곱근 값을 수치적으로 컴퓨터를 활용하여 구할 수 있다. while같은 루프를 활용하여 반복적으로 해를 구하는 것도 가능하지만 재귀를 활용하여 해를 구하는 방법이 코드를 작성하고 읽는 개발자 관점에서는 훨씬 더 편리하고 권장된다.\n하지만, 속도는 while 루프를 사용하는 것이 R에서는 득이 많다. 이유는 오랜 세월에 걸쳐 최적화 과정을 거쳐 진화했기 때문이다.\n\n\nwhile 루프를 사용한 방법\n\nfind_root &lt;- function(guess, init, eps = 10^(-10)){\n    while(abs(init**2 - guess) &gt; eps){\n        init &lt;- 1/2 *(init + guess/init)\n        cat(\"현재 값: \", init, \"\\n\")\n    }\n    return(init)\n}\n\nfind_root(612, 10)\n#&gt; 현재 값:  35.6 \n#&gt; 현재 값:  26.39551 \n#&gt; 현재 값:  24.79064 \n#&gt; 현재 값:  24.73869 \n#&gt; 현재 값:  24.73863 \n#&gt; 현재 값:  24.73863\n#&gt; [1] 24.73863\n\n\n\n재귀를 사용한 방법\n\nfind_root_recur &lt;- function(guess, init, eps = 10^(-10)){\n    if(abs(init**2 - guess) &lt; eps){\n        return(init)\n    } else{\n        init &lt;- 1/2 *(init + guess/init)\n        cat(\"재귀방법 현재 값: \", init, \"\\n\")\n        return(find_root_recur(guess, init, eps))\n    }\n}\n\nfind_root_recur(612, 10)\n#&gt; 재귀방법 현재 값:  35.6 \n#&gt; 재귀방법 현재 값:  26.39551 \n#&gt; 재귀방법 현재 값:  24.79064 \n#&gt; 재귀방법 현재 값:  24.73869 \n#&gt; 재귀방법 현재 값:  24.73863 \n#&gt; 재귀방법 현재 값:  24.73863\n#&gt; [1] 24.73863",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#map-reduce-apply",
    "href": "functions_purrr.html#map-reduce-apply",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.4 Map(), Reduce()와 *apply() 함수",
    "text": "19.4 Map(), Reduce()와 *apply() 함수\n함수를 인자로 받는 함수를 고차함수(High-order function)라고 부른다. 대표적으로 Map(), Reduce()가 있다. 숫자 하나가 아닌 벡터에 대한 제곱근을 구하기 위해서 Map 함수를 사용한다. 2 3\n\n# 제곱근 함수 -------------------------------------------\n\nfind_root_recur &lt;- function(guess, init, eps = 10^(-10)){\n    if(abs(init**2 - guess) &lt; eps){\n        return(init)\n    } else{\n        init &lt;- 1/2 *(init + guess/init)\n        return(find_root_recur(guess, init, eps))\n    }\n}\n\n# 벡터에 대한 제곱근 계산 \n\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nMap(find_root_recur, numbers, init=1, eps = 10^-10)\n#&gt; [[1]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 5\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 6\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 8\n#&gt; \n#&gt; [[6]]\n#&gt; [1] 9\n\n숫자 하나를 받는 함수가 아니라, 벡터를 인자로 받아 제곱근을 계산하는 함수를 작성할 경우 함수 내부에서 함수를 인자로 받을 수 있도록 Map 함수를 활용한다.\n\n# `Map` 벡터 제곱근 계산\n\nfind_vec_root_recur &lt;- function(numbers, init, eps = 10^(-10)){\n    return(Map(find_root_recur, numbers, init, eps))\n}\n\nnumbers_z &lt;- c(9, 16, 25, 49, 121)\nfind_vec_root_recur(numbers_z, init=1, eps=10^(-10))\n#&gt; [[1]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 5\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 11\n\n이러한 패턴이 많이 활용되어 *apply 함수가 있어, 이전에 많이 사용했을 것이다. 벡터를 인자로 먼저 넣고, 함수명을 두번째 인자로 넣고, 함수에 들어갈 매개변수를 순서대로 쭉 나열하여 lapply, sapply 함수에 넣는다.\n\n# `lapply` 활용 제급근 계산\n\nlapply(numbers_z, find_root_recur, init=1, eps=10^(-10))\n#&gt; [[1]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 5\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 11\nsapply(numbers_z, find_root_recur, init=1, eps=10^(-10))\n#&gt; [1]  3  4  5  7 11\n\nReduce 함수도 삶을 편안하게 할 수 있는, 루프를 회피하는 또다른 방법이다. 이름에서 알 수 있듯이 numbers_z 벡터 원소 각각에 대해 해당 연산작업 +, %%을 수행시킨다. %%는 나머지 연산자로 기본디폴트 설정으로 \\(\\frac{10}{7}\\)로 몫 대신에 나머지 3을 우선 계산하고, 그 다음으로 \\(\\frac{3}{5}\\)로 최종 나머지 3을 순차적으로 계산하여 결과를 도출한다.\n\n# Reduce ----------------------------------------------\nnumbers_z\n#&gt; [1]   9  16  25  49 121\nReduce(`+`, numbers_z)\n#&gt; [1] 220\n\nnumbers_z &lt;- c(10,7,5)\nReduce(`%%`, numbers_z)\n#&gt; [1] 3",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr",
    "href": "functions_purrr.html#functional-programming-purrr",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.5 purrr 팩키지",
    "text": "19.5 purrr 팩키지\n*apply 계열 함수는 각각의 자료형에 맞춰 기억하기가 쉽지 않아, 매번 도움말을 찾아 확인하고 코딩을 해야하는 번거러움이 많다. 데이터 분석을 함수형 프로그래밍 패러다임으로 실행하도록 purrr 팩키지가 개발되었다. 이를 통해 데이터 분석 작업이 수월하게 되어 저녁이 있는 삶이 길어질 것으로 기대된다.\n\n19.5.1 purrr 헬로월드\npurrr 팩키지를 불러와서 map_dbl() 함수에 구문에 맞게 작성하면 동일한 결과를 깔끔하게 얻을 수 있다. 즉,\n\n\nmap_dbl(): 벡터, 데이터프레임, 리스트에 대해 함수를 원소별로 적용시켜 결과를 double 숫자형으로 출력시킨다.\n\nnumbers: 함수를 각 원소별로 적용시킬 벡터 입력값\n\nfind_root_recur: 앞서 작성한 뉴톤 방법으로 제곱근을 수치적으로 구하는 사용자 정의함수.\n\ninit=1, eps = 10^-10: 뉴톤 방법을 구현한 사용자 정의함수에 필요한 초기값.\n\n\nlibrary(purrr)\nnumbers &lt;- c(16, 25, 36, 49, 64, 81)\nmap_dbl(numbers, find_root_recur, init=1, eps = 10^-10)\n#&gt; [1] 4 5 6 7 8 9",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-read-iris",
    "href": "functions_purrr.html#functional-programming-purrr-read-iris",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.6 병렬 데이터 분석",
    "text": "19.6 병렬 데이터 분석\n구글 검색을 통해서 쉽게 iris(붓꽃) 데이터를 구할 수 있다. 이를 불러와서 각 종별로 setosa versicolor, virginica로 나눠 로컬 .csv 파일로 저장하고 나서 이를 다시 불러오는 사례를 함수형 프로그래밍으로 구현해본다.\n\n\n붓꽃 데이터 불러오기\n\n먼저 iris.csv 파일을 R로 불러와서 각 종별로 나눠서 iris_종명.csv 파일형식으로 저장시킨다.\n\nlibrary(tidyverse)\niris_df &lt;- read_csv(\"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv\")\n\niris_species &lt;- iris_df %&gt;% \n  count(species) %&gt;% pull(species)\n\nfor(i in 1:nrow(iris_df)) {\n  tmp_df &lt;- iris_df %&gt;% \n    filter(species == iris_species[i])\n  species_name &lt;- iris_species[i]\n  tmp_df %&gt;% write_csv(paste0(\"data/iris_\", species_name, \".csv\"))\n}\n\nSys.glob(\"data/iris_*.csv\")\n#&gt; [1] \"data/iris_NA.csv\"         \"data/iris_setosa.csv\"    \n#&gt; [3] \"data/iris_versicolor.csv\" \"data/iris_virginica.csv\"\n\n로컬 파일 iris_종명.csv 형식으로 저장된 데이터를 함수형 프로그래밍을 통해 불러와서 분석작업을 수행해보자. map() 함수를 사용해서 각 종별로 데이터를 깔끔하게 불러왔다.\niris_filename 벡터에 iris_종명.csv과 경로명이 포함된 문자열을 저장시켜 놓고 read_csv() 함수를 각 벡터 원소에 적용시켜 출력값으로 리스트 iris_list 객체를 생성시켰다.\n\niris_filename &lt;- c(\"data/iris_setosa.csv\", \"data/iris_versicolor.csv\", \"data/iris_virginica.csv\")\n\niris_list &lt;- map(iris_filename, read_csv) %&gt;% \n  set_names(iris_species)\n\niris_list |&gt; enframe()\n#&gt; # A tibble: 3 × 2\n#&gt;   name       value              \n#&gt;   &lt;chr&gt;      &lt;list&gt;             \n#&gt; 1 setosa     &lt;spc_tbl_ [50 × 5]&gt;\n#&gt; 2 versicolor &lt;spc_tbl_ [50 × 5]&gt;\n#&gt; 3 virginica  &lt;spc_tbl_ [50 × 5]&gt;\n\niris_list 각 원소는 데이터프레임이라 summary 함수를 사용해서 기술 통계량을 구할 수도 있다. 물론 cor() 함수를 사용해서 iris_list의 각 원소를 지정하는 .x 여기서는 종별 데이터프레임에서 변수 두개를 추출하여 sepal_length, sepal_width 이 둘간의 스피커만 상관계수를 계산하는데 출력값이 double 연속형이라 map_dbl로 저정하여 작업시킨다.\n\nmap(iris_list, summary)\n#&gt; $setosa\n#&gt;   sepal_length    sepal_width     petal_length    petal_width   \n#&gt;  Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100  \n#&gt;  1st Qu.:4.800   1st Qu.:3.125   1st Qu.:1.400   1st Qu.:0.200  \n#&gt;  Median :5.000   Median :3.400   Median :1.500   Median :0.200  \n#&gt;  Mean   :5.006   Mean   :3.418   Mean   :1.464   Mean   :0.244  \n#&gt;  3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300  \n#&gt;  Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600  \n#&gt;    species         \n#&gt;  Length:50         \n#&gt;  Class :character  \n#&gt;  Mode  :character  \n#&gt;                    \n#&gt;                    \n#&gt;                    \n#&gt; \n#&gt; $versicolor\n#&gt;   sepal_length    sepal_width     petal_length   petal_width   \n#&gt;  Min.   :4.900   Min.   :2.000   Min.   :3.00   Min.   :1.000  \n#&gt;  1st Qu.:5.600   1st Qu.:2.525   1st Qu.:4.00   1st Qu.:1.200  \n#&gt;  Median :5.900   Median :2.800   Median :4.35   Median :1.300  \n#&gt;  Mean   :5.936   Mean   :2.770   Mean   :4.26   Mean   :1.326  \n#&gt;  3rd Qu.:6.300   3rd Qu.:3.000   3rd Qu.:4.60   3rd Qu.:1.500  \n#&gt;  Max.   :7.000   Max.   :3.400   Max.   :5.10   Max.   :1.800  \n#&gt;    species         \n#&gt;  Length:50         \n#&gt;  Class :character  \n#&gt;  Mode  :character  \n#&gt;                    \n#&gt;                    \n#&gt;                    \n#&gt; \n#&gt; $virginica\n#&gt;   sepal_length    sepal_width     petal_length    petal_width   \n#&gt;  Min.   :4.900   Min.   :2.200   Min.   :4.500   Min.   :1.400  \n#&gt;  1st Qu.:6.225   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800  \n#&gt;  Median :6.500   Median :3.000   Median :5.550   Median :2.000  \n#&gt;  Mean   :6.588   Mean   :2.974   Mean   :5.552   Mean   :2.026  \n#&gt;  3rd Qu.:6.900   3rd Qu.:3.175   3rd Qu.:5.875   3rd Qu.:2.300  \n#&gt;  Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  \n#&gt;    species         \n#&gt;  Length:50         \n#&gt;  Class :character  \n#&gt;  Mode  :character  \n#&gt;                    \n#&gt;                    \n#&gt; \n\nmap_dbl(iris_list, ~cor(.x$sepal_length, .x$sepal_width, method = \"spearman\"))\n#&gt;     setosa versicolor  virginica \n#&gt;  0.7686085  0.5176060  0.4265165",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-random-number",
    "href": "functions_purrr.html#functional-programming-purrr-random-number",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.7 표본추출",
    "text": "19.7 표본추출\n서로 다른 난수를 생성시키는 방법을 살펴보자. 정규분포를 가정하고 평균과 표준편차를 달리하는 모수를 지정하고 난수갯수도 숫자를 달리하여 난수를 생성시킨다.\n\n19.7.1 \\(\\mu\\) 평균 변화\n정규분포에서 난수를 10개 추출하는데 표준편차는 1로 고정시키고, 평균만 달리한다. 평균만 달리하기 때문에 map() 함수를 그대로 사용한다. 즉, 입력값으로 평균만 달리하는 리스트를 입력값으로 넣는다.\n\n## 평균을 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\n\nsim_mu_name &lt;- paste0(\"mu: \", normal_mean)\n\nsim_mu_list &lt;- map(normal_mean, ~ data.frame(mean = .x, \n                            random_number = rnorm(mean=.x, sd=1, n=10))) %&gt;% \n  set_names(sim_mu_name)\n\nmap_dbl(sim_mu_list, ~mean(.x$random_number))\n#&gt;      mu: 1      mu: 5     mu: 10 \n#&gt;  0.8770432  4.4837620 10.1274353\n\nsim_mu_list |&gt; enframe()\n#&gt; # A tibble: 3 × 2\n#&gt;   name   value        \n#&gt;   &lt;chr&gt;  &lt;list&gt;       \n#&gt; 1 mu: 1  &lt;df [10 × 2]&gt;\n#&gt; 2 mu: 5  &lt;df [10 × 2]&gt;\n#&gt; 3 mu: 10 &lt;df [10 × 2]&gt;\n\n\n19.7.2 \\(\\mu\\) 평균, \\(\\sigma\\) 표준편차\n난수갯수만 고정시키고 평균과 표준편차를 달리하여 난수를 정규분포에서 추출한다. 입력값으로 평균과 표준편차 두개가 되기 때문에 map2() 함수를 사용한다.\n\n## 평균과 표준편차를 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\nnormal_sd   &lt;- list(10,5,1)\n\nsim_mu_sd_name &lt;- paste0(\"mu: \", normal_mean, \",  sd: \", normal_sd)\n\nsim_mu_sd_list &lt;- map2(normal_mean, normal_sd, \n                        ~ data.frame(mean = .x, sd = .y,\n                            random_number = rnorm(mean=.x, sd=.y, n=10))) %&gt;% \n  set_names(sim_mu_sd_name)\n\nmap_dbl(sim_mu_sd_list, ~sd(.x$random_number))\n#&gt; mu: 1,  sd: 10  mu: 5,  sd: 5 mu: 10,  sd: 1 \n#&gt;      15.477497       7.385885       1.577088\n\nsim_mu_sd_list |&gt; enframe()\n#&gt; # A tibble: 3 × 2\n#&gt;   name           value        \n#&gt;   &lt;chr&gt;          &lt;list&gt;       \n#&gt; 1 mu: 1,  sd: 10 &lt;df [10 × 3]&gt;\n#&gt; 2 mu: 5,  sd: 5  &lt;df [10 × 3]&gt;\n#&gt; 3 mu: 10,  sd: 1 &lt;df [10 × 3]&gt;\n\n\n19.7.3 \\(\\mu\\), \\(\\sigma\\), 표본크기\n\\(\\mu\\) 평균, \\(\\sigma\\) 표준편차, 표본크기를 모두 다르게 지정하여 난수를 추출한다. 이런 경우 pmap() 함수를 사용하고 입력 리스트가 다수라 이를 normal_list로 한번더 감싸서 이름이 붙은 리스트(named list)형태로 넣어주고, 이를 function() 함수의 내부 인수로 사용한다.\n\n## 평균, 표준편차, 표본크기를 달리하는 경우\nnormal_mean &lt;- list(1,5,10)\nnormal_sd   &lt;- list(10,5,1)\nnormal_size &lt;- list(10,20,30)\n\nsim_mu_sd_size_name &lt;- paste0(\"mu: \", normal_mean, \",  sd: \", normal_sd,\n                              \"  size: \", normal_size)\n\nnormal_list &lt;- list(normal_mean=normal_mean, normal_sd=normal_sd, normal_size=normal_size)\n\nsim_mu_sd_size_list &lt;- pmap(normal_list,\n                            function(normal_mean, normal_sd, normal_size)\n                        data.frame(mean=normal_mean, sd = normal_sd, size = normal_size,\n                            random_number = rnorm(mean=normal_mean, sd=normal_sd, n=normal_size))) %&gt;% \n  set_names(sim_mu_sd_size_name)\n\nmap_dbl(sim_mu_sd_size_list, ~length(.x$random_number))\n#&gt; mu: 1,  sd: 10  size: 10  mu: 5,  sd: 5  size: 20 mu: 10,  sd: 1  size: 30 \n#&gt;                       10                       20                       30\n\nsim_mu_sd_size_list |&gt; enframe()\n#&gt; # A tibble: 3 × 2\n#&gt;   name                     value        \n#&gt;   &lt;chr&gt;                    &lt;list&gt;       \n#&gt; 1 mu: 1,  sd: 10  size: 10 &lt;df [10 × 4]&gt;\n#&gt; 2 mu: 5,  sd: 5  size: 20  &lt;df [20 × 4]&gt;\n#&gt; 3 mu: 10,  sd: 1  size: 30 &lt;df [30 × 4]&gt;",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#functional-programming-purrr-ggplot",
    "href": "functions_purrr.html#functional-programming-purrr-ggplot",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.8 ggplot 시각화",
    "text": "19.8 ggplot 시각화\nlist-column을 활용하여 티블(tibble) 데이터프레임에 담아서 시각화를 진행해도 되고, 다른 방법으로 리스트에 담아서 이를 한장에 찍는 것도 가능하다. 4\n\nlibrary(gapminder)\n\n## 데이터 -----\nthree_country &lt;-  c(\"Korea, Rep.\", \"Japan\", \"China\")\n\ngapminder_tbl &lt;- gapminder %&gt;% \n  filter(str_detect(continent, \"Asia\")) %&gt;% \n  group_by(continent, country) %&gt;% \n  nest() %&gt;% \n  select(-continent) %&gt;% \n  filter(country %in% three_country )\n\n## 티블 데이터 시각화 -----\ngapminder_plot_tbl &lt;- gapminder_tbl %&gt;% \n  mutate(graph = map2(data, country, \n                     ~ggplot(.x, aes(x=year, y=gdpPercap)) +\n                       geom_line() +\n                       labs(title=.y)))\n\ngapminder_plot_tbl\n#&gt; # A tibble: 3 × 4\n#&gt; # Groups:   continent, country [3]\n#&gt;   continent country     data              graph \n#&gt;   &lt;fct&gt;     &lt;fct&gt;       &lt;list&gt;            &lt;list&gt;\n#&gt; 1 Asia      China       &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n#&gt; 2 Asia      Japan       &lt;tibble [12 × 4]&gt; &lt;gg&gt;  \n#&gt; 3 Asia      Korea, Rep. &lt;tibble [12 × 4]&gt; &lt;gg&gt;\n\n## 리스트 데이터 시각화 -----\ngapminder_plot &lt;- map2(gapminder_tbl$data , three_country, \n                     ~ggplot(.x, aes(x=year, y=gdpPercap)) +\n                       geom_line() +\n                       labs(title=.y))\n\nwalk(gapminder_plot, print)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 리스트 데이터 시각화 - 한장에 찍기 -----\ncowplot::plot_grid(plotlist = gapminder_plot)",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#fp-theory-practice",
    "href": "functions_purrr.html#fp-theory-practice",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.9 함수형 프로그래밍 이론과 실제",
    "text": "19.9 함수형 프로그래밍 이론과 실제\n함수는 다음과 같이 될 수도 있어 함수형 프로그래밍 언어가 된다. 5\n\n함수의 인자\n함수로 반환\n리스트에 저장\n변수에 저장\n무명함수\n조작할 수 있다.\n\nFirstly, functional languages have first-class functions, functions that behave like any other data structure. In R, this means that you can do anything with a function that you can do with a vector: you can assign them to variables, store them in lists, pass them as arguments to other functions, create them inside functions, and even return them as the result of a function.\n\n\n\n\n\n\nJohn Chambers 창시자가 말하는 R 계산의 기본원칙\n\n\n\n\n존재하는 모든 것은 객체다. (Everything that exists is an object.)\n일어나는 모든 것은 함수호출이다. (Everything that happens is a function call.)\n\n\nlibrary(tidyverse)\nclass(`%&gt;%`)\n#&gt; [1] \"function\"\nclass(`$`)\n#&gt; [1] \"function\"\nclass(`&lt;-`)\n#&gt; [1] \"function\"\nclass(`+`)\n#&gt; [1] \"function\"",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#pure-vs-impure-function",
    "href": "functions_purrr.html#pure-vs-impure-function",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.10 순수한 함수 vs 불순한 함수",
    "text": "19.10 순수한 함수 vs 불순한 함수\n순수한 함수(pure function)는 입력값에만 출력값이 의존하게 되는 특성과 부수효과(side-effect)를 갖지 않는 반면 순수하지 않은 함수(impure function)는 환경에 의존하며 부수효과도 갖는다.\n\n\n순수한 함수(pure function)\n\nmin(1:100)\n#&gt; [1] 1\n\nmean(1:100)\n#&gt; [1] 50.5\n\n\n순수하지 않은 함수(impure function)\n\nSys.time()\n#&gt; [1] \"2024-03-30 16:36:38 KST\"\n\nrnorm(10)\n#&gt;  [1] -0.78652204 -1.10631230  0.33855390  0.00653906 -1.06369148  1.02831818\n#&gt;  [7] -1.16109585 -1.17524358  0.50474407 -0.20784798\n\n# write_csv(\"data/sample.csv\")\n\n\n\n\n19.10.1 무명함수와 매퍼\n\\(\\lambda\\) (람다) 함수는 무명(anonymous) 함수는 함수명을 갖는 일반적인 함수와 비교하여 함수의 좋은 점은 그대로 누리면서 함수가 많아 함수명으로 메모리가 난잡하게 지져분해지는 것을 막을 수 있다.\n무명함수로 기능르 구현한 후에 매퍼(mapper)를 사용해서 as_mapper() 명칭을 부여하여 함수처럼 사용하는 것도 가능하다. 매퍼(mapper)를 사용하는 이유를 다음과 같이 정리할 수 있다.\n\n간결함(Concise)\n가독성(Easy to read)\n재사용성(Reusable)\n\n정치인 페이스북 페이지에서 팬수를 추출한다. 그리고 이를 이름이 부은 리스트(named list)로 일자별 팬수 추이를 리스트로 준비한다. 그리고 나서 안철수, 문재인, 심상정 세 후보에 대한 최고 팬수증가를 무명함수로 계산한다.\n\nlibrary(tidyverse)\n## 데이터프레임을 리스트로 변환\nahn_df  &lt;- read_csv(\"data/fb_ahn.csv\")  %&gt;% rename(fans = ahn_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nmoon_df &lt;- read_csv(\"data/fb_moon.csv\") %&gt;% rename(fans = moon_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\nsim_df  &lt;- read_csv(\"data/fb_sim.csv\")  %&gt;% rename(fans = sim_fans) %&gt;% \n  mutate(fans_lag = lag(fans),\n         fans_diff = fans - fans_lag) %&gt;% \n  select(fdate, fans = fans_diff) %&gt;% \n  filter(!is.na(fans))\n\nconvert_to_list &lt;- function(df) {\n  df_fans_v &lt;- df$fans %&gt;% \n    set_names(df$fdate)\n  return(df_fans_v)\n}\n\nahn_v  &lt;- convert_to_list(ahn_df)\nmoon_v &lt;- convert_to_list(moon_df)\nsim_v  &lt;- convert_to_list(sim_df)\n\nfans_lst &lt;- list(ahn_fans  = ahn_v,\n                 moon_fans = moon_v,\n                 sim_fans  = sim_v)\n\nlistviewer::jsonedit(fans_lst)\n\n\n\n\n\n## 무명함수 테스트\nmap_dbl(fans_lst, ~max(.x))\n#&gt;  ahn_fans moon_fans  sim_fans \n#&gt;       796      1464      2029\n\nrlang_lambda_function 무명함수로 increase_1000_fans 작성해서 일별 팬수 증가가 1000명 이상인 경우 keep() 함수를 사용해서 각 후보별로 추출할 수 있다. discard() 함수를 사용해서 반대로 버려버릴 수도 있다.\n\nincrease_1000_fans &lt;- as_mapper( ~.x &gt; 1000)\n\nmap(fans_lst, ~keep(.x, increase_1000_fans))\n#&gt; $ahn_fans\n#&gt; named numeric(0)\n#&gt; \n#&gt; $moon_fans\n#&gt; 2017-03-28 2017-04-18 2017-04-20 \n#&gt;       1464       1310       1093 \n#&gt; \n#&gt; $sim_fans\n#&gt; 2017-03-12 2017-03-13 2017-04-14 2017-04-19 2017-04-20 2017-04-21 2017-04-24 \n#&gt;       1301       1079       1070       1441       1190       1025       1948 \n#&gt; 2017-04-25 \n#&gt;       2029\n\n술어논리(predicate logic)은 조건을 테스트하여 참(TRUE), 거짓(FALSE)을 반환시킨다. every, some을 사용하여 팬수가 증가한 날이 매일 1,000명이 증가했는지, 전부는 아니고 일부 특정한 날에 1,000명이 증가했는지 파악할 수 있다.\n\n## 세후보 팬수가 매일 모두 1000명 이상 증가했나요?\nmap(fans_lst, ~every(.x, increase_1000_fans))\n#&gt; $ahn_fans\n#&gt; [1] FALSE\n#&gt; \n#&gt; $moon_fans\n#&gt; [1] FALSE\n#&gt; \n#&gt; $sim_fans\n#&gt; [1] FALSE\n## 세후보 팬수가 전체는 아니고 일부 특정한 날에 1000명 이상 증가했나요?\nmap(fans_lst, ~some(.x, increase_1000_fans))\n#&gt; $ahn_fans\n#&gt; [1] FALSE\n#&gt; \n#&gt; $moon_fans\n#&gt; [1] TRUE\n#&gt; \n#&gt; $sim_fans\n#&gt; [1] TRUE\n\n\n19.10.2 고차 함수(High order function)\n고차 함수(High order function)는 함수의 인자로 함수를 받아 함수로 반환시키는 함수를 지칭한다. high_order_fun 함수는 함수를 인자(func)로 받아 함수를 반환시키는 고차함수다. 평균 함수(mean)를 인자로 넣어 출력값으로 mean_na() 함수를 새롭게 생성시킨다. NA가 포함된 벡터를 넣어 평균값을 계산하게 된다.\n\nhigh_order_fun &lt;- function(func){\n  function(...){\n    func(..., na.rm = TRUE)\n  }\n}\n\nmean_na &lt;- high_order_fun(mean)\nmean_na( c(NA, 1:10) )\n#&gt; [1] 5.5\n\n벡터가 입력값으로 들어가서 벡터가 출력값으로 나오는 보통 함수(Regular Function)외에 고차함수는 3가지 유형이 있다.\n\n벡터 → 함수: 함수공장(Function Factory)\n함수 → 벡터: Functional - for루프를 purrr 팩키지 map() 함수로 대체\n함수 → 함수: 함수연산자(Function Operator) - Functional과 함께 사용될 경우 adverbs로서 강력한 기능을 발휘\n\n\n\n\n\n\n고차함수 유형\n\n\n19.10.3 부사 - safely, possibly,…\npurrr 팩키지의 대표적인 부사(adverbs)에는 possibly()와 safely()가 있다. 그외에도 silently(), surely() 등 다른 부사도 있으니 필요한 경우 purrr 팩키지 문서를 참조한다.\nsafely(mean)은 동사 함수(mean())를 받아 부사 safely()로 “부사 + 동사”로 기능이 추가된 부사 동사를 반환시킨다. 따라서, NA가 추가된 벡터를 넣을 경우 $result와 $error를 원소로 갖는 리스트를 반환시킨다.\n\nmean_safe &lt;- safely(mean)\nclass(mean_safe)\n#&gt; [1] \"function\"\n\nmean_safe(c(NA, 1:10))\n#&gt; $result\n#&gt; [1] NA\n#&gt; \n#&gt; $error\n#&gt; NULL\n\n이를 활용하여 오류처리작업을 간결하게 수행시킬 수 있다. $result와 $error을 원소로 갖는 리스트를 반환시키기 때문에 오류와 결과값을 추출하여 후속작업을 수행하여 디버깅하는데 유용하게 활용할 수 있다.\n\ntest_lst &lt;- list(\"NA\", 1,2,3,4,5)\nlog_safe &lt;- safely(log)\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"result\")\n#&gt; [[1]]\n#&gt; NULL\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0.6931472\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 1.098612\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 1.386294\n#&gt; \n#&gt; [[6]]\n#&gt; [1] 1.609438\n\nmap(test_lst, log_safe) %&gt;% \n  map(\"error\")\n#&gt; [[1]]\n#&gt; &lt;simpleError in .Primitive(\"log\")(x, base): 수학함수에 숫자가 아닌 인자가 전달되었습니다&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; NULL\n#&gt; \n#&gt; [[3]]\n#&gt; NULL\n#&gt; \n#&gt; [[4]]\n#&gt; NULL\n#&gt; \n#&gt; [[5]]\n#&gt; NULL\n#&gt; \n#&gt; [[6]]\n#&gt; NULL\n\n반면에 possibly()는 결과와 otherwise 값을 반환시켜서 오류가 발생되면 중단되는 것이 아니라 오류가 있다는 사실을 알고 예외처리시킨 후에 쭉 정상진행시킨다.\n\nmax_possibly &lt;- possibly(sum, otherwise = \"watch out\")\n\nmax_possibly(c(1:10))\n#&gt; [1] 55\n\nmax_possibly(c(NA, 1:10))\n#&gt; [1] NA\n\nmax_possibly(c(\"NA\", 1:10))\n#&gt; [1] \"watch out\"\n\npossibly()는 부울 논리값, NA, 문자열, 숫자를 반환시킬 수 있다.\ntranspose()와 결합하여 safely(), possibly() 결과를 변형시킬 수도 있다.\n\nmap(test_lst, log_safe) %&gt;% length()\n#&gt; [1] 6\n\nmap(test_lst, log_safe) %&gt;% transpose() %&gt;% length()\n#&gt; [1] 2\n\ncompact()를 사용해서 NULL을 제거하는데, 앞서 possibly()의 인자로 otherwise=를 지정하는 경우 otherwise=NULL와 같이 정의해서 예외처리로 NULL을 만들어 내고 compact()로 정상처리된 데이터만 얻는 작업흐름을 갖춘다.\n\nnull_lst &lt;- list(1, NULL, 3, 4, NULL, 6, 7, NA)\ncompact(null_lst)\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 6\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 7\n#&gt; \n#&gt; [[6]]\n#&gt; [1] NA\n\npossibly_log &lt;- possibly(log, otherwise = NULL)\nmap(null_lst, possibly_log) %&gt;% compact()\n#&gt; [[1]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 1.098612\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 1.386294\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 1.791759\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 1.94591\n#&gt; \n#&gt; [[6]]\n#&gt; [1] NA",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#fp-clean-code",
    "href": "functions_purrr.html#fp-clean-code",
    "title": "19  함수형 프로그래밍",
    "section": "\n19.11 깨끗한 코드",
    "text": "19.11 깨끗한 코드\nround_mean() 함수를 compose() 함수를 사용해서 mean() 함수로 평균을 구한 후에 round()함수로 반올림하는 코드를 다음과 같이 쉽게 작성할 수 있다. 6\n\nround_mean &lt;- compose(round, mean)\nround_mean(1:10)\n#&gt; [1] 6\n\n두번째 사례로 전형적인 데이터 분석 사례로 lm() → anova() → tidy()를 통해 한방에 선형회귀 모형 산출물을 깨끗한 코드로 작성하는 사례를 살펴보자.\nmtcars 데이터셋에서 연비 예측에 변수 두개를 넣고 일반적인 lm() 선형예측모형 제작방식과 동일하게 인자를 넣는다.\n\nclean_lm &lt;- compose(broom::tidy, anova, lm)\nclean_lm(mpg ~ hp + wt, data=mtcars)\n#&gt; # A tibble: 3 × 6\n#&gt;   term         df sumsq meansq statistic   p.value\n#&gt;   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 hp            1  678. 678.       101.   5.99e-11\n#&gt; 2 wt            1  253. 253.        37.6  1.12e- 6\n#&gt; 3 Residuals    29  195.   6.73      NA   NA\n\ncompose()를 통해 함수를 조합하는 경우 함수의 인자를 함께 전달해야될 경우가 있다. 이와 같은 경우 partial()을 사용해서 인자를 넘기는 함수를 제작하여 compose()에 넣어준다.\n\nrobust_round_mean &lt;- compose(\n  partial(round, digits=1),\n  partial(mean, na.rm=TRUE))\nrobust_round_mean(c(NA, 1:10))\n#&gt; [1] 5.5\n\n리스트 칼럼(list-column)과 결합하여 모형에서 나온 데이터 분석결과를 깔끔하게 코드로 제작해보자. 먼저 lm을 돌려 모형 요약하는 함수 summary를 통해 r.squared값을 추출하는 함수를 summary_lm으로 제작한다.\n그리고 나서 nest() 함수로 리스트 칼럼(list-column)을 만들고 두개의 집단 수동/자동을 나타내는 am 변수를 그룹으로 삼아 두 집단에 속한 수동/자동 데이터에 대한 선형 회귀모형을 적합시키고 나서 “r.squared”값을 추출하여 이를 티블 데이터프레임에 저장시킨다.\n\nsummary_lm &lt;- compose(summary, lm) \n\nmtcars %&gt;%\n  group_by(am) %&gt;%\n  nest() %&gt;%\n  mutate(lm_mod = map(data, ~ summary_lm(mpg ~ hp + wt, data = .x)),\n         r_squared = map(lm_mod, \"r.squared\")) %&gt;%\n  unnest(r_squared)\n#&gt; # A tibble: 2 × 4\n#&gt; # Groups:   am [2]\n#&gt;      am data               lm_mod     r_squared\n#&gt;   &lt;dbl&gt; &lt;list&gt;             &lt;list&gt;         &lt;dbl&gt;\n#&gt; 1     1 &lt;tibble [13 × 10]&gt; &lt;smmry.lm&gt;     0.837\n#&gt; 2     0 &lt;tibble [19 × 10]&gt; &lt;smmry.lm&gt;     0.768\n\n\n\n\n함수로 이해하는 데이터 분석\n데이터 분석 작업흐름\n데이터 분석과 툴/패키지 도구 개발\n뉴튼 방법 도식화\n붓꽃 데이터 불러오기\n고차함수 유형",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "functions_purrr.html#footnotes",
    "href": "functions_purrr.html#footnotes",
    "title": "19  함수형 프로그래밍",
    "section": "",
    "text": "Bruno Rodrigues(2016), “Functional programming and unit testing for data munging with R”, LeanPub, 2016-12-23↩︎\npurrr tutorial: Lessons and Examples↩︎\npurrr tutorial GitHub Webpage↩︎\nVery statisticious (August 20, 2018), “Automating exploratory plots with ggplot2 and purrr”↩︎\nAdvanced R, “Introduction”↩︎\nColin Fay, “A Crazy Little Thing Called {purrr} - Part 5: code optimization”↩︎",
    "crumbs": [
      "**5부** 프로그래밍",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>함수형 프로그래밍</span>"
    ]
  },
  {
    "objectID": "comm_map.html",
    "href": "comm_map.html",
    "title": "\n20  지도 공간정보\n",
    "section": "",
    "text": "20.1 도구의 진화\nGIS, 좌표계, 파일 형식, 계층(Layer), 교차분석 등 주요개념을 바탕으로 도구 진화과정을 살펴보자. 초기 PC가 보급되면서 1980년대 GIS 시스템은 사일로 형태 고가 시스템이었으며, 1990년대에는 1980년대 개발된 GIS 시스템간 연결이 시작되면서 스파게티 현상이 심해지면서 새로운 애플리케이션과 파일 형식이 도입되는데 필요 이상의 낭비가 심해지는 구조가 되었다.\n2000년대 GDAL(Geospatial Data Abstraction Layer)이 등장하여 개발자가 각 파일 형식별로 서로 다른 드라이버를 작성하는 대신 GDAL 클라이언트 드라이버만 개발하면 되기 때문에 일대 혁신이 일어났다. 2010년대는 인공위성을 통한 대량의 데이터가 넘쳐나며 이를 저장하기 위해서 클라우드 서비스가 우후죽순처럼 생겨났고 각기 다른 파편화된 진화과정이 일어났다. 2020년대 넘어서면서 1990년대와 유사한 상황이 재현되었고, 이를 해결하고 새로운 전환점을 만들기 위해 OpenEO가 제시되며 사용자와 데이터 센터 사이에서 새로운 표준으로 자리잡게 되면서 R, 파이썬 등 데이터 과학 프로그래밍 언어를 통한 공간정보 가치 창출이 용이해졌다. (Pebesma 기타 2016)",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_map.html#도구의-진화",
    "href": "comm_map.html#도구의-진화",
    "title": "\n20  지도 공간정보\n",
    "section": "",
    "text": "GIS 도구 진화과정",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_map.html#gis-생태계",
    "href": "comm_map.html#gis-생태계",
    "title": "\n20  지도 공간정보\n",
    "section": "\n20.2 GIS 생태계",
    "text": "20.2 GIS 생태계\n인간은 오랜 역사 동안 다양한 도구를 개발하고 사용해 왔으며, GIS에서도 도구는 단순한 계산부터 복잡한 데이터 분석, 시각화를 통한 의사결정지원, 최근 거대언어모형(LLM) 인공지능까지 다양한 형태로 기여하고 있다.\n현재 직면한 문제를 해결하는데 과거 도구를 사용하는 것은 마치 철기시대에 석기시대 도구를 사용하는 것과 다름이 없다. 따라서, 데이터를 통해 가치를 만드는 데이터 과학자 입장에서 도구의 선택은 매우 중요할 수 밖에 없다.\n\n\n\n\n\n그림 20.2: sf 패키지와 의존성 - 화살표는 강한 의존성, 점선 화살표는 약한 의존성\n\n\n현재 R 공간정보 생태계는 공간정보 도구 진화과정을 몇차례 경험한 후에 그림 20.2 처럼 자리를 잡았다. 결론부터 들어가면 sf 패키지가 생태계의 중심으로 자리 잡았으며, 사용자와 개발자는 sf 패키지를 통해 상당수 공간정보 데이터 문제를 해결할 수 있게 되었다. 하지만, sf 패키지는 C/C++ 라이브러리에 크게 의존성하기 때문에 각 라이브러리를 살펴보는 것은 향후 GIS 개발자와 분석가로 현업에서 활약하는데 큰 도움이 될 것으로 보인다.\nGDAL, GEOS, PROJ, liblwgeom, s2geometry, NetCDF, udunits2는 C/C++ 라이브러리를 개발, 유지보수, 지원하는 커뮤니티가 존재하는 반면 R, 파이썬, 줄리아(Julia), 자바스크립트는 대화형 인터페이스를 통해 라이브러리를 활용하는 커뮤니티도 존재한다. (Pebesma 와/과 Bivand 2023)\n\n\nGDAL (Geospatial Data Abstraction Library) 라이브러리는 공간 데이터 처리에 있어 멀티툴의 대명사인 스위스군 칼(SAK, Swiss Army Knife)이라는 별명을 갖고 있고 100개가 넘는 다른 라이브러리와 연결되어 다양한 공간 데이터를 불러오고, 처리하고, 내보내는 기능을 제공한다.\n\nPROJ는 지도 투영 및 데이터 변환을 위한 라이브러리로, 하나의 좌표계에서 다른 좌표계로 좌표를 변환할 때 유용하다.\n\nGEOS (Geometry Engine Open Source)와 s2geometry 라이브러리는 기하학 연산에 사용하며, 길이, 면적, 거리를 측정하거나 연산작업에 사용되며 \\(R^2\\)로 표기되며 GEOS는 평평한 2차원 공간에, \\(S^2\\)로 표기되며 s2geometry는 구형 공간에 사용한다.\n\nNetCDF는 파일 형식이며 C 라이브러리로, 어떤 차원 배열도 정의할 수 있으며 특히 (기후) 모형개발 커뮤니티에서 널리 사용된다. Udunits2는 측정 단위의 데이터베이스 및 소프트웨어 라이브러리로, 단위의 변환과 파생 단위, 사용자 정의 단위를 처리한다. liblwgeom은 PostGIS 구성 요소로, GDAL, GEOS에서 누락된 몇 가지 루틴을 포함한다. (Pebesma 기타 2016)",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_map.html#사례",
    "href": "comm_map.html#사례",
    "title": "\n20  지도 공간정보\n",
    "section": "\n20.3 사례",
    "text": "20.3 사례\nsf 패키지가 추상화된 함수를 제공하기 때문에 R 공간정보 생태계에서 GDAL 라이브러리를 GIS 분석가가 직접적으로 다룰 일은 없다. 하지만, GIS 도구를 활용하여 2023년 7월 기준 시도별 인구수를 대한민국 지도위에 도식화하기 위해서는 R 공간정보 생태계를 구성하는 다양한 도구가 꼭 필요하다. 먼저, 지도 데이터를 다루기 위해 sf 패키지를 사용하고 통계청(KOSIS) 데이터를 처리하기 위해 tidyverse 패키지 도구를 활용하고 오픈지도 개발자가 공개한 geojson 파일을 결합하여 인구수를 시도 수준에서 시도별 인구수 색상을 달리하여 지도위에 시각화한다. 작성된 코드는 R로 작성되었지만 파이썬 진영에도 공간정보 생태계도 유사한 도구가 준비되어 있어 각자 사용하기 좋은 도구를 가지고 의미있는 결과물을 만들어내고 있다.\n\nlibrary(sf)\nlibrary(tidyverse)\nsf_use_s2(FALSE)\n\n## 지도\nkorea_map &lt;- read_sf(\"data/HangJeongDong_ver20230401.geojson\")\n\nsido_map &lt;- korea_map |&gt; \n  group_by(sidonm) |&gt; \n  summarise(geometry = sf::st_union(geometry))\n\n## 23년 7월 인구수(KOSIS) 행정구역별, 성별 인구수\npop_tbl &lt;- read_csv(\"data/행정구역_시군구_별__성별_인구수_20230831223248.csv\",\n         locale=locale(encoding=\"euc-kr\"), skip = 1) |&gt; \n  set_names(c(\"sidonm\", \"인구수\")) |&gt; \n  mutate(sidonm = if_else(sidonm == \"강원특별자치도\", \"강원도\", sidonm))\n\nsigo_gg &lt;- sido_map |&gt; \n  left_join(pop_tbl) |&gt; \n  ggplot() +\n    geom_sf(aes(geometry = geometry, fill = cut(인구수, 10)), show.legend = FALSE) +\n    ggrepel::geom_label_repel(aes(label = sidonm, geometry = geometry), \n                              size = 3, stat = \"sf_coordinates\") +\n    theme_void() +\n    scale_fill_brewer(palette = \"OrRd\")\n\nragg::agg_jpeg(\"images/GIS_tools.jpeg\",\n               width = 10, height = 7, units = \"in\", res = 600)\nsigo_gg\ndev.off()\n\n\n\nGIS 도구 활용 대한민국 시도별 인구수",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_map.html#마무리",
    "href": "comm_map.html#마무리",
    "title": "\n20  지도 공간정보\n",
    "section": "\n20.4 마무리",
    "text": "20.4 마무리\n공간정보 개발자와 사용자가 개발하는 코드는 기계보다 사람 친화적으로 바뀌었으며, 효과적인 디버그와 신속한 개발을 위한 파이프 철학도 도입되어 생산성 향상이 비약적으로 높아졌고 도구의 추상화 수준도 대폭 향상되었다. IoT와 인공위성을 통해 엄청난 공간정보 데이터가 축적되고 있지만, GIS 도구가 꾸준히 발전하면서 이제 누구나 이러한 도구를 활용하여 공간정보 데이터를 통해 의미있는 산출물을 제작하고 도구도 개발할 수 있게 되었고, 다른 한편으로는 도구 없이 도구에 대한 이해없이 프롭테크를 논하는 것조차 의미없는 시대로 접어들고 있다.\n\n\n\n그림 20.1: GIS 벡터와 래스터 계층, 출처: http://gis.sbcounty.gov/\nGIS 도구 진화과정\n그림 20.2: sf 패키지와 의존성 - 화살표는 강한 의존성, 점선 화살표는 약한 의존성\nGIS 도구 활용 대한민국 시도별 인구수\n\n\n\nPebesma, Edzer, 와/과 Roger Bivand. 2023. Spatial Data Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian Briese, 와/과 Markus Neteler. 2016. “OpenEO: a GDAL for Earth Observation Analytics”. 2016년. https://r-spatial.org/2016/11/29/openeo.html.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법”. 프롭빅스(PROPBIX), 호 13 (9월). http://www.kahps.org/.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html",
    "href": "comm_knitr.html",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "",
    "text": "21.1 문학적 프로그래밍\n이상적으로는 그런 분석 보고서가 재현 가능한 문서가 되어야 한다. 만약 오류가 발견되거나 추가 분석 주제가 데이터에 추가되면, 다시 컴파일하여 새롭게 수정된 결과를 얻을 수 있어야 한다. (반대로 워드나 한글 등 오피스 프로그램으로 작업하면 그림을 다시 생성하고 문서에 붙여넣고 수작업으로 상세 결과를 편집해야 한다.)\nR에서 문학적 프로그래밍을 구현하는 주된 도구가 knitr이다. knitr는 텍스트와 R 코드가 뒤섞인 문서를 생성할 수 있게 한다. 문서가 knitr로 처리되면 R 코드가 실행되어 그래프와 분석 결과가 문서에 자동으로 삽입된다.\n이런 유형의 아이디어를 문학적 프로그래밍(literate programming)이라고 부른다.\nknitr는 R 코드뿐만 아니라 다양한 프로그래밍 언어를 텍스트와 뒤섞을 수 있도록 하지만, 기본적으로 R 마크다운을 추천한다. R 마크다운은 마크다운과 R 코드를 함께 사용할 수 있도록 한다. 마크다운은 가벼운 마크업 언어로 웹페이지를 생성하는 데 사용된다.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#r-마크다운-파일-생성",
    "href": "comm_knitr.html#r-마크다운-파일-생성",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.2 R 마크다운 파일 생성",
    "text": "21.2 R 마크다운 파일 생성\nRStudio에서 File → New File → R Markdown을 클릭하면 다음과 같은 대화상자가 열린다.\n\n\n\n\n\n그림 21.1: 쿼토 문서 생성 시작\n\n\n기본 설정(HTML 출력)된 대로 사용할 수도 있고, 제목을 입력할 수도 있다.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#r-마크다운-기본-요소",
    "href": "comm_knitr.html#r-마크다운-기본-요소",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.3 R 마크다운 기본 요소",
    "text": "21.3 R 마크다운 기본 요소\n초기 설정된 텍스트 청크에는 R에 대한 지시 사항이 담겨 있다. 제목, 저자명, 날짜, HTML 출력 여부(다른 말로, 웹페이지 생성 여부)가 포함된다.\n---\ntitle: \"Initial R Markdown document\"\nauthor: \"Karl Broman\"\ndate: \"April 23, 2015\"\noutput: html_document\n---\n원하지 않는 경우 상기 필드를 임의로 삭제할 수 있다. 엄밀히 말해서 인용 부호는 상기 경우에 꼭 필요한 것은 아니다. 제목에 콜론(:)이 포함되는 경우 대체로 인용 부호가 필요하다.\nRStudio에는 시작을 돕기 위한 예제 문서가 포함되어 있다. 다음과 같은 R 코드 청크(chunk)가 포함되어 있는 점을 주목한다.\n```{r}\nsummary(cars)\n```\n\n이것은 knitr로 실행되는 R 코드 청크로, R 코드 실행 결과로 치환된다. 뒤에서 더 자세히 다루겠다.\n또한, 웹주소는 &lt; &gt; 꺾쇠 기호로 담아내고, **Knit**처럼 별표 두개를 사용하기도 한다. 이것이 전형적인 Markdown 구문의 한 사례가 된다.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#마크다운",
    "href": "comm_knitr.html#마크다운",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.4 마크다운",
    "text": "21.4 마크다운\nHTML 코드로 직접 작성하는 대신 전자 우편에서 문서를 작성하는 것처럼 텍스트에 마크업을 적용해서 웹페이지를 저작하는 시스템이 마크다운이다. 마크업 텍스트는 적절한 HTML 코드로 치환되어 최종 HTML로 변환된다.\n지금은 자동 생성된 모든 코드를 삭제하고, 마크다운으로 저작을 시작해 보자.\n별표 두 개를 사용해서 굵게할 수 있는데 **굵게**와 같이 타이핑하면 된다. 밑줄 혹은 별표 한 개를 사용해서 이탤릭도 구현할 수 있는데, _이탤릭_이나 *이탤릭*과 같이 타이핑하면 된다.\n다음과 같이 하이픈이나 별표를 적용해서 블릿 기호가 붙은 항목을 생성할 수 있다.\n- 별표 두 개로 굵게 표시\n- 밑줄로 이탤릭 표시\n- 백틱으로 코드 서체 표시  \n혹은 다음과 같이도 가능하다:\n* 별표 두 개로 굵게 표시\n* 밑줄로 이탤릭 표시  \n* 백틱으로 코드 서체 표시\n다음과 같이 웹페이지로 출력된다.\n\n별표 두 개로 굵게 표시\n밑줄로 이탤릭 표시\n백틱으로 코드 서체 표시\n\n숫자를 사용하면 번호가 매겨진 항목도 생성할 수 있다. 원하는 만큼 동일한 숫자를 반복해서 사용하면 된다.\n1. 별표 두 개로 굵게 표시\n1. 밑줄로 이탤릭 표시\n1. 백틱으로 코드 서체 표시\n다음과 같이 출력된다.\n\n별표 두 개로 굵게 표시\n\n밑줄로 이탤릭 표시\n백틱으로 코드 서체 표시\n\n# 기호를 각 줄 첫 번째에 사용하면 섹션 제목을 만들 수 있으며, #의 개수에 따라 제목의 크기가 달라진다.\n# 아주 큰 제목\n## 큰 제목 \n### 작은 제목\n#### 아주 작은 제목\n좌측 상단에 있는 “Knit” 버튼을 클릭하면 R 마크다운 문서가 HTML 웹페이지로 컴파일된다. 바로 옆에 작은 아래쪽 화살표 버튼이 있음에 주목한다. 클릭하면 “Markdown Quick Reference”(마크다운 구문 안내)와 RStudio IDE에서의 R 마크다운 문서 작성법을 참고할 수 있다.\n\n\n\n\n\n\n도전 과제\n\n\n\nR 마크다운 문서를 새로 생성한다. 모든 R 코드 청크를 삭제하고 마크다운 문법을 사용하여 문서를 작성한다. (제목, 이탤릭 텍스트, 블릿 기호가 붙은 항목 등)\n작성한 문서를 웹페이지로 변환한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\nRStudio에서 File &gt; New file &gt; R Markdown…을 선택한다. 기본으로 입력된 텍스트를 삭제하고 다음 내용을 추가한다:\n\n# 서론\n\n## 데이터 배경\n\n이 보고서는 *gapminder* 데이터셋을 사용하며, 포함된 열은 다음과 같다:\n\n* country (국가)\n* continent (대륙)\n* year (연도)\n* lifeExp (기대수명)\n* pop (인구)\n* gdpPercap (1인당 GDP)\n\n## 분석 방법 배경",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#마크다운-심화",
    "href": "comm_knitr.html#마크다운-심화",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.5 마크다운 심화",
    "text": "21.5 마크다운 심화\n[text to show](http://the-web-page.com)와 같이 하이퍼링크를 추가할 수 있다. ![caption](http://url/for/file)과 같이 이미지를 삽입할 수 있다. 아래 첨자는 F~2~와 같이, 윗첨자는 F^2^와 같이 작성한다.\n수식은 LaTeX 문법을 사용하여 $E = mc^2$처럼 $ $ 사이에 작성한다. 별도 줄로 수식을 쓰려면 $$ $$를 사용한다.\n\n$$y = \\mu + \\sum_{i=1}^p \\beta_i x_i + \\epsilon$$",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#r-코드-청크",
    "href": "comm_knitr.html#r-코드-청크",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.6 R 코드 청크",
    "text": "21.6 R 코드 청크\n마크다운은 흥미롭고 유용하지만, 실제 진정한 힘은 마크다운을 R 코드 청크와 함께 사용할 때 발휘된다. 이를 R 마크다운이라고 한다. R 마크다운이 처리되면 R 코드가 실행되고, 생성된 그림이 최종 문서에 자동으로 삽입된다.\n데이터를 불러오는 R 코드는 다음과 같이 생겼다:\n```{r load_data}\ngapminder &lt;- read.csv(\"~/Desktop/gapminder.csv\")\n```\n\n즉, ```{r chunk_name}와 ``` 사이에 R 코드 청크를 위치시킨다. 각 청크에는 고유한 이름을 부여해야 하는데, 이는 오류를 수정하는 데 도움이 되고 그래프가 생성되는 경우 파일명이 해당 그래프를 생성한 코드 청크의 이름을 기반으로 하기 때문이다. RStudio에서는 단축키를 사용하여 빠르게 코드 청크를 생성할 수 있다. Windows와 Linux에서는 Ctrl+Alt+I를, Mac에서는 Cmd+Option+I를 사용한다.\n\n\n\n\n\n\n도전 과제\n\n\n\n다음 R 코드를 문서에 추가한다.\n\n\nggplot2 패키지를 불러오는 코드\n\ngapminder 데이터를 불러오는 코드\n\n그래프를 생성하는 코드\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n```{r}\nlibrary(\"ggplot2\")\n```\n```{r}\ngapminder\n```\n```{r}\nplot(lifeExp ~ year, data = gapminder)\n```",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#컴파일-과정",
    "href": "comm_knitr.html#컴파일-과정",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.7 컴파일 과정",
    "text": "21.7 컴파일 과정\n“Knit” 버튼을 클릭하면 knitr 프로그램이 R 마크다운 문서를 처리하여 일반 마크다운 문서(그림 포함)를 생성한다. R 코드가 실행되면서 입력과 출력이 모두 치환되고, 생성된 그림에 대한 링크도 포함된다.\n마크다운 문서와 그림은 pandoc 도구에 의해 처리되어 그림이 내장된 HTML 파일로 변환된다.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#청크-옵션",
    "href": "comm_knitr.html#청크-옵션",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.8 청크 옵션",
    "text": "21.8 청크 옵션\n코드 청크가 처리되는 방식을 제어하는 다양한 옵션이 있다.\n\n\necho=FALSE: R 코드 출력을 숨긴다.\n\nresults=\"hide\": 출력 결과물이 문서에 포함되지 않도록 한다.\n\n\neval=FALSE: 코드는 출력되지만 실행되지는 않는다.\n\nwarning=FALSE, message=FALSE: 경고와 메시지 출력을 숨긴다.\n\nfig.height, fig.width: 인치 단위로 그림의 높이와 폭을 지정한다.\n\n다음과 같이 작성할 수 있다.\n```{r load_libraries, echo=FALSE, message=FALSE}\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\n```\n\n전역 청크 옵션을 사용해 반복적으로 사용되는 R 마크다운 옵션을 다음과 같이 지정할 수도 있다.\n```{r global_options, echo=FALSE}\nknitr::opts_chunk$set(fig.path=\"Figs/\", message=FALSE, warning=FALSE,\n                      echo=FALSE, results=\"hide\", fig.width=11)\n```\n\nfig.path 옵션은 그림이 저장되는 경로를 지정한다. /를 사용하지 않으면 Figs로 시작하는 파일명으로 기본 저장소에 그림이 저장된다.\n작업 디렉터리에 R 마크다운 파일이 여러 개 있는 경우 fig.path=\"Figs/cleaning-\", fig.path=\"Figs/analysis-\"와 같이 그림 파일명 접두사를 다르게 지정할 수 있다.\n\n\n\n\n\n\n도전 과제\n\n\n\n그림 크기를 변경하고 코드 출력을 감추도록 R 코드 청크 옵션을 설정한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n```{r}\n#| echo: false\nplot(faithful)\n```",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#인라인-r-코드",
    "href": "comm_knitr.html#인라인-r-코드",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.9 인라인 R 코드",
    "text": "21.9 인라인 R 코드\n보고서의 모든 숫자를 재현 가능하게 만들 수 있다. 인라인 코드를 작성할 때는 `r `와 `를 사용한다. 예를 들어 `{r} round(some_value, 2)`와 같이 한다. 이 R 코드가 실행되어 결과값으로 치환된다.\n두 줄 이상에 걸쳐 인라인 코드 청크를 나누어 작성하지 않도록 주의한다.\n약간 긴 코드의 경우 include=FALSE(echo=FALSE와 results=\"hide\"를 합친 것)를 사용하여 계산과 할당을 처리하는 것이 좋다.\n이런 경우 소수점 자리수를 원하는 대로 조정할 수 있다. 예를 들어 2.0을 원한다면 round(2.03, 1) 대신 적절한 포맷팅 함수를 사용해야 한다.\nR/broman 패키지의 myround 함수가 도움이 될 수 있다.\n\n\n\n\n\n\n도전 과제\n\n\n\n인라인 R 코드를 사용해 본다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n다음은 2 + 2 = {r} 2+2임을 보여주는 인라인 코드다.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_knitr.html#기타-출력-옵션",
    "href": "comm_knitr.html#기타-출력-옵션",
    "title": "\n21  데이터 분석 보고서\n",
    "section": "\n21.10 기타 출력 옵션",
    "text": "21.10 기타 출력 옵션\nR 마크다운 문서를 PDF나 워드 문서로 변환할 수도 있다. “Knit” 버튼 옆의 작은 아래쪽 화살표를 클릭하면 드롭다운 메뉴가 나타난다. 혹은 R 마크다운 .Rmd 파일의 헤더 부분에서 output 옵션을 pdf_document나 word_document로 설정하면 된다.\n\n\n\n\n\n\nPDF 문서 생성하기\n\n\n\nPDF 문서를 생성하려면 추가 소프트웨어를 설치해야 한다. 필요한 소프트웨어가 설치되어 있지 않은 상태에서 PDF 문서 생성을 시도하면 상세한 오류 메시지가 출력된다.\n\n\nTeX installers for Windows\nTeX installers for macOS\n\n\n\n\n\n\n\n그림 21.1: 쿼토 문서 생성 시작",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>데이터 분석 보고서</span>"
    ]
  },
  {
    "objectID": "comm_quarto.html",
    "href": "comm_quarto.html",
    "title": "22  쿼토",
    "section": "",
    "text": "쿼토를 사용하면 저자들은 코드, 문서 요소, 텍스트를 하나의 문서로 매끄럽게 통합할 수 있어데이터 분석 과정을 투명하고 재현 가능하게 공유할 수 있다.\n쿼토 문서는 마치 살아있는 것처럼 역동적이다. R, 파이썬, 줄리아, SQL, JS 코드를 수정하면 분석결과와 데이터 시각화가 자동으로 업데이트되고, 문장을 고치면 전체 문서의 흐름이 자연스럽게 되어 문서 작성이 한층 더 생생해지는 경험을 할 수 있고 무엇보다 문서의 외양보다 콘텐츠 자체 집중할 수 있다.\n쿼토는 보고서, 학술논문, 프레젠테이션, 대시보드 등 다양한 형식으로 문서를 제작할 수 있을 뿐만 아니라, PDF, Word, HTML, 슬라이드와 인터랙티브 웹페이지까지 개발할 수 있어 작성한 문서를 상황에 맞게 유연하게 공유할 수 있다.\n쿼토는 데이터 과학자에게 코딩과 글쓰기를 자유롭게 오가며 통합적으로 사고할 수 있는 환경을 제공하여 AI 데이터 과학 시대 최적의 커뮤니케이션 도구로 평가받고 있다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“챗GPT 쿼토” 책\n\n\n\n이 책은 쿼토(Quarto) 대한 기본적인 내용을 다루고 있다. 과학기술 디지털 글쓰기에 대한 기본적인 지식을 습득하고 싶은 분들에게 추천한다.\n📘 교보 POD 종이책: https://bit.ly/3wElKuk  📗 교보 전자책: https://bit.ly/3IkWE6f  🌐 웹사이트: https://bit.ly/3P7PVjX  💻 소스코드: https://bit.ly/3InK46m",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "comm_export.html",
    "href": "comm_export.html",
    "title": "\n23  저장하기\n",
    "section": "",
    "text": "23.1 그림 저장하기\nggsave 명령어를 사용해서, ggplot2에서 생성한 가장 최신 도식화 결과물을 저장하는 방법을 이미 살펴봤다. 다시 상기시키기 위해 명령어를 적어보면 다음과 같다:\n# eval: false\nggsave(\"My_most_recent_plot.pdf\")\nRStudio 내부에서 그림을 저장할 경우, ‘Plot’ 윈도우에서 ‘Export’ 버튼을 사용한다. 버튼을 클릭하면 .pdf, .png, .jpg 혹은 다른 이미지 형식으로 저장할지 선택 옵션이 제시된다.\n종종 ‘Plot’ 윈도우에 먼저 찍어보지 말고 도표를 저장하고 싶을 때도 있다. 아마도 여러 페이지에 걸친 PDF 문서를 생성하고 싶을 것이다. 예를 들어, 각각은 다른 도표로 말이다. 혹은 다수 파일에서 부분집합으로 데이터를 뽑아내고, 각 하위 데이터에 대해 도식화를 하고, 결과물을 도표로 저장하고자 한다. 하지만 각각에 대해 ‘Export’ 버튼을 클릭하려고 루프를 중단할 수는 없는 노릇이다.\n이런 경우 더 유연한 접근법을 사용할 수 있다. pdf 함수는 새로운 PDF 장치를 생성한다. pdf 함수에 여러 인자를 사용해서 크기와 해상도를 조절할 수 있다.\npdf(\"Life_Exp_vs_time.pdf\", width=12, height=4)\nggplot(data=gapminder, aes(x=year, y=lifeExp, colour=country)) +\n  geom_line() +\n  theme(legend.position = \"none\")\n\n# 그런 다음에는 반드시 PDF 장치를 끄는 것을 잊지 말아야 한다!\ndev.off()\n저장한 문서를 열어서 살펴본다.\njpeg, png 등의 명령어도 다양한 형식으로 문서를 저장하는 데 유사하게 사용할 수 있다.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>저장하기</span>"
    ]
  },
  {
    "objectID": "comm_export.html#그림-저장하기",
    "href": "comm_export.html#그림-저장하기",
    "title": "\n23  저장하기\n",
    "section": "",
    "text": "도전과제\n\n\n\n\npdf 명령어를 다시 작성해서 PDF 파일에 두 번째 페이지를 생성한다. 각 창 패널에 대륙별로 데이터를 패싯 도표(힌트: facet_grid 사용)로 출력한다.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n\npdf(\"Life_Exp_vs_time.pdf\", width = 12, height = 4)\n\np &lt;- ggplot(data = gapminder, aes(x = year, y = lifeExp, colour = country)) +\n  geom_line() +\n  theme(legend.position = \"none\")\np\n\np + facet_grid(. ~continent)\n\ndev.off()",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>저장하기</span>"
    ]
  },
  {
    "objectID": "comm_export.html#데이터를-파일-저장",
    "href": "comm_export.html#데이터를-파일-저장",
    "title": "\n23  저장하기\n",
    "section": "\n23.2 데이터를 파일 저장",
    "text": "23.2 데이터를 파일 저장\n어느 시점이 되면 R에서 데이터를 내보내서 파일에 저장하기도 한다. 이런 목적으로 write.table 함수를 사용하는데, 앞서 살펴본 read.table 함수와 매우 유사하다.\n데이터 정제 스크립트를 생성하자. gapminder 데이터에서 Australia 호주만 집중한다:\n\naust_subset &lt;- gapminder[gapminder$country == \"Australia\",]\n\nwrite.table(aust_subset,\n  file=\"cleaned-data/gapminder-aus.csv\",\n  sep=\",\"\n)\n\n쉘로 다시 전환해서 모든 것이 정상인지 데이터를 살펴본다.\n\n$ head cleaned-data/gapminder-aus.csv\n\n음… 엄밀하게 보면 원하는 바는 아니다. 이 모든 인용부호는 어디서 왔을까? 또한 행 번호도 보이는데 무의미하다.\n도움말 파일을 살펴보고 파일에 저장하는 방식을 변경해 본다.\n\n? write.table\n\n기본 설정으로 데이터를 파일에 저장할 때 R은 자동으로 인용부호로 문자 벡터를 감싼다. 행과 열 명칭도 파일에 저장한다.\n다음과 같이 고쳐본다:\n\nwrite.table(\n  gapminder[gapminder$country == \"Australia\",],\n  file=\"cleaned-data/gapminder-aus.csv\",\n  sep=\",\", quote=FALSE, row.names=FALSE\n)\n\n쉘 기술을 사용해서 다시 데이터를 살펴본다:\n\n$ head cleaned-data/gapminder-aus.csv\n\n훨씬 좋아 보인다!\n\n\n\n\n\n\n도전과제\n\n\n\n1990년 이후 수집된 데이터를 gapminder 데이터에서 부분집합으로 구성하는 데이터 정제 스크립트를 작성한다. 상기 스크립트를 사용해서 작업한 새로운 부분집합 데이터를 cleaned-data/ 디렉터리에 파일로 저장한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nwrite.table(\n   gapminder[gapminder$year &gt; 1990, ],\n   file = \"cleaned-data/gapminder-after1990.csv\",\n   sep = \",\", quote = FALSE, row.names = FALSE\n)\n\n\n\n\n\n\n\n# 수업 저장소에 이 디렉터리가 포함되는 것을 원치 않기 때문에,\n# cleaned-data 디렉터리를 삭제한다.\nunlink(\"cleaned-data\", recursive=TRUE)",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>저장하기</span>"
    ]
  },
  {
    "objectID": "ingest_database.html",
    "href": "ingest_database.html",
    "title": "24  데이터베이스",
    "section": "",
    "text": "graph LR\n    subgraph 가져오기[\"가져오기\"]\n        ascii[아스키 파일] --&gt; 핸들러_가져오기\n        스프레드시트[\"스프레드시트/통계패키지\"] --&gt; 핸들러_가져오기\n        데이터베이스 --&gt; 핸들러_가져오기\n        통계패키지 --&gt; 핸들러_가져오기    \n        웹[\"웹 데이터\"] --&gt; 핸들러_가져오기\n    end\n\n    핸들러_가져오기[\"핸들러\"] --&gt; import[가져오기]\n    \n    subgraph 이해하기[\"이해하기\"]\n        모형 --&gt; 시각화\n        변환 --&gt; 모형\n        시각화 --&gt; 변환\n    end\n\n    import --&gt; 깔끔화 --&gt; 이해하기\n    이해하기 --&gt; 의사소통\n\nclassDef modern fill:#fff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\nclassDef emphasize fill:#8CBDE3,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\nclassDef subgraphStyle fill:#f0f8ff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:15px;\n\nclass 깔끔화,변환,모형,시각화,의사소통,ascii,웹,스프레드시트,데이터베이스,통계패키지,핸들러_가져오기 modern;\nclass import,데이터베이스 emphasize;\nclass 이해하기,가져오기 subgraphStyle;\n\n\n\n\n\n그림 24.1: 데이터 과학 작업흐름도\n\n\n\n\n\n\n\n파일 형태 데이터는 로컬 컴퓨터에 저장되어 있어 데이터를 불러오기 쉽지만, 데이터의 크기가 커지면 로컬 컴퓨터 저장공간이 부족하여 서버에 데이터를 저장하게 된다. 데이터베이스에 저장된 데이터는 데이터베이스 관리 시스템(DBMS)을 통해 관리되며, 여러 사용자와 개발자들에게 데이터 관련 서비스를 제공하기 위해 인증단계와 권한을 달리 부여하여 활용된다.\n데이터베이스에서 R로 데이터를 가져오는 자세한 기술적인 사항은 “챗GPT SQL” 책을 참조한다. 대표적인 정형 데이터베이스에서 데이터를 가져오는 방법을 주로 다루고 있고, 특히 DuckDB는 속도를 중요시하는 데이터 과학자에게 추천한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“챗GPT SQL” 책\n\n\n\n이 책은 데이터베이스와 SQL에 대한 기본적인 내용을 다루고 있다. 데이터베이스와 SQL에 대한 기본적인 지식을 습득하고 싶은 분들에게 추천한다.\n📘 교보 POD 종이책: https://bit.ly/3OJmMeT  📗 교보 전자책: https://bit.ly/48fujZD  🌐 웹사이트: https://bit.ly/48gYn7d  💻 소스코드: https://bit.ly/3wrgeuP",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터베이스</span>"
    ]
  },
  {
    "objectID": "ingest_file.html",
    "href": "ingest_file.html",
    "title": "25  파일 데이터",
    "section": "",
    "text": "25.1 유니코드와 UTF-8\n사람 간의 의사소통은 다양한 기호 체계를 통해 이루어진다. 영어 알파벳, 한글, 한자 등 문자가 의사소통에 사용되는 좋은 예이다. 디지털 환경에서 이러한 의사소통을 가능하게 하는 기술적 장치가 바로 문자 집합과 문자 인코딩 및 디코딩이다.\n컴퓨터 시스템은 이진수 바이트를 기본 단위로 사용한다. 바이트는 파일 형태로 묶이거나 네트워크를 통해 전송되어 다른 시스템에 도달한다. 이 데이터가 사람에게 의미 있는 정보로 전달되기 위해서는 인코딩(부호화)과 디코딩(복호화) 과정을 거쳐야 한다.\n컴퓨터 시스템은 데이터를 바이트(Byte) 형태로 처리한다. 이 바이트 데이터는 이진수, 즉 010101과 같은 형태로 표현되고, 바이트 데이터를 사람이 읽을 수 있는 문자로 변환하는 최초의 표준이 ASCII(아스키)다. 하지만 ASCII는 256개 문자만을 지원하기 때문에, CJK(중국, 일본, 한국)와 같은 동아시아 문화권에서는 그 한계가 명확하다. 이러한 한계를 해결하기 위해 유니코드(Unicode)가 도입되었다. 유니코드는 영문자는 물론이고 지구상의 거의 모든 문자와 기호를 디지털로 표현할 수 있는 방법을 제공한다.\n유니코드(Unicode)는 글자와 코드가 1:1 매핑되어 있는 단순한 코드표에 불과하고 산업 표준으로 일종의 국가 당사자 간 약속이다. 한글이 표현된 유니코드 영역도 위키백과 유니코드 영역에서 찾을 수 있다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#유니코드와-utf-8",
    "href": "ingest_file.html#유니코드와-utf-8",
    "title": "25  파일 데이터",
    "section": "",
    "text": "유니코드와 UTF-8\n\n\n\n\n\n\n\n인코딩 (Encoding)\n\n\n\n문자 인코딩(character encoding), 줄여서 인코딩은 사용자가 입력한 문자나 기호들을 컴퓨터가 이용할 수 있는 신호로 만드는 것을 말한다. 넓은 의미의 컴퓨터는 이러한 신호를 입력받고 처리하는 기계를 뜻하며, 신호 처리 시스템을 통해 이렇게 처리된 정보를 사용자가 이해할 수 있게 된다.\n\nAll text has a character encoding.\n\n\n\n\n25.1.1 인코딩 문제\n문자 인코딩은 컴퓨터가 텍스트를 바이트로 변환하거나 바이트를 텍스트로 변환하는 방법이다. 인코딩 과정에서는 다양한 문제가 발생할 수 있고, 그중 세 가지 문제가 많이 알려져 있다. 첫 번째는 ’두부(Tofu)’라 불리는 상황으로, 컴퓨터가 어떤 문자를 표현해야 할지 알지만, 화면에 어떻게 출력해야 할지 모르기 때문에 빈 사각형 상자로 표시된다. 두 번째는 ’문자 깨짐(Mojibake, 文字化け)’이다. 특히 일본어에서 자주 발생하며, 한 인코딩 방식으로 작성된 텍스트가 다른 인코딩 방식으로 해석될 때 문자가 깨지는 현상을 의미한다. 세 번째는 ’의문부호(Question Marks)’로, 특정 문자가 다른 문자로 변환될 때 발생된다. 문자 집합과 인코딩 궁합이 맞지 않을 때 발생하며, 데이터 손실과 오류도 야기된다.\n\n\n세 가지 인코딩 문제\n\n\n25.1.2 문자 집합\n아스키 코드\n디지털 글쓰기는 내용과 상관없이 결국 텍스트로 표현되고, 텍스트는 단지 문자이다. 하지만, 컴퓨터가 문자 하나를 어떻게 표현할까?\n1960년대 미국식 영문자를 컴퓨터로 표현하는 해결책은 간단했다. 알파벳 26개(대문자, 소문자), 숫자 10개, 구두점 몇 개, 그리고 전신을 보내던 시절에 제어를 위해 사용된 몇 개의 특수 문자(“새줄로 이동”, “본문 시작”, “경고음” 등)가 전부였다. 모두 합쳐도 128개보다 적어서, 아스키(ASCII) 위원회가 문자마다 7비트( \\(2^7\\) = 128)를 사용하는 인코딩으로 표준화했다. 1\n\n\n\n\n\n그림 25.1: 제어문자와 출력 가능한 아스키 문자표 알파벳 예시\n\n\n그림 25.1 는 아스키 문자표에서 제어문자 10개와 출력 가능한 아스키 문자표 중 영문 대문자 A-I까지 10개를 뽑아 사례로 보여준다. 즉, 문자표는 어떤 문자가 어떤 숫자에 해당하는지를 정의하고 있다.\n확장 아스키\n아스키(ASCII) 방식으로 숫자 2, 문자 q, 혹은 곡절 악센트 ^를 표현하는 데 충분하다. 하지만, 투르크어족 추바시어 ĕ, 그리스 문자 β, 러시아 키릴 문자 Я는 어떻게 저장하고 표현해야 할까? 7비트를 사용하면 0에서 127까지 숫자를 부여할 수 있지만, 8비트(즉, 1바이트)를 사용하게 되면 255까지 표현할 수 있다. 그렇다면, ASCII 표준을 확장해서 추가되는 128개 숫자에 대해 추가로 문자를 표현할 수 있게 된다.\n\n아스키: 0…127\n확장된 아스키: 128…255\n\n불행하게도, 영어 문자를 사용하지 않는 세계 곳곳에서 많은 사람들이 시도를 했지만, 방식도 다르고, 호환이 되지 않는 방식으로 작업이 되어, 결과는 엉망진창이 되었다. 예를 들어, 실제 텍스트가 불가리아어로 인코딩되었는데 스페인어 규칙을 사용해서 인코딩한 것으로 프로그램이 간주하고 처리될 경우 결과는 무의미한 횡설수설 값이 출력된다. 이와는 별도로 한중일(CJK) 동아시아 국가들을 비롯한 많은 국가들이 256개 이상의 기호를 사용한다. 왜냐하면 8비트로는 특히 동아시아 국가 문자를 표현하는 데 부족하기 때문이다.\n한글 완성형과 조합형\n1980년대부터 컴퓨터를 사용하신 분이라면 완성형과 조합형의 표준화 전쟁을 지켜봤을 것이고, 그 이면에는 한글 워드프로세서에 대한 주도권 쟁탈전이 있었던 것을 기억할 것이다. 결국 완성형과 조합형을 모두 포용하는 것으로 마무리되었지만, 여기서 끝난 게 아니다. 유닉스 계열에서 KSC5601을 표준으로 받아들인 EUC-KR과 90년대와 2000년대를 호령한 마이크로소프트 CP949가 있었다. 결국 대한민국 정부에서 주도한 표준화 전쟁은 유닉스/리눅스, 마이크로소프트 모두를 녹여내는 것으로 마무리되었고, 웹과 모바일 시대는 유니코드로 넘어가서 KSC5601이 유니코드의 원소로 들어가는 것으로 마무리되었다.\n이제 신경 쓸 것은 인코딩, 즉 utf-8만 신경 쓰면 된다. 그리고 남은 디지털 레거시 유산을 잘 처리하면 된다.\n\n\n\n\n\n\n유닉스/리눅스(EUC-KR), 윈도우(CP949)\n\n\n\nEUC-KR, CP949 모두 2바이트 한글을 표현하는 방식으로 동일점이 있지만, EUC-KR 방식은 KSC5601-87 완성형을 초기에 사용하였으나, KSC5601-92 조합형도 사용할 수 있도록 확장되었다. CP949는 확장 완성형으로도 불리며 EUC-KR에서 표현할 수 없는 한글 글자 8,822자를 추가한 것으로 마이크로소프트 코드페이지(Code Page) 949를 사용하면서 일반화되었다.\n\n\n유니코드\n1990년대에 나타나기 시작한 해결책을 유니코드(Unicode)라고 부른다. 예를 들어, 영어 A 대문자는 1바이트, 한글 가는 3바이트다. 유니코드는 정수값을 서로 다른 수만 개 문자와 기호를 표현하는 데 정의한다. ’A’는 U+0041, ’가’는 U+AC00과 같이 고유한 코드 포인트를 가진다. 하지만, 파일에 혹은 메모리에 문자열로 정수값을 저장하는 방식을 정의하지는 않는다.\n각 문자마다 8비트를 사용하던 방식에서 32비트 정수를 사용하는 방식으로 전환하면 되지만, 영어, 에스토니아어, 브라질 포르투갈어 같은 알파벳 언어권에는 상당한 공간 낭비가 발생된다. 접근 속도가 중요한 경우 메모리에 문자당 32비트를 종종 사용한다. 하지만, 파일에 데이터를 저장하거나 인터넷을 통해 전송하는 경우 대부분의 프로그램과 프로그래머는 이와는 다른 방식을 사용한다.\n다른 방식은 (거의) 항상 UTF-8으로 불리는 인코딩으로, 문자마다 가변 바이트를 사용한다. 하위 호환성을 위해, 첫 128개 문자(즉, 구 아스키 문자 집합)는 바이트 1개에 저장된다. 다음 1920개 문자는 바이트 2개를 사용해서 저장된다. 다음 61,000개는 바이트 3개를 사용해서 저장해 나간다.\n궁금하다면, 동작 방식이 다음 표에 나타나 있다. “전통적” 문자열은 문자마다 1바이트를 사용한다. 반대로, “유니코드” 문자열은 문자마다 충분한 메모리를 사용해서 어떤 텍스트 유형이든 저장한다. R, 파이썬 3.x에서 모든 문자열은 유니코드다. 엄청난 바이트를 읽어오거나 저장하여 내보내려고 할 때, 인코딩을 지정하는 것은 엄청난 고통이다.\n유니코드 문자열은 여는 인용부호 앞에 소문자 U를 붙여 표시한다. 유니코드 문자열을 바이트 문자열로 전환하려면, 인코딩을 명세해야만 된다. 항상 UTF-8을 사용해야 하고, 그 밖의 인코딩을 사용하는 경우 매우, 매우 특별히 좋은 사유가 있어야만 된다. 특별한 인코딩을 사용하는 경우 두 번 생각해 보라.\n\n\n아스키에서 유니코드로 진화과정\n\n컴퓨터가 처음 등장할 때 미국 영어권 중심 아스키가 아니고 4바이트로 전 세계 모든 글자를 표현할 수 있는 유니코드가 사용되었다면 한글을 컴퓨터에 표현하기 위한 지금과 같은 번거로움은 없었을 것이다. 돌이켜보면 초기 컴퓨터가 저장 용량 한계로 인해 유니코드가 표준으로 자리를 잡더라도 실용적인 이유로 인해서 한글을 컴퓨터에 표현하기 위한 다른 대안이 제시됐을 것도 분명해 보인다. 초창기 영어권을 중심으로 아스키 표준이 정립되어 현재까지 내려오고, 유니코드와 UTF-8 인코딩이 사실상 표준으로 자리 잡았으며, 그 사이 유닉스/리눅스 EUC-KR, 윈도우즈 CP949가 빈틈을 한동안 메우면서 역할을 담당했다.\n\n\n\n\n\n\n\n\n\n항목\nASCII (1963)\nEUC-KR (1980s)\nCP949 (1990s)\nUnicode (1991)\n\n\n\n범위\n128개의 문자\n2,350개의 한글 문자 등\n약 11,172개의 완성형 한글 문자 등\n143,859개의 문자 (버전 13.0 기준)\n\n\n비트 수\n7비트\n8~16비트\n8~16비트\n다양한 인코딩 방식 (UTF-8, UTF-16, UTF-32 등)\n\n\n표준\nANSI, ISO/IEC 646\nKS X 2901\n마이크로소프트\nISO/IEC 10646\n\n\n플랫폼\n다양한 시스템\nUNIX 계열, 일부 Windows\nWindows 계열\n다양한 플랫폼\n\n\n문자 집합\n영문 알파벳, 숫자, 특수 문자\n한글, 영문 알파벳, 숫자, 특수 문자\n한글, 한자, 영문 알파벳, 숫자, 특수 문자\n전 세계 언어, 특수 문자, 이모티콘 등\n\n\n확장성\n확장 불가능\n한정적\n더 많은 문자 지원\n높은 확장성\n\n\n국제성\n영어 중심\n한국어 중심\n한국어 중심\n다국어 지원\n\n\n유니코드 호환\n호환 가능 (U+0000 ~ U+007F)\n호환 불가, 변환 필요\n유니코드와 상호 변환 가능\n자체가 표준\n\n\nUTF-8\nUTF-8(Universal Coded Character Set + Transformation Format – 8-bit의 약자)은 유니코드 중에서 가장 널리 쓰이는 인코딩으로, 유니코드를 위한 가변 길이 문자 인코딩 방식 중 하나로 켄 톰프슨과 롭 파이크가 제작했다.\nUTF-8 인코딩의 가장 큰 장점은 아스키(ASCII), 라틴-1(ISO-8859-1)과 호환되어, 문서를 처리하는 경우 아스키, 라틴-1 문서를 변환 없이 그대로 처리할 수 있고 영어를 비롯한 라틴계열 문서로 저장할 때 용량이 매우 작다. 이러한 이유로 많은 오픈소스 소프트웨어와 데이터를 생산하는 미국을 비롯한 유럽 언어권에서 UTF-8이 많이 사용되고 있지만, 한글은 한 글자당 3바이트 용량을 차지한다.\n웹 표준 인코딩\n스마트폰의 대중화에 따라 더이상 윈도우 운영체제에서 사용되는 문자체계가 더이상 표준이 되지 못하고 여러 문제점을 야기함에 따라 유니코드 + UTF-8 체제가 대세로 자리잡고 있는 것이 확연히 나타나고 있다.\n2010년 구글에서 발표한 자료에 의하면 2010년 UTF-8 인코딩이 웹에서 주류로 부상하기 시작한 것이 확인되었다. (unicode2010?) 웹 기반 플롯 디지털 도구를 활용하여 그래프(WebPlotDigitizer)에서 데이터를 추출하여 시각화하면 유사한 결과를 시각적으로 표현할 수 있다. 2010년 이후 웹에서 가장 점유율이 높은 인코딩 방식은 UTF-8으로 W3Tech 웹 기술 조사(Web Technology Surveys)를 통해 확인할 수 있다. 여기서 주목할 점은, 프랑스어, 독일어, 스페인어와 같은 서유럽 언어의 문자와 기호를 표현하는 ISO-8859-1 인코딩, 종종 “Latin-1”으로 불리는 8비트 문자 인코딩이 현저히 줄고 있다는 점이다.\n\n\n2010 ~ 2012 웹에서 UTF-8 성장세",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#아스키-파일",
    "href": "ingest_file.html#아스키-파일",
    "title": "25  파일 데이터",
    "section": "\n25.2 아스키 파일",
    "text": "25.2 아스키 파일\n아스키 파일은 텍스트 파일로, 데이터를 저장하는 가장 기본적인 형태이다. R에서 데이터프레임으로 다양한 데이터를 가져올 때, 아스키 파일은 CSV(Comma-Separated Values) 파일, TSV(Tab-Separated Values) 파일, 고정길이 파일 등 다양한 형식으로 존재한다. CSV 파일은 쉼표로 구분된 값들로 이루어진 텍스트 파일이며, TSV 파일은 탭으로 구분된 값들로 이루어진 텍스트 파일이다. 고정길이 파일은 각 필드가 고정된 길이를 가지는 텍스트 파일이다. 또한, R에서는 데이터를 직접 입력하여 데이터프레임을 생성할 수도 있다. 아스키 파일을 데이터프레임으로 가져올 때는 read.csv(), read.table(), read.fwf() 등의 함수를 사용하며, 데이터를 직접 입력할 때는 열중심 혹은 행중심에 따라 tibble(), tribble() 함수를 사용한다.\n\n\n\n\n\ngraph TB\n\nsubgraph 가져오기[\"&lt;strong&gt;가져오기&lt;/strong&gt;\"]\n\n    스프레드쉬트 --&gt; 핸들러\n    데이터베이스 --&gt; 핸들러\n    아스키 --&gt; 핸들러\n    웹데이터 --&gt; 핸들러\n    핸들러 --&gt; 데이터프레임\n\n    subgraph 아스키[\"&lt;strong&gt;아스키 파일&lt;/strong&gt;\"]\n        데이터입력[데이터 입력]\n        csv[CSV 파일]\n        tsv[TSV 파일]\n        고정길이파일[고정길이 파일]\n    end\n\nend\n\nclassDef modern fill:#f0f0f0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\nclassDef emphasize fill:#d0d0d0,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\nclassDef subgraphStyle fill:#e0e0e0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:20px;\n\nclass csv,데이터입력,tsv,고정길이파일,스프레드쉬트,데이터베이스,웹데이터,핸들러 modern\nclass 데이터프레임 emphasize\nclass 아스키,가져오기 subgraphStyle\n\n\n\n\n그림 25.2: 다양한 데이터 종류",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#데이터-입력방식",
    "href": "ingest_file.html#데이터-입력방식",
    "title": "25  파일 데이터",
    "section": "\n25.3 데이터 입력방식",
    "text": "25.3 데이터 입력방식\n파일 크기가 적은 경우 즉, 눈으로 식별가능한 크기를 갖는 아스키 파일을 .csv, .txt 등의 형식으로 저장한 후 readr 패키지 read_csv(), read_table(), read_delim() 등의 함수로 불러오는 것이 오히려 적절하지 못한 경우가 있다.\ntibble() 혹은 tribble() 함수를 사용해서 인라인 데이터를 생성하는 것이 더 효율적일 수 있다. 다음과 같이 쇼핑몰 초창기 고객 주문 데이터를 입력하여 R로 불러와서 분석하는 방법를 살펴보자.\n주문일자,주문번호,고객번호,상품명,상품범주,주문금액\n\"2023-05-19 13:45:32\",203451,A20193,\"슬림핏 반팔 티셔츠\",의류,21800\n2023/05/19 14:23:11,203452,B10582,\"여성용 스니커즈, 240mm\",,68000\n\"2023.05.19 16:05:49\",203453,\"C30281\",\"진공 보온병, 500ml\",주방용품,\"35,600\"\n2023-05-20 09:18:22, 203454,\"D18734\",\"\"귀걸이\"세트 (실버)\",액세서리,112000\n2023-05-20 11:36:58,,E42097,남성용 슬림 진 (32인치),의류,54900\n2023/05/21 08:02:44,\"203,456\",F61052,\"무선 게이밍 마우스\",전자기기,\"\"88,700\"\"\n2023.05.21 15:30:05,203457,,,,42300\n데이터가 크지 않기 때문에 칼럼 혹은 행 기준으로 데이터프레임으로 불러올 수 있다. 먼저 tibble() 함수를 사용해서 데이터프레임을 생성한다. 실무에서 결측값도 있고 주문금액에 천단위 , 구분자도 포함되어 있고 날짜 형식도 다양하게 표현되어 있을 수 있다.\n\nlibrary(tibble)\n\norders &lt;- tribble(\n  ~주문일자, ~주문번호, ~고객번호, ~상품명, ~상품범주, ~주문금액,\n  \"2023-05-19 13:45:32\", \"203451\", \"A20193\", \"슬림핏 반팔 티셔츠\", \"의류\", \"21800\",\n  \"2023/05/19 14:23:11\", \"203452\", \"B10582\", \"여성용 스니커즈, 240mm\", NA, \"68000\",\n  \"2023.05.19 16:05:49\", \"203453\", \"C30281\", \"진공 보온병, 500ml\", \"주방용품\", \"35,600\",\n  \"2023-05-20 09:18:22\", \"203454\", \"D18734\", \"귀걸이세트 (실버)\", \"액세서리\", \"112000\",\n  \"2023-05-20 11:36:58\", NA, \"E42097\", \"남성용 슬림 진 (32인치)\", \"의류\", \"54900\",\n  \"2023/05/21 08:02:44\", \"203,456\", \"F61052\", \"무선 게이밍 마우스\", \"전자기기\", \"88,700\",\n  \"2023.05.21 15:30:05\", \"203457\", NA, NA, NA, \"42300\"\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300\n\ntibble() 함수는 벡터를 기준으로 칼럼을 생성하고 이를 tibble() 함수로 결합하여 데이터프레임을 생성한다.\n\nlibrary(tibble)\n\norders &lt;- tibble(\n 주문일자 = c(\"2023-05-19 13:45:32\", \"2023/05/19 14:23:11\", \"2023.05.19 16:05:49\", \n             \"2023-05-20 09:18:22\", \"2023-05-20 11:36:58\", \"2023/05/21 08:02:44\", \n             \"2023.05.21 15:30:05\"),\n 주문번호 = c(\"203451\", \"203452\", \"203453\", \"203454\", NA, \"203,456\", \"203457\"),\n 고객번호 = c(\"A20193\", \"B10582\", \"C30281\", \"D18734\", \"E42097\", \"F61052\", NA),\n 상품명 = c(\"슬림핏 반팔 티셔츠\", \"여성용 스니커즈, 240mm\", \"진공 보온병, 500ml\", \n           \"귀걸이세트 (실버)\", \"남성용 슬림 진 (32인치)\", \"무선 게이밍 마우스\", NA),\n 상품범주 = c(\"의류\", NA, \"주방용품\", \"액세서리\", \"의류\", \"전자기기\", NA),\n 주문금액 = c(\"21800\", \"68000\", \"35,600\", \"112000\", \"54900\", \"88,700\", \"42300\")\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300\n\ntribble(), tibble() 함수 모두 데이터 입력을 통해 orders 데이터프레임을 생성하였으나 결측값에 대한 처리와 자료형이 모두 문자형(&lt;chr&gt;)으로 되어 있어 후속작업을 위해 추가 데이터 정제 작업이 필수적이다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#자료형",
    "href": "ingest_file.html#자료형",
    "title": "25  파일 데이터",
    "section": "\n25.4 자료형",
    "text": "25.4 자료형\n가장 많이 사용되는 콤마 구분자 아스키 파일(.csv) 파일로 불러오면서 함께 고민해야 하는 사항이 바로 자료형이다.\nR에서는 다양한 자료형을 지원한다. 가장 기본적인 자료형은 숫자형, 문자형, 범주형, 논리형이다. 숫자형은 정수형과 실수형으로 구분되며, 문자형은 문자열을 저장하는 자료형이다. 논리형은 참과 거짓을 나타내는 자료형이고, 범주형은 내부적으로 정수로 저장되지만 한정된 범주를 갖는 문자형으로 표현된다. 그외에도 날짜와 시간을 저장하는 자료형, 지도정보를 담고 있는 자료형, 이미지 정보를 담고 있는 자료형 등 다양한 자료형이 있다.\nreadr 패키지 spec() 함수를 사용하면 아스키 파일을 불러읽어 오면서 각 열의 자료형을 확인할 수 있다. spec() 함수에서 출력한 각 열 자료형이 정답은 아니지만 나름 최선의 추정으로 각 열의 자료형을 살펴본 후 최종 열별 자료형을 지정하는데 도움이 되는 것은 사실이다.\n\nspec( read_csv(\"data/file/nine_penguins.csv\") )\n#&gt; cols(\n#&gt;   species = col_character(),\n#&gt;   island = col_character(),\n#&gt;   bill_length_mm = col_double(),\n#&gt;   flipper_length_mm = col_double(),\n#&gt;   body_mass_g = col_double(),\n#&gt;   sex = col_character(),\n#&gt;   year = col_double()\n#&gt; )\n\nreadr 패키지 col_types 인자를 사용하여 각 열의 자료형을 지정할 수 있다. col_types 인자에는 cols() 함수를 사용하여 각 열의 자료형을 지정한다. cols() 함수에는 col_factor(), col_character(), col_double(), col_integer(), col_logical() 함수를 사용하여 각 열의 자료형을 지정한다. col_factor() 함수는 범주형 자료형을 지정할 때 사용하며, col_character() 함수는 문자형 자료형을 지정할 때 사용한다. col_double() 함수는 실수형 자료형을 지정할 때 사용하며, col_integer() 함수는 정수형 자료형을 지정할 때 사용한다. col_logical() 함수는 논리형 자료형을 지정할 때 사용한다.\nspec() 함수가 텍스트로 된 열은 모두 문자형(col_character())으로 인식하였지만, species, sex 열은 범주형 자료형으로 지정하는 것이 더 적절하다. bill_length_mm, flipper_length_mm, body_mass_g, year 열은 실수형, 정수형 자료형으로 지정하는 것이 적절하다고 판단되어 다음과 같이 .csv 파일을 불러오면서 각 열의 자료형도 함께 지정한다.\n\npenguins_tbl &lt;- read_csv(\"data/file/nine_penguins.csv\",\n         col_types = cols(\n            species = col_factor(level = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")),\n            island = col_character(),\n            bill_length_mm = col_double(),\n            flipper_length_mm = col_double(),\n            body_mass_g = col_double(),\n            sex = col_factor(levels = c(\"female\", \"male\")),\n            year = col_integer()\n          )\n)\n\npenguins_tbl\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island    bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;fct&gt;     &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 Adelie    Dream               41.1               205        4300 male    2008\n#&gt; 2 Adelie    Torgersen           40.2               176        3450 female  2009\n#&gt; 3 Adelie    Biscoe              34.5               187        2900 female  2008\n#&gt; 4 Gentoo    Biscoe              43.2               208        4450 female  2008\n#&gt; 5 Gentoo    Biscoe              46.2               221        5300 male    2008\n#&gt; 6 Gentoo    Biscoe              42.6               213        4950 female  2008\n#&gt; 7 Chinstrap Dream               49.7               195        3600 male    2008\n#&gt; 8 Chinstrap Dream               49                 210        3950 male    2008\n#&gt; 9 Chinstrap Dream               51.3               193        3650 male    2007",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#다양한-파일형태",
    "href": "ingest_file.html#다양한-파일형태",
    "title": "25  파일 데이터",
    "section": "\n25.5 다양한 파일형태",
    "text": "25.5 다양한 파일형태\n\n25.5.1 파일 저장\n펭귄 데이터에서 종별로 3마리를 무작위로 추출해서 nine_penguins 데이터프레임을 만든 후에 다양한 형식 아스키 파일로 저장한다. 펭귄 9마리 데이터프레임으로 아스크 파일 형식으로 저장된 다양한 형태(탭 구분자, 콤마 구분자, 고정길이) 데이터를 불러오는 방법을 살펴본다. 구분자로 탭과 콤마가 가장 많이 사용되지만 경우에 따라서는 “;”, “:”, “|” 등 다양한 구분자를 사용할 수 있다.\n\nlibrary(palmerpenguins)\n\nnine_penguins &lt;- palmerpenguins::penguins |&gt; \n    drop_na() |&gt; \n    slice_sample(n = 3, replace = FALSE, by = species) |&gt; \n    select(-bill_depth_mm)\n\n탭 구분자\nwrite_delim() 함수에 delim 인자를 탭으로 명시하여 탭 구분자 아스키 파일로 저장하는 방법과 write_tsv() 함수를 사용하는 방법이 있다. 탭 구분자 파일로 저장하는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins |&gt; \n    # write_tsv(\"data/file/ASCII/nine_penguins.tsv\") |&gt; \n    write_delim(\"data/file/nine_penguins.txt\", delim = \"\\t\") \n\nspecies\tisland\tbill_length_mm\tflipper_length_mm\tbody_mass_g\tsex\tyear\nAdelie\tDream\t39.5\t178\t3250\tfemale\t2007\nAdelie\tTorgersen\t40.9\t191\t3700\tfemale\t2008\nAdelie\tTorgersen\t41.5\t195\t4300\tmale\t2009\nGentoo\tBiscoe\t45.5\t214\t4650\tfemale\t2007\nGentoo\tBiscoe\t43.3\t208\t4575\tfemale\t2009\nGentoo\tBiscoe\t50.4\t222\t5750\tmale\t2009\nChinstrap\tDream\t46.2\t187\t3650\tfemale\t2008\nChinstrap\tDream\t51.4\t201\t3950\tmale\t2009\nChinstrap\tDream\t58\t181\t3700\tfemale\t2007\nCSV 구분자\nCSV(Comma-Separated Values) 파일은 콤마 구분자를 사용하여 데이터를 저장하는 형식으로 모든 운영체제에서 특별한 별도 프로그램없이 열어볼 수 있다는 장점이 있어 호환성에서 큰 장점이 있지만 파일에 많은 정보가 담기게 되면 파일크기가 커져서 저장공간을 많이 차지한다는 단점이 있다. write_csv() 함수를 사용하여 콤마 구분자 아스키 파일로 저장하는 방법과 write_delim() 함수를 사용하는 방법이 있다. 콤마 구분자 파일로 저장하는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins |&gt; \n    write_csv(\"data/file/nine_penguins.csv\")\n\nspecies,island,bill_length_mm,flipper_length_mm,body_mass_g,sex,year\nAdelie,Dream,39.5,178,3250,female,2007\nAdelie,Torgersen,40.9,191,3700,female,2008\nAdelie,Torgersen,41.5,195,4300,male,2009\nGentoo,Biscoe,45.5,214,4650,female,2007\nGentoo,Biscoe,43.3,208,4575,female,2009\nGentoo,Biscoe,50.4,222,5750,male,2009\nChinstrap,Dream,46.2,187,3650,female,2008\nChinstrap,Dream,51.4,201,3950,male,2009\nChinstrap,Dream,58,181,3700,female,2007\n고정길이 파일\n고정길이 아스키 파일(Fixed-width ASCII file, FWF)은 데이터 저장 및 교환을 위해 초기 컴퓨팅 시대에 개발되었다. 당시에는 데이터 저장 공간이 제한적이었기 때문에 고정길이 파일은 구분자를 사용하지 않고 데이터를 더 촘촘하게 저장할 수 있었고, 하드웨어와 소프트웨어도 고정 길이 레코드 처리에 최적화되어 있었다.\n현재까지도 고정길이 파일은 레거시 시스템과의 호환성, 데이터 무결성 유지, 데이터 밀도 향상, 대용량 데이터 처리 성능 개선 등의 이유로 명맥을 유지하고 있으며, 의료 및 금융 분야에서 고정길이 파일을 데이터 교환 표준으로 활용하기도 한다.\n하지만, 고정길이 파일은 파일 구조를 이해하기 위해 별도 문서나 스키마 정의가 필요하고, 데이터 추가나 수정 시 레코드 길이 조정이 요구되는 단점이 크고, 구분자로 구분되는 구조화된 데이터 형식과 비교하면 사용 편의성이 크게 떨어진다.\nAdelie    Dream     37.6          181            3300     female   2007\nAdelie    Biscoe    35.3          187            3800     female   2007\nAdelie    Biscoe    37.8          174            3400     female   2007\nGentoo    Biscoe    47.4          212            4725     female   2009\nGentoo    Biscoe    49.1          220            5150     female   2008\nGentoo    Biscoe    47.5          209            4600     female   2008\nChinstrap Dream     40.9          187            3200     female   2008\nChinstrap Dream     47.6          195            3850     female   2008\nChinstrap Dream     46            195            4150     female   2007\n\n25.5.2 불러오기\n탭 구분자\nread_delim() 함수에 delim 인자를 탭으로 명시하여 탭 구분자 아스키 파일을 불러오는 방법과 read_tsv()` 함수를 사용하는 방법이 있다. 탭 구분자 파일을 불러오는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins &lt;- \n    # read_tsv(\"data/file/ASCII/nine_penguins.tsv\") |&gt; \n    read_delim(\"data/file/nine_penguins.txt\", delim = \"\\t\") \n\nCSV 구분자\n고정길이 파일\nreadr 패키지 read_fwf() 함수를 사용하여 고정길이 파일을 불러읽어오는 방식에서 fwf_widths 인자로 각 열의 길이를 지정하고 col_names 인자로 열 이름을 지정한다.\n\nnine_penguins_fwf &lt;-read_fwf(\"data/file/nine_penguins.fwf\",\n                             skip = 0,\n         col_positions = fwf_widths(c(10, 10, 14, 15, 9, 9, 5),\n           col_names = c(\"species\", \"island\", \"bill_length_mm\",\n                         \"flipper_length_mm\", \"body_mass_g\", \"sex\", \"year\")))\n\nnine_penguins_fwf\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Adelie    Dream            37.6               181        3300 female  2007\n#&gt; 2 Adelie    Biscoe           35.3               187        3800 female  2007\n#&gt; 3 Adelie    Biscoe           37.8               174        3400 female  2007\n#&gt; 4 Gentoo    Biscoe           47.4               212        4725 female  2009\n#&gt; 5 Gentoo    Biscoe           49.1               220        5150 female  2008\n#&gt; 6 Gentoo    Biscoe           47.5               209        4600 female  2008\n#&gt; 7 Chinstrap Dream            40.9               187        3200 female  2008\n#&gt; 8 Chinstrap Dream            47.6               195        3850 female  2008\n#&gt; 9 Chinstrap Dream            46                 195        4150 female  2007",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#공공-데이터",
    "href": "ingest_file.html#공공-데이터",
    "title": "25  파일 데이터",
    "section": "\n25.6 공공 데이터",
    "text": "25.6 공공 데이터\n공공데이터포털을 비롯한 많은 정부기관에서 제공하는 데이터는 대부분 EUC-KR로 인코딩되어 있다. 이유는 여러가지가 있겠지만 가장 큰 이유는 아마도 엑셀에서 .csv 파일을 열었을 때 한글이 깨지는 민원을 처리하기 위함이 아닐까 싶다. 정형 .csv 파일 형태로 데이터를 받게 되면 먼저 인코딩을 확인해야 한다. readr 패키지의 guess_encoding() 함수를 사용하면 파일의 인코딩을 확인할 수 있다.\n공공데이터포털 인천광역시_정류장별 이용승객 현황 데이터를 다운로드 받아 로컬 파일로 저장한 후 인코딩을 확인한다.\n\nlibrary(readr)\n\nfile_path &lt;- \"data/file/인천광역시_정류장별 이용승객 현황_20220630.csv\"\nguess_encoding(file_path)\n#&gt; # A tibble: 4 × 2\n#&gt;   encoding confidence\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 EUC-KR         1   \n#&gt; 2 GB18030        0.81\n#&gt; 3 Big5           0.48\n#&gt; 4 EUC-JP         0.3\n\n따라서, 이를 바로 read_csv() 함수로 읽을 경우 오류가 발생된다. 왜냐하면 read_csv() 함수는 인코딩을 UTF-8을 기본으로 가정하고 있기 때문이다.\n\nread_csv(file_path)\n#&gt; Error in nchar(x, \"width\"): invalid multibyte string, element 1\n\n따라서, read_csv() 함수를 사용할 때는 locale 인수를 사용하여 인코딩을 지정해주어야 한다. “EUC-KR”로 인코딩을 지정하면 파일을 오류없이 읽을 수 있다.\n\nincheon_bus &lt;- spec(read_csv(file_path, locale = locale(encoding = \"EUC-KR\")))\nincheon_bus |&gt; names() |&gt; dput()\n#&gt; c(\"cols\", \"default\", \"delim\")\n\n데이터 가져오기는 데이터 분석의 첫 단계로, 외부 데이터를 R로 불러오는 과정으로 첫단추가 이후 이어질 분석단계에서 중요한 역할을 한다.\n먼저, 파일 형식에 따라 적절한 함수를 선택해야 한다. 텍스트 파일은 read.csv, read.table 등의 함수를 사용하고, 엑셀 파일은 readxl 패키지의 read_excel 함수를 사용한다. 특히, 인코딩도 이 단계에서 반듯이 확인해야 한다.\n데이터 전처리 단계에서는 구분자와 헤더 유무를 확인하고, 자료형과 칼럼명을 결정해야 한다. 결측값 처리를 위해 na = 옵션을 사용할 수 있고, 필요에 따라 특정 행/열을 선택하는 등의 추가 옵션을 설정할 수 있다.\n전처리 과정을 거쳐 최종적으로 데이터프레임을 생성하게 된다. 다소 번거럽더라도 데이터를 가져오는 단계에서 전처리 과정을 충실히 수행하게 되면 이후 dplyr, tidyr 패키지 등을 활용해 다양한 데이터 조작 및 시각화를 수월하게 할 수 있다.\n\nfile_path &lt;- \"data/file/인천광역시_정류장별 이용승객 현황_20220630.csv\"\n\nincheon_bus &lt;- read_csv(file_path, locale = locale(encoding = \"EUC-KR\"),\n                        skip = 1,\n                        na = c(\"---\", \"\"),\n                        col_names = c(\"정류소명\", \"정류소_id\", \"승차건수_총합계\", \n                                     \"하차건수_총합계\",\"승차건수_카드\", \"하차건수_카드\",\n                                     \"승차건수_현금\", \"일평균_승하차건수\"),\n                       col_types = cols(\n                         정류소명 = col_character(),\n                         정류소_id = col_double(),\n                         승차건수_총합계 = col_double(),\n                         하차건수_총합계 = col_double(),\n                         승차건수_카드 = col_double(),\n                         하차건수_카드 = col_double(),\n                         승차건수_현금 = col_double(),\n                         일평균_승하차건수 = col_double()\n                       ))  \nincheon_bus\n#&gt; # A tibble: 6,386 × 8\n#&gt;    정류소명             정류소_id 승차건수_총합계 하차건수_총합계 승차건수_카드\n#&gt;    &lt;chr&gt;                    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1 (구)국제여객터미널       35051              95            1923            21\n#&gt;  2 (구)국제여객터미널          NA            2512              43          2465\n#&gt;  3 (주)경동세라믹스         89146             341              26           335\n#&gt;  4 (주)경인양행앞           42096             945             923           938\n#&gt;  5 (주)경인양행앞           42097            1322            3536          1294\n#&gt;  6 (주)대한특수금속         39050            1243              89          1238\n#&gt;  7 (주)두남                 39135             147              29           147\n#&gt;  8 (주)세모입구(린나이…     37585            1410            1517          1404\n#&gt;  9 (주)세모입구(린나이…     40893             307             556           304\n#&gt; 10 (주)스킨이데아           89388             147             148           147\n#&gt; # ℹ 6,376 more rows\n#&gt; # ℹ 3 more variables: 하차건수_카드 &lt;dbl&gt;, 승차건수_현금 &lt;dbl&gt;,\n#&gt; #   일평균_승하차건수 &lt;dbl&gt;\n\n지금까지 작업한 전반적인 작업흐름은 그림 25.3 에 대략적으로 나와있다. 공공데이터포털에서 다운로드 받은 인천광역시_정류장별 이용승객 현황_20220630.csv은 EUC-KR로 인코딩 되어 있고 헤더를 갖고 있으며 쉼표로 구분되어 있다. 결측치는 없으나 임의로 --- 으로 정류장 한 곳을 달리 표현하여 na = c(\"---\", \"\")로 결측값 처리를 하였다.\n\n\n\n\n\ngraph LR\n    subgraph \"&lt;strong&gt;파일 형식 결정&lt;/strong&gt;\"\n    A[파일 형식 결정] --&gt; |\"read.csv, read.table 등\"| B[인코딩 확인]\n    A --&gt; |\"readxl::read_excel 등\"| B\n    end\n\n    subgraph \"&lt;strong&gt;데이터 전처리&lt;/strong&gt;\"\n    C[구분자 확인] --&gt; |\"쉼표, 탭 등\"| B\n    D[헤더 유무] --&gt; |\"header = TRUE/FALSE\"| B\n    B --&gt; E[자료형 및&lt;br&gt; 칼럼명 결정]\n    F[결측값 처리] --&gt; |\"na 옵션\"| E\n    G[추가 옵션] --&gt; |\"특정 행/열 선택 등\"| E\n    end\n\nE --&gt; H[데이터프레임&lt;br&gt;생성]\n\nstyle A fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle B fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle C fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle D fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle E fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle F fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle G fill:#c0c0c0,stroke:#333,stroke-width:2px\nstyle H fill:#c0c0c0,stroke:#333,stroke-width:2px\n\n\n\n\n그림 25.3: 인천광역시 정류장별 이용승객 현황 데이터 데이터프레임 가져오는 과정",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#다수-파일",
    "href": "ingest_file.html#다수-파일",
    "title": "25  파일 데이터",
    "section": "\n25.7 다수 파일",
    "text": "25.7 다수 파일\n다수 파일을 불러오는 경우를 상정하기 위해서 먼저 앞서 준비한 nine_penguins 데이터프레임을 재사용한다. split() 함수로 species 열을 기준으로 nine_penguins를 분할하여 분할된 데이터를 리스트 형태로 penguins_split에 저장한다. here() 함수로 data/file/ 폴더의 경로를 data_folder 변수에 저장한다. walk2() 함수로 penguins_split 리스트의 각 요소와 해당 요소의 이름을 순회하면서 write_csv() 함수를 사용해 각 분할된 데이터프레임을 CSV 파일로 저장한다. 코드 실행 결과, 프로젝트 디렉토리 내의 data/file/ 폴더에 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일이 생성되며, 각 파일에는 해당 펭귄 종 3마리 관측점 데이터가 저장된다.\n\n# species로 데이터 분할\npenguins_split &lt;- split(nine_penguins, nine_penguins$species)\n\n# 분할된 데이터를 CSV 파일로 저장\ndata_folder &lt;- here::here(\"data\", \"file\")\nwalk2(penguins_split, str_glue(\"penguin_{names(penguins_split)}\"), ~ write_csv(.x, here::here(data_folder, str_c(.y, \".csv\"))))\n\nlist.files() 함수로 data/file 폴더에 penguin으로 시작하는 .csv 파일 3개를 확인할 수 있다.\n\nlist.files(data_folder, pattern = \"^penguin\")\n#&gt; [1] \"penguin_Adelie.csv\"    \"penguin_Chinstrap.csv\" \"penguin_Gentoo.csv\"\n\n이제 데이터가 준비되었으니 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일을 읽어와서 하나의 데이터프레임으로 만들어보자. 동일한 자료구조를 갖는 아스키 파일은 시도, 시군구 데이터처럼 공간적으로 관리를 위해 구분되거나 일, 월, 분기, 년 처럼 시점을 달리하는 경우 관리 목적으로 구분되어 흔히 접하게 되는 데이터다.\ndata_folder 변수에 CSV 파일들이 저장된 폴더 경로를 지정한다. list.files() 함수로 data_folder 내의 모든 CSV 파일 경로를 csv_files 변수에 저장한다. map_df() 함수로 csv_files의 각 파일 경로에 대해 read_csv() 함수를 적용하여 CSV 파일을 읽어와 읽어온 데이터프레임으로 결합한다. 코드 실행 결과, data/file/ 폴더에 있는 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일을 읽어와서 하나의 데이터프레임으로 결합한 penguins_tbl이 생성되며, 총 9 마리 펭귄 3종의 데이터가 포함되어 있다.\n\n# CSV 파일 경로 지정\ndata_folder &lt;- \"data/file/\"\ncsv_files &lt;- list.files(data_folder, pattern = \"^penguin\", full.names = TRUE)\n\n# CSV 파일들을 읽어와 데이터프레임 결합\npenguins_tbl &lt;- purrr::map_df(csv_files, read_csv)\n\npenguins_tbl\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island    bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Adelie    Dream               39.5               178        3250 female  2007\n#&gt; 2 Adelie    Torgersen           40.9               191        3700 female  2008\n#&gt; 3 Adelie    Torgersen           41.5               195        4300 male    2009\n#&gt; 4 Chinstrap Dream               46.2               187        3650 female  2008\n#&gt; 5 Chinstrap Dream               51.4               201        3950 male    2009\n#&gt; 6 Chinstrap Dream               58                 181        3700 female  2007\n#&gt; 7 Gentoo    Biscoe              45.5               214        4650 female  2007\n#&gt; 8 Gentoo    Biscoe              43.3               208        4575 female  2009\n#&gt; 9 Gentoo    Biscoe              50.4               222        5750 male    2009\n\n\n\n\n유니코드와 UTF-8\n세 가지 인코딩 문제\n그림 25.1: 제어문자와 출력 가능한 아스키 문자표 알파벳 예시\n아스키에서 유니코드로 진화과정\n2010 ~ 2012 웹에서 UTF-8 성장세",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#footnotes",
    "href": "ingest_file.html#footnotes",
    "title": "25  파일 데이터",
    "section": "",
    "text": "미국정보교환표준부호(American Standard Code for Information Interchange, ASCII)는 영문 알파벳을 사용하는 대표적인 문자 인코딩으로 컴퓨터와 통신 장비를 비롯한 문자를 사용하는 많은 장치에서 사용되며, 대부분의 문자 인코딩이 아스키에 기초하고 있다.↩︎",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_web.html",
    "href": "ingest_web.html",
    "title": "26  웹 데이터",
    "section": "",
    "text": "26.1 엑셀\n제21대 국회의원 선거 당선인 명부 데이터를 엑셀 형태로 구하려면, 선관위 자료공간 웹사이트 게시판을 통해 엑셀 파일을 다운로드할 수 있다.\n다운로드 받은 엑셀 데이터를 가져오는 코드를 목록 26.1 와 같이 작성할 수 있다. 먼저, library(readxl)과 library(tidyverse)를 통해 필요한 패키지를 로드한다. nec_sheets &lt;- readxl::excel_sheets(...)는 엑셀 파일의 시트 이름을 가져오는 코드로 엑셀 파일에서 가져올할 시트를 선택한다. winner_raw &lt;- readxl::read_excel(...)는 선택한 시트의 데이터를 읽어오는 코드로 당선인 명부 데이터를 R 환경으로 불러온다.\n이후, winner_raw |&gt; count(소속정당명, name = \"당선인수\", sort = TRUE)는 파이프 연산자(|&gt;)를 사용하여 데이터를 조작하여 정당별로 집계하고, 당선인 수를 계산한 후, 결과를 내림차순으로 정렬하여 당선인 수를 쉽게 파악할 수 있다. janitor::adorn_totals(where = \"row\", name = \"합계\")는 janitor 패키지 함수로, 집계 결과에 총합 행을 추가하여 전체 당선인 수를 확인할 수 있다.\nlibrary(readxl)\nlibrary(tidyverse)\n\nnec_sheets &lt;- readxl::excel_sheets(\"data/nec/제21대_국회의원선거(재보궐선거_포함)_당선인명부.xlsx\")\n\nwinner_raw &lt;- readxl::read_excel(\"data/nec/제21대_국회의원선거(재보궐선거_포함)_당선인명부.xlsx\", sheet = nec_sheets[1])\n\nwinner_raw |&gt; \n  count(소속정당명, name = \"당선인수\", sort = TRUE) |&gt; \n    janitor::adorn_totals(where = \"row\", name = \"합계\")  \n#&gt;    소속정당명 당선인수\n#&gt;  더불어민주당      163\n#&gt;    미래통합당       84\n#&gt;        무소속        5\n#&gt;        정의당        1\n#&gt;          합계      253\n\n\n목록 26.1: 제21대 국회의원 선거 당선인 명부 데이터 불러오기",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>웹 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_web.html#엑셀",
    "href": "ingest_web.html#엑셀",
    "title": "26  웹 데이터",
    "section": "",
    "text": "그림 26.2: 선관위 제21대 국회의원 당선인 명부 엑셀파일",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>웹 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_web.html#api",
    "href": "ingest_web.html#api",
    "title": "26  웹 데이터",
    "section": "\n26.2 API",
    "text": "26.2 API\n공공데이터포털 중앙선거관리위원회 당선인정보 API를 활용하여 당선인 정보를 프로그래밍을 통해 직접 가져올 수 있다. 아래아한글로 작성된 당선인 정보 조회 서비스 API 명세서를 참고하여 Java, Javascript, C#, PHP, Curl, Objective-C, Python, Nodejs, R 언어로 예제 코드가 작성되어 있어 사용자가 편리하게 API를 활용할 수 있다.\n\n\n\n\n\n그림 26.3: 국가선거정보 - 당선인 정보 조회 서비스 API 명세서\n\n\n공공데이터포털 API 끝점(Endpoint)와 API KEY를 발급받고 API 서비스 신청을 하였다면 다음 단계로 아래아한글 당선인 정보 조회 서비스 API 명세서 내용을 참고하여 코드를 작성한다.\n\n\n\n\n\ngraph LR\n\n  subgraph API 파악\n    A[공공데이터포털&lt;br&gt;API 발급] --&gt; B[API 명세서&lt;br&gt;참고]\n  end\n  \n  subgraph 스크립트 작성\n    B --&gt; C[당선인 정보 API 호출&lt;br&gt;스크립트 작성]\n    B --&gt; D[선거구 정보 API 호출&lt;br&gt;스크립트 작성]\n  end\n  \n  subgraph 함수 작성\n    C --&gt; E[당선인 정보 API 호출&lt;br&gt;함수 제작]\n    D --&gt; F[선거구 정보 API 호출&lt;br&gt;함수 제작]\n  end\n  \n  subgraph 데이터 가져오기 및 저장\n    F --&gt; G[선거구 데이터프레임&lt;br&gt;제작]\n    G --&gt; H[선거구 당선인&lt;br&gt;데이터프레임 제작]\n    H --&gt; I[데이터 저장]\n  end\n  \nE --&gt; H\n\nlinkStyle 8 stroke:red,stroke-width:4px\n\nstyle A fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle B fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle C fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle D fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle E fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle F fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle G fill:#c0c0c0,stroke:#333,stroke-width:2px\nstyle H fill:#c0c0c0,stroke:#333,stroke-width:2px\nstyle I fill:#c0c0c0,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n공공데이터포털에서 제공하는 API를 활용하여 당선인 정보와 선거구 정보를 수집하고 분석에 용이한 형태로 가공하는 과정은 다음과 같다. API 사용을 위해 활용신청과 API KEY 발급을 진행한 후 명세서를 참고하여 API 호출 방법과 반환되는 데이터 형식을 파악한다.\n당선인 정보와 선거구 정보를 가져오기 위한 API 호출 스크립트를 작성하고, 반복을 줄이고 재사용성을 높이기 위해 함수로 변환하는 작업을 수행한다. 선거구 데이터프레임에서 기본정보 즉, 시도명과 선거구명을 작성한 함수 get_winner() 에 전달하여 선거구별 당선인 정보를 데이터프레임으로 가져온다.\n\n26.2.1 스크립트\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\n\nresponse &lt;- GET(\"http://apis.data.go.kr/9760000/WinnerInfoInqireService2/getWinnerInfoInqire\",\n                query = list(sgId = \"20200415\",\n                              sgTypecode = \"2\",\n                              sdName = \"서울특별시\",\n                              sggName = \"종로구\",\n                              pageNo = 1,\n                              numOfRows = 10,\n                              resultType = \"json\",\n                              serviceKey = Sys.getenv('DATA_GO_DECODE_KEY')))\n\nprint(status_code(response))\n#&gt; [1] 200\n\nresponse_list &lt;- content(response, \"text\") |&gt; \n  fromJSON()\n\nresponse_tbl &lt;- response_list$response$body$items$item\n\nresponse_tbl |&gt; \n    select(sgId, sggName, sdName, giho, jdName, name)\n\n#&gt;       sgId sggName     sdName giho       jdName   name\n#&gt; 1 20200415  종로구 서울특별시    1 더불어민주당 이낙연\n\n\n26.2.2 함수\n\nget_winner &lt;- function(sdName = \"서울특별시\", sggName = \"종로구\") {\n    response &lt;- GET(\"http://apis.data.go.kr/9760000/WinnerInfoInqireService2/getWinnerInfoInqire\",\n                query = list(sgId = \"20200415\",\n                              sgTypecode = \"2\",\n                              sdName = sdName,\n                              sggName = sggName,\n                              pageNo = 1,\n                              numOfRows = 1000,\n                              resultType = \"json\",\n                              serviceKey = Sys.getenv('DATA_GO_DECODE_KEY')))\n\n    response_list &lt;- content(response, \"text\") |&gt; \n      fromJSON()\n    \n    response_tbl &lt;- response_list$response$body$items$item |&gt; \n        select(sgId, sggName, sdName, giho, jdName, name)\n    \n    return(response_tbl)\n}\n\nget_winner(\"서울특별시\", \"종로구\")\n\n#&gt;       sgId sggName     sdName giho       jdName   name\n#&gt; 1 20200415  종로구 서울특별시    1 더불어민주당 이낙연\n\n\n26.2.3 선거구\n중앙선거관리위원회 코드정보 API를 활용하여 선거구 정보를 프로그래밍을 통해 가져올 수 있다. 당선인 명부 데이터를 불러올 때 선거구 정보가 필수적이라 이 과정을 생략할 수는 없다. 당선인 정보와 동일하기 때문에 스크립트 제작과정은 생략하고 명세서에 나와 있는 내용을 바탕으로 R 코드를 작성해서 선거구 데이터프레임을 제작한다.\n\nget_precinct &lt;- function(pageNo = 1) {\n  response &lt;- GET(\"http://apis.data.go.kr/9760000/CommonCodeService/getCommonSggCodeList\",\n              query = list(sgId = \"20200415\",\n                            sgTypecode = \"2\",\n                            pageNo =  pageNo,\n                            numOfRows = 1000,\n                            resultType = \"json\",\n                            serviceKey = Sys.getenv('DATA_GO_DECODE_KEY')))\n  \n  response_list &lt;- content(response, \"text\") |&gt; \n    fromJSON()\n  \n  response_tbl &lt;- response_list$response$body$items$item\n  \n  return(response_tbl)\n}\n\nprecinct_raw &lt;- tibble(page = 1:3) |&gt; \n  mutate(data = map(page, get_precinct)) \n\nprecinct_tbl &lt;- precinct_raw |&gt; \n  unnest(data)\n\nprecinct_tbl  |&gt; \n  select(sgId, sdName, sggName, sggJungsu)\n\n#&gt; # A tibble: 30 × 4\n#&gt;    sgId     sdName     sggName      sggJungsu\n#&gt;    &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;    \n#&gt;  1 20200415 서울특별시 종로구       1        \n#&gt;  2 20200415 서울특별시 중구성동구갑 1        \n#&gt;  3 20200415 서울특별시 중구성동구을 1        \n#&gt;  4 20200415 서울특별시 용산구       1        \n#&gt;  5 20200415 서울특별시 광진구갑     1        \n#&gt;  6 20200415 서울특별시 광진구을     1        \n#&gt;  7 20200415 서울특별시 동대문구갑   1        \n#&gt;  8 20200415 서울특별시 동대문구을   1        \n#&gt;  9 20200415 서울특별시 중랑구갑     1        \n#&gt; 10 20200415 서울특별시 중랑구을     1        \n#&gt; # ℹ 20 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\n26.2.4 선거구 당선인\n\nwinners_raw &lt;- precinct_tbl |&gt; \n  mutate(winner = map2(sdName, sggName, get_winner))\n\nwinners_tbl &lt;-winners_raw |&gt; \n  select(winner) |&gt; \n  unnest(winner)\n\nwinners_tbl\n\n#&gt; # A tibble: 253 × 6\n#&gt;    sgId     sggName      sdName     giho  jdName       name  \n#&gt;    &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; \n#&gt;  1 20200415 종로구       서울특별시 1     더불어민주당 이낙연\n#&gt;  2 20200415 중구성동구갑 서울특별시 1     더불어민주당 홍익표\n#&gt;  3 20200415 중구성동구을 서울특별시 1     더불어민주당 박성준\n#&gt;  4 20200415 용산구       서울특별시 2     미래통합당   권영세\n#&gt;  5 20200415 광진구갑     서울특별시 1     더불어민주당 전혜숙\n#&gt;  6 20200415 광진구을     서울특별시 1     더불어민주당 고민정\n#&gt;  7 20200415 동대문구갑   서울특별시 1     더불어민주당 안규백\n#&gt;  8 20200415 동대문구을   서울특별시 1     더불어민주당 장경태\n#&gt;  9 20200415 중랑구갑     서울특별시 1     더불어민주당 서영교\n#&gt; 10 20200415 중랑구을     서울특별시 1     더불어민주당 박홍근\n#&gt; # ℹ 243 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\n26.2.5 데이터 저장\n\nwinners_tbl |&gt; \n  write_csv(\"data/21st_election_winners.csv\")",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>웹 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_web.html#동적-웹-페이지",
    "href": "ingest_web.html#동적-웹-페이지",
    "title": "26  웹 데이터",
    "section": "\n26.3 동적 웹 페이지",
    "text": "26.3 동적 웹 페이지\n중앙선거관리위원회 선거통계시스템에서 제공하는 제21대 국회의원선거 당선인 명단을 R과 RSelenium을 활용한 동적 웹 크롤링을 통해 수집하고 정제하는 과정을 살펴본다. RSelenium 패키지를 설치하고 불여우(Firefox) 브라우저를 제어하기 위해 드라이버를 설치하고 선관위 웹사이트로 이동한다.\n다음으로 CSS 선택자(selector)를 이용하여 선거유형, 선거명, 선거코드, 시도 등의 조회조건을 순차적으로 선택하고 클릭 이벤트를 실행하여 검색조건을 완성한 후 검색 버튼을 클릭하여 해당 조건에 맞는 당선인 명단이 포함된 HTML 표를 브라우저에 렌더링한다. 마지막 단계로, 다시 CSS 선택자로 해당 표를 선택하고 getElementAttribute() 함수를 통해 HTML 표를 추출한 다음, rvest 패키지 read_html(), html_table() 함수를 이용하여 HTML 표를 데이터프레임으로 변환하여 tibble 형태로 크롤링 작업을 마무리 힌다.\n과거 RSelenium을 install.packages() 명령어를 통해서 CRAN에서 다운로드를 할 수는 없었으나 이제 CRAN, RSelenium 에서 직접 설치가 가능하고 GitHub rOpenSci 저장소에서 devtools로 설치한다.\n\n# devtools::install_github(\"ropensci/binman\")\n# devtools::install_github(\"ropensci/wdman\")\n# devtools::install_github(\"ropensci/RSelenium\")\n\nlibrary(RSelenium)\nlibrary(tidyverse)\n\n# 1. 데이터 ----\n\nrem_driver &lt;- rsDriver(browser = \"firefox\", port = 4568L)\nremdrv_client &lt;- rem_driver[[\"client\"]]\nremdrv_client$navigate(\"http://info.nec.go.kr/main/showDocument.xhtml?electionId=0000000000&topMenuId=EP&secondMenuId=EPEI01\")\n\n# 선거유형 \"electionType2\" 선택 후 클릭\nelectionType2 &lt;- remdrv_client$findElement(using = \"css selector\", \"#electionType2\")\nelectionType2$clickElement()\n\n# 조회조건: 제21대 선택\nelectionName &lt;- remdrv_client$findElement(using = \"css selector\", \"#electionName &gt; option:nth-child(2)\")\nelectionName$clickElement()\n\n# 조회조건: 제21대 선택 &gt; 국회의원선거\nelectionCode &lt;- remdrv_client$findElement(using = \"css selector\", \"#electionCode &gt; option:nth-child(2)\")\nelectionCode$clickElement()\n\n# 조회조건: 제21대 선택 &gt; 국회의원선거 &gt; 시도 &gt; 서울특별시\ncityCode &lt;- remdrv_client$findElement(using = \"css selector\", \"#cityCode &gt; option:nth-child(2)\")\ncityCode$clickElement()\n\n# 검색 실행\nrunButton &lt;- remdrv_client$findElement(using = \"css selector\", \"#searchBtn\")\nrunButton$clickElement()\n\n# HTML 표 --&gt; 데이터프레임 변환\nwinner_html &lt;- remdrv_client$findElement(\"css\", \"#table01\")$getElementAttribute(\"outerHTML\")[[1]]\n\nwinner_table &lt;- read_html(winner_html) %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  .[[1]] \n\nremdrv_client$close()\n\nwinner_table |&gt; \n  select(-직업, -학력, -경력)\n\n#&gt; # A tibble: 49 × 6\n#&gt;    선거구명     정당명       `성명(한자)`   성별  `생년월일(연령)` `득표수(득표율)`\n#&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;           \n#&gt;  1 종로구       더불어민주당 이낙연(李洛淵) 남    1952.12.20(67세) 54,902(58.38)   \n#&gt;  2 중구성동구갑 더불어민주당 홍익표(洪翼杓) 남    1967.11.20(52세) 70,387(54.25)   \n#&gt;  3 중구성동구을 더불어민주당 박성준(朴省俊) 남    1969.04.23(50세) 64,071(51.96)   \n#&gt;  4 용산구       미래통합당   권영세(權寧世) 남    1959.02.24(61세) 63,891(47.80)   \n#&gt;  5 광진구갑     더불어민주당 전혜숙(全惠淑) 여    1955.05.05(64세) 56,608(53.68)   \n#&gt;  6 광진구을     더불어민주당 고민정(高旼廷) 여    1979.08.23(40세) 54,210(50.37)   \n#&gt;  7 동대문구갑   더불어민주당 안규백(安圭伯) 남    1961.04.29(58세) 51,551(52.72)   \n#&gt;  8 동대문구을   더불어민주당 장경태(張耿態) 남    1983.10.12(36세) 55,230(54.54)   \n#&gt;  9 중랑구갑     더불어민주당 서영교(徐瑛敎) 여    1964.11.11(55세) 55,185(57.76)   \n#&gt; 10 중랑구을     더불어민주당 박홍근(朴洪根) 남    1969.10.08(50세) 74,131(59.28)   \n#&gt; # ℹ 39 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n\n\n그림 26.4: 동적 웹 페이지 데이터 추출과정\n\n\n\n\n\n\n\n\n자바(Java) 설치 오류\n\n\n\nRSelenium 패키지를 사용하기 위해서는 자바(Java)가 설치되어 있어야 한다. 데이터 과학 PC (Java) - 윈도우 블로그 게시글을 참조하거나 library(multilinguer); install_java() 함수를 사용해서 설치하여 사용할 수 있다.\n\njava_check()에서 다음과 같은 에러가 발생했습니다: PATH to JAVA not found. Please check JAVA is installed.\n\n\n\n\n\n\n그림 26.2: 선관위 제21대 국회의원 당선인 명부 엑셀파일\n그림 26.3: 국가선거정보 - 당선인 정보 조회 서비스 API 명세서\n그림 26.4: 동적 웹 페이지 데이터 추출과정",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>웹 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html",
    "href": "ingest_spreadsheet.html",
    "title": "27  상용 데이터",
    "section": "",
    "text": "27.1 통계패키지\nSPSS, SAS, STATA는 널리 사용되는 통계 분석 소프트웨어 패키지로, 각각 고유한 파일 형식을 사용한다. 고유한 파일 형식을 갖게 되면 데이터가 통계 패키지 내부에서 원활히 동작할 수 있는 메타 정보를 담을 수 있고 속도 향상도 기대할 수 있다. 그러나 이러한 독점적인 파일 형식은 다른 통계 패키지와의 상호 운용성을 제한할 수 있고, 장기적으로 데이터 보존 및 이식성에 문제를 일으킬 수 있다.\nSPSS는 .sav 확장자를 사용하는 이진 파일 형식을 사용한다. .sav 파일은 데이터, 변수 레이블, 값 레이블 등의 메타데이터를 포함하고 있다. SPSS .por 확장자를 가진 파일은 다른 시스템으로 이식도 가능하다.\nSAS는 .sas7bdat 확장자를 사용하는 이진 파일 형식을 사용한다. .sas7bdat 파일은 데이터와 메타데이터를 모두 포함하며, SAS에서만 읽을 수 있다. SAS도 SPSS .por처럼 .xpt 확장자를 가진 다른 시스템에 이식 가능한 파일 형식도 지원한다.\nSTATA는 .dta 확장자를 사용하는 이진 파일 형식을 사용한다. .dta 파일에는 데이터, 변수 레이블, 값 레이블 등 메타데이터가 포함되어 있다. .dta 파일은 STATA에서만 읽을 수 있고 SAS, SPSS에서 읽을 수는 없다 하지만, ‘SAS STATA Transfer’ 프로시저를 ’SPSS Data Access Pack’을 구매하여 STATA 파일을 불러읽을 수 있으며, STATA에서 CSV 파일 형태로 내보낸 후 별도 프로시저나 팩없이 SPSS, SAS에서 불러읽을 수 있는 방법이 있다.\n하지만, 통계 패키지 간에 데이터를 교환하려면 일반적으로 .csv(쉼표로 분리된 값) 또는 .txt(탭으로 분리된 값) 형식과 같은 중간 파일 형식을 사용하는 과정에서 변수 레이블과 값 레이블과 같은 일부 메타데이터가 손실될 수 있다.\n따라서, 단기적으로 SAS/SPSS/STATA와 같은 독점 파일 형식이 제공하는 장점보다 개방형 파일 형식이 장기적으로 데이터 접근성과 재사용성을 높일 수 있다는 면에서 장점이 크다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html#통계패키지",
    "href": "ingest_spreadsheet.html#통계패키지",
    "title": "27  상용 데이터",
    "section": "",
    "text": "27.1.1 SPSS\n세종시에 위치한 한국보건사회연구원에서 조사하여 발표하는 한국복지패널데이터는 특이하게도 오픈 파일 형식만 제외하고 상용 통계 패키지가 있어야 열어볼 수 있는 SPSS, STATA, SAS 파일 형식으로 제공되고 있다. 총4가지 종류 파일을 제공하고 있지만 여기서는 다양한 파일 데이터를 불러오는 방법을 중심으로 살펴보기 때문에 가장 단순한 파일만 R 환경으로 불러오는 방법을 살펴보자.\n\n가구용데이터(SAS, SPSS, STATA):koweps_h17_2022_Beta1\n가구원용데이터(SAS, SPSS, STATA):koweps_p17_2022_Beta1\n복지인식설문용데이터(SAS, SPSS, STATA):koweps_wc17_2022_Beta1\n가구용, 가구원용, 복지인식설문용 머지데이터(SAS, SPSS, STATA):koweps_hpwc17_2022_Beta1\n\nSPSS 로 작성된 .sav 파일으로 R 환경으로 불러오기 위해서는 haven 패키지를 로드하여 SPSS (.sav) 데이터 파일을 R로 읽어온다. read_spss() 함수를 사용하여 “koweps_hpwc17_2022_Beta1.sav” 파일을 welfare_raw 데이터 프레임으로 저장한 후, map_chr() 함수를 사용하여 welfare_raw의 각 변수에 대해 attributes(.x)$label을 적용하여 변수의 레이블을 추출하고 후속 작업을 위해서 문자형 벡터로 변환시킨다.\nenframe() 함수를 사용하여 추출된 레이블을 데이터 프레임으로 변환하고, filter() 함수와 str_detect() 함수를 사용하여 “성별”, “종교”, “태어난 연도”, “혼인상태”, “가구원수”라는 키워드가 포함된 변수만 선택한다. pull() 함수를 사용하여 선택된 변수의 이름을 추출하고, setdiff() 함수를 사용하여 정규표현식 작성과정에서 함께 추출된”h1707_6aq6” 변수를 제외시킨 후 demo_vars 변수로 저장한다.\nwelfare_raw 데이터 프레임에서 select() 함수와 all_of() 함수를 사용하여 demo_vars에 해당하는 변수만 선택한 후 set_names() 함수를 사용하여 선택된 변수명을 “성별”, “종교”, “태어난 연도”, “혼인상태”, “가구원수”로 변경한다. str_split()과 dput()을 사용하여 변수 이름을 파이프(|)로 연산으로 한 명령어로 처리한다. janitor 패키지 clean_names() 함수를 사용하여 변수 이름을 깔끔하게 정리하는데, ascii = FALSE 옵션을 사용하여 한글 변수명을 유지한다.\n한국보건사회연구원에서 한국복지패널 데이터가 SPSS로 제공되고 있지만 상용 SPSS 패키지가 없더라도 R 환경에서 haven 패키지와 janitor 패키지를 활용하여 SPSS 데이터를 불러와서 본격적인 분석을 오픈 데이터 분석 및 통계 언어 R로 수행할 준비가 되었다.\n\nlibrary(haven) # install.packages(\"foreign\")\n\n# Read the .sav file\nwelfare_raw &lt;- read_spss(\"data/file/SPSS/koweps_hpwc17_2022_Beta1.sav\")\n\n## 관심 변수 추출\ndemo_vars &lt;- welfare_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt; \n    enframe() |&gt; \n    filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt; \n    pull(name) |&gt; \n    setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nwelfare_raw %&gt;%\n  select(all_of(demo_vars)) |&gt; \n    set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt; \n    janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows\n\n\n27.1.2 SAS\nSAS 통계패키지 koweps_hpwc17_2022_beta1.sas7bdat 파일을 작성된 동일한 한국보건사회연구원에서 한국복지패널 데이터도 haven 패키지를 사용하여 read_sas() 함수를 사용하여 SAS 데이터 파일(.sas7bdat)을 불러온다. 이후 코드는 앞서 SPSS 데이터를 R 인구통계 데이터프레임으로 변환시켜 가져온 것과 동일한 방법으로 진행된다. 즉, 코드를 재사용하게 된다.\n\nlibrary(haven) # install.packages(\"foreign\")\n\nsas_raw &lt;- read_sas(\"data/file/SAS/koweps_hpwc17_2022_Beta1.sas7bdat\")\n\n## 관심 변수 추출\nsas_vars &lt;- sas_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt;\n  enframe() |&gt;\n  filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt;\n  pull(name) |&gt;\n  setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nsas_raw %&gt;%\n  select(all_of(sas_vars)) |&gt;\n  set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt;\n  janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows\n\n\n27.1.3 STATA\nSTATA 통계패키지 koweps_hpwc17_2022_beta1.dta 파일은 SAS 버전과 동일한 한국복지패널 데이터다. R에서 haven 패키지 read_dta() 함수를 사용하여 STATA 데이터 파일(.dta)을 불러올 수 있다. 이후 코드는 앞서 SPSS, SAS 데이터를 R로 가져와 인구통계 데이터프레임으로 변환한 것과 동일한 방법으로 진행된다. 따라서 이전에 작성한 코드를 그대로 재사용할 수 있다.\n\nlibrary(haven) # install.packages(\"haven\")\n\n# STATA 파일 불러오기\nstata_raw &lt;- read_dta(\"data/file/STATA/Koweps_hpwc17_2022_beta1.dta\")\n\n## 관심 변수 추출\nstata_vars &lt;- stata_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt;\n  enframe() |&gt;\n  filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt;\n  pull(name) |&gt;\n  setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nstata_raw %&gt;%\n  select(all_of(stata_vars)) |&gt;\n  set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt;\n  janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html#엑셀",
    "href": "ingest_spreadsheet.html#엑셀",
    "title": "27  상용 데이터",
    "section": "\n27.2 엑셀",
    "text": "27.2 엑셀\n엑셀에서 복잡한 데이터를 관리하는 방법 중 하나는 여러 시트에 데이터를 나누어 저장하는 것이다. readxl 패키지를 사용하여 엑셀 파일을 불러올 때 excel_sheets() 함수를 사용하여 엑셀 파일에 저장된 시트 이름을 확인할 수 있다. 이후 read_excel() 함수를 사용하여 원하는 시트를 불러올 수 있다. 제20대 국회의원 선거 지역구와 비례대표 당선자 명단을 담은 엑셀 파일을 불러오는 예제를 살펴보자. 국회의원 정수는 총 300 명이기 때문에 지역구 당선자와 비례 당선자를 하나로 묶어 각 당별로 당선자가 몇명인지 확인할 수 있다.\n\n\n\n\n\n그림 27.2: 제20대 국회의원 선거 당선자 명단\n\n\nreadxl 패키지의 excel_sheets() 함수를 사용하여 제20대와 제21대 국회의원선거 당선인 정보가 담긴 엑셀 파일 “data/국회의원_당선인명부.xlsx”에서 시트명을 추출하여 winners_sheetname 변수에 저장한다. read_excel() 함수를 사용하여 지역구 당선자 명단은 winners_sheetname의 첫 번째 시트에서, 비례대표 당선자 명단은 두 번째 시트에서 추출하여 각각 winners_precinct와 winners_prop 변수에 저장한다. bind_rows() 함수를 사용하여 지역구와 비례대표 당선자 명단을 하나로 합치고, 비례대표 당선자의 선거구명을 “비례대표”로 설정하여 지역구와 자료구조를 맞춘 후 winners 데이터프레임에 저장한다. count() 함수를 사용하여 소속정당명별 당선자수를 집계하고, 당선자수 순으로 정렬한다.\n\nlibrary(tidyverse)\nlibrary(readxl) \n\n# 엑셀 파일에서 시트명 추출\nwinners_sheetname &lt;- readxl::excel_sheets(\"data/국회의원_당선인명부.xlsx\")\nwinners_sheetname\n#&gt; [1] \"지역구20\"   \"비례대표20\" \"지역구21\"   \"비례대표21\"\n\n# 지역구와 비례 당선자 명단 추출\nwinners_precinct &lt;- readxl::read_excel(\"data/국회의원_당선인명부.xlsx\", sheet = winners_sheetname[1])\nwinners_prop &lt;- readxl::read_excel(\"data/국회의원_당선인명부.xlsx\", sheet = winners_sheetname[2])\n\n# 지역구와 비례대표 당선자 명단을 하나로 합침\nwinners &lt;- bind_rows(winners_precinct, winners_prop |&gt; \n                       mutate(선거구명 = \"비례대표\"))\n\n# 정당별 당선자수 집계\nwinners |&gt; \n  count(소속정당명, sort = TRUE, name = \"당선자수\")\n#&gt; # A tibble: 5 × 2\n#&gt;   소속정당명   당선자수\n#&gt;   &lt;chr&gt;           &lt;int&gt;\n#&gt; 1 더불어민주당      123\n#&gt; 2 새누리당          122\n#&gt; 3 국민의당           38\n#&gt; 4 무소속             11\n#&gt; 5 정의당              6",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html#구글시트",
    "href": "ingest_spreadsheet.html#구글시트",
    "title": "27  상용 데이터",
    "section": "\n27.3 구글시트",
    "text": "27.3 구글시트\n스프레드시트는 데이터 입력, 저장, 분석 및 시각화를 위해 널리 사용되는 소프트웨어 도구다. 하지만, 데이터 입력과 저장 측면에 초점을 맞추어 보면 오류도 많다. 흔히 보게 되는 엑셀 파일의 문제점 중 하나는 엉망진창(messy) 데이터다. 논문(Broman 와/과 Woo 2018)에 제시된 엉망진창 데이터 중 하나를 뽑아 깔끔한 데이터로 변환해보자.\n\n\n\n\n\n그림 27.3: 엉망진창(messy) 데이터를 깔끔한(tidy) 데이터 변환\n\n\n논문 데이터를 OCR 과정을 거친 후에 누구나 실습할 수 있도록 구글시트 messy_data로 공유되어 있다. 구글시트는 엑셀과 유사한 스프레드시트 프로그램이다. 구글시트에 저장된 데이터를 불러오기 위해서는 googlesheets4 패키지를 사용한다. googlesheets4 패키지를 사용하여 구글 시트의 엉망진창(messy) 데이터를 불러온다. read_sheet() 함수로 데이터를 읽어오고, set_names() 함수로 열 이름을 설정한다. pivot_longer() 함수를 사용하여 엉망진창 데이터를 깔끔한(tidy) 데이터로 변환한다. starts_with(\"week\") 열을 선택하고, names_sep 인수로 “_”를 기준으로 열 이름을 분리한다. names_to와 values_to 인수를 사용하여 새로운 열 이름과 값을 지정한다. select(), rename(), mutate() 함수를 사용하여 필요한 열을 선택하고 이름을 변경한다. 최종적으로 깔끔한 데이터 형태로 변환된 데이터프레임을 얻는다.\n\nlibrary(googlesheets4)\n\n# messy 데이터 불러오기\nsheet_url &lt;- \"https://docs.google.com/spreadsheets/d/19L3ASfmWTCd1YikWHIHeWKdK29nXz-hhwktFU8qLAYc/\"\n\nmessy_import &lt;- read_sheet(sheet_url, skip = 1, \n                         col_types = \"ccccccccccc\" )\nmessy_data &lt;- messy_import |&gt; \n  set_names(c(\"Mouse_ID\", \"SEX\", \"week_4_date\", \"week_4_weight\", \n              \"week_4_glucose\", \"week_6_date\", \"week_6_weight\", \n              \"week_6_glucose\", \"week_8_date\", \"week_8_weight\", \n              \"week_8_glucose\"))\n\n# messy 데이터를 tidy 데이터로 변환\ntidy_data &lt;- messy_data %&gt;%\n  mutate(across(everything(), as.character)) |&gt; \n  pivot_longer(cols = starts_with(\"week\"),\n               names_to = c(\"week\", \"type\", \".value\"),\n               values_to = c(\"date\", \"glucose\", \"weight\"),\n               names_sep = \"_\")  |&gt; \n  select(-week) |&gt; \n  rename(week = type) |&gt; \n  mutate(mouse_id = Mouse_ID,\n         sex = SEX) %&gt;%\n  select(mouse_id, sex, week, date, glucose, weight)\n\ntidy_data\n#&gt; # A tibble: 15 × 6\n#&gt;    mouse_id sex   week  date       glucose weight\n#&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; \n#&gt;  1 3005     M     4     3/30/2007  635     19.3  \n#&gt;  2 3005     M     6     4/11/2007  460.7   31    \n#&gt;  3 3005     M     8     4/27/2007  530.2   39.6  \n#&gt;  4 3017     M     4     10/6/2006  202.4   25.9  \n#&gt;  5 3017     M     6     10/19/2006 384.7   45.1  \n#&gt;  6 3017     M     8     11/3/2006  458.7   57.2  \n#&gt;  7 3434     F     4     11/22/2006 238.9   26.6  \n#&gt;  8 3434     F     6     12/6/2006  378     45.9  \n#&gt;  9 3434     F     8     12/22/2006 409.8   56.2  \n#&gt; 10 3449     M     4     1/5/2007   121     27.5  \n#&gt; 11 3449     M     6     1/19/2007  191.3   42.9  \n#&gt; 12 3449     M     8     2/2/2007   182.5   56.7  \n#&gt; 13 3499     F     4     1/5/2007   220.2   19.8  \n#&gt; 14 3499     F     6     1/19/2007  556.9   36.6  \n#&gt; 15 3499     F     8     2/2/2007   446     43.6\n\n깔끔한 데이터를 구글시트의 tidy_data 시트에 저장한다. sheet_write() 함수를 사용하여 깔끔한 데이터를 구글시트에 저장한다. 이때 해당 시트에 권한이 부여되어야만 데이터를 저장할 수 있다. 권한이 부여되지 않은 경우에는 그림 27.4 와 같이 편집자 권한을 부여하면 깔끔한 데이터가 구글시트에 저장된다.\n\n\n\n\n\n그림 27.4: 구글시트 messy_data 권한부여\n\n\n\ntidy_data |&gt; \n  sheet_write(ss = sheet_url, sheet = \"tidy_data\")\n✔ Writing to mess_data.\n✔ Writing to sheet tidy_data.\n\n\n\n\n그림 27.2: 제20대 국회의원 선거 당선자 명단\n그림 27.3: 엉망진창(messy) 데이터를 깔끔한(tidy) 데이터 변환\n그림 27.4: 구글시트 messy_data 권한부여\n\n\n\nBroman, Karl W., 와/과 Kara H. Woo. 2018. “Data Organization in Spreadsheets”. The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "gpt_python.html",
    "href": "gpt_python.html",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "",
    "text": "28.1 파이썬 설치\n가장 최신 파이썬 3.12 버전 대신 안정성이 검증된 파이썬 3.11 버전 다운로드 웹사이트에서 운영체제에 맞게 설치한다. 설치 과정에서 Add Python 3.11 to PATH 옵션을 반드시 선택한다.\nlibrary(reticulate)\nrepl_python()\n\n#&gt; Python 3.11.8 (C:/Users/&lt;사용자명&gt;/AppData/Local/Programs/Python/Python311/python.exe)\n#&gt; Reticulate 1.35.0 REPL -- A Python interpreter in R.\n#&gt; Enter 'exit' or 'quit' to exit the REPL and return to R.\n#&gt; &gt;&gt;&gt;",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#파이썬-설치",
    "href": "gpt_python.html#파이썬-설치",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "",
    "text": "그림 28.1: 파이썬 설치 과정\n\n\n\n\n28.1.1 가상환경\n파이썬 가상환경을 생성하는 이유는 프로젝트별로 필요한 패키지를 분리하여 관리하여 충돌을 방지하기 위함이다. 파일명(--name) 방식과 경로명(--prefix) 방식이 있으며 서로 장단점이 있다. 여기서 경로명 방식과 reticulate 패키지를 활용하여 R 내부적으로 수행하는 방식을 소개한다. virtualenv_remove() 함수를 사용하여 가상환경을 삭제할 수 있다.\n\nvirtualenv_create(\"d:/envs/gpt-ds\")\n\n#&gt; virtualenv_remove(\"d:/envs/gpt-ds\")\n#&gt; Remove virtual environment 'd:/envs/gpt-ds'? [Y/n]: Y\n#&gt; Virtual environment \"d:/envs/gpt-ds\" removed.\n\nvirtualenv_list()\n#&gt; [1] \"gpt-ds\"\n\nuse_virtualenv() 함수를 사용하여 가상환경을 활성화하고 virtualenv_install() 함수를 사용하여 필요한 패키지를 설치한다. 감성분석을 위해서 transformers 패키지가 필요한데 torch, torchvision 의존성으로 함께 설치한다.\n\nlibrary(reticulate)\nuse_virtualenv(\"gpt-ds\", required = TRUE)\n\nvirtualenv_install(\"d:/envs/gpt-ds\", c(\"transformers\", \"torch\", \"torchvision\", \"torchtext\"))\n\n\n28.1.2 .Rprofile 설정\n매번 R을 실행할 때마다 파이썬 가상환경을 활성화하는 것은 번거롭다. .Rprofile 파일에 RETICULATE_PYTHON 환경변수를 설정하게 되면, .Rprofile 파일은 R을 시작할 때 자동으로 파이썬 가상환경이 활성화된다.\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 파이썬 가상환경 gpt-ds의 python.exe 경로를 RETICULATE_PYTHON 환경변수로 설정한다.\nSys.setenv(RETICULATE_PYTHON=\"D:/envs/gpt-ds/Scripts/python.exe\")\n다시 R 세션을 시작한 후 py_config() 함수를 통해 파이썬 환경을 확인하면 앞서 설정한 gpt-ds 가상환경이 해당 디렉토리로 지정된 것을 확인할 수 있다.\n\nlibrary(reticulate)\npy_config()\n\n#&gt; python:         D:/envs/gpt-ds/Scripts/python.exe\n#&gt; libpython:      C:/Users/statkclee/AppData/Local/Programs/Python/Python311/python311.dll\n#&gt; pythonhome:     D:/envs/gpt-ds\n#&gt; version:        3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          D:/envs/gpt-ds/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.4\n#&gt; \n#&gt; NOTE: Python version was forced by RETICULATE_PYTHON",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#감성분석-사례",
    "href": "gpt_python.html#감성분석-사례",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "\n28.2 감성분석 사례",
    "text": "28.2 감성분석 사례\n앞서 구축한 파이썬 가상환경에서 음식점 리뷰에 대한 감성분석을 수행하는 사례를 살펴본다. 먼저 transformers 라이브러리 pipeline을 사용하여 텍스트 분류 모델을 로드하고, 한국어 문장을 입력으로 받아 다국어 버트(bert) 모형으로 감성 분석을 수행한다. 사전 학습된 모델을 활용하여 자연어 처리 감성분석 작업을 수행하는 방법이 앞서 구축한 파이썬 가상환경에서 수행되고 있다.\n\nfrom transformers import pipeline\n\nprompt = \"분위기도 좋았고 음식도 아주 훌륭했어.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n#&gt; [{'label': '4 stars', 'score': 0.5038140416145325}]",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#rstudio-부조종사",
    "href": "gpt_python.html#rstudio-부조종사",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "\n28.3 RStudio 부조종사",
    "text": "28.3 RStudio 부조종사\nGitHub Copilot은 Visual Studio Code를 비롯한 다양한 코드 편집기와 통합할 수 있는 AI 기반 코딩 도우미(Assistant)다. 부조종사(Copilot)는 개발자가 보다 효율적이고 정확하게 코드를 작성할 수 있도록 도와주는 지능형 도우미 기능 제공하기 위해 GitHub와 OpenAI가 공동으로 개발했다.\nCopilot은 머신 러닝 알고리즘을 사용하여 다른 개발자가 작성한 코드를 분석하고 학습한 다음 현재 코드베이스에 삽입할 수 있는 제안 및 코드 조각을 생성한다. 이 기능은 방대한 코드 코퍼스를 학습한 신경망을 사용하여 개발자가 작성할 가능성이 있는 다음 코드 줄을 예측하는 방식으로 작동한다.\n이 기술을 통해 Copilot은 현재 코드의 컨텍스트를 기반으로 전체 함수 또는 클래스를 제안하고 구문적으로 정확하고 모범 사례를 준수하는 코드를 생성할 수도 있다. 또한 Copilot은 생성하는 코드의 의미와 목적을 이해할 수 있으므로 새로운 아이디어를 빠르게 프로토타입으로 만들거나 문제에 대한 다양한 해결책을 모색해야 하는 개발자에게 유용한 도구다.\nGitHub Copilot은 Python, JavaScript, TypeScript, Ruby, Go, R 등 다양한 프로그래밍 언어와 원활하게 작동하도록 설계되었다. 또한 사용자 지정이 가능하므로 개발자가 특정 코드베이스에 대해 학습시켜 제안을 개선하고 더욱 정확하게 만들 수 있다.\nCopilot을 사용하면 얻을 수 있는 잠재적 이점은 상당하다. 코드 작성에 필요한 시간과 노력을 줄임으로써 개발자는 새로운 기능을 설계하거나 기존 기능을 개선하는 등 더 복잡한 작업에 집중할 수 있다. 또한 Copilot은 모범 사례를 따르고 구조적으로 건전하고 읽기 쉬운 코드를 생성하도록 프로그래밍되어 있으므로 오류와 버그를 줄이는 데 도움이 될 수 있다.\nR 사용자는 RStudio를 많이 사용했으나 최근 챗GPT, Github Copilot의 부상으로 개발방식에 변화가 생겨나고 있다. 하지만, RStudio가 곧 Copilot 지원하지는 않을 예정이다. RStudio는 무료 오픈 소스인 반면 Copilot은 Microsoft의 독점 기술이며, Microsoft는 공식 비공개 소스 소프트웨어 및 플러그인에서만 사용할 수 있도록 라이선스를 부여하고 있다. 시중에 존재하는 몇몇 타사 플러그인은 공식 플러그인에서 바이너리를 추출하여 작동하지만, RStudio IDE에는 이런 우회 편법적인 방법을 취하고 있지는 않고 있다.1\n이러한 기술적, 문화적 제약으로 인해 Microsoft와 Posit이 RStudio 내에서 Copilot을 허용하는 방법과 RStudio가 공개 데이터 및 기술을 사용하여 Copilot과 유사한 AI 프로그래밍 도우미를 구현하는 방법 등 다양한 방식을 검토한 후에 2023년 7월 RStudio IDE에 GitHub Copilot 통합 버전을 출시했다.\nRStudio 부조종사는 RStudio IDE에 GitHub Copilot을 통합한 버전으로, RStudio 사용자가 RStudio IDE에서 Copilot을 사용할 수 있도록 지원한다. RStudio 부조종사는 RStudio IDE의 기능과 GitHub Copilot의 기능을 통합하여 RStudio 사용자가 더욱 효율적으로 코드를 작성하고 디버깅할 수 있도록 도와준다. RStudio 부조종사는 RStudio IDE의 사용성을 향상시키고 RStudio 사용자가 더욱 효율적으로 코드를 작성할 수 있도록 도와준다.\n\n\n\n\n\n그림 28.2: RStudio 부조종사 설정방법",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#openai-api",
    "href": "gpt_python.html#openai-api",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "\n28.4 OpenAI API",
    "text": "28.4 OpenAI API\nOpenAI API는 OpenAI의 인공지능 모형을 활용하는 API 서비스로, 개발자가 쉽게 인공지능 모형을 활용할 수 있도록 제공된다. OpenAI API를 사용하면 개발자는 챗GPT, DALL-E, 위스퍼, 임베딩 등 OpenAI의 다양한 인공지능 언어, 음성, 시각 모형을 활용하여 다양한 인공지능 응용프로그램을 개발할 수 있다. OpenAI API는 RESTful API로 제공되며, 개발자는 HTTP 요청을 통해 OpenAI의 인공지능 모형을 활용할 수 있다. OpenAI, 챗GPT(ChatGPT), OpenAI API를 명확히 구분하는 것이 필요하다.\n\nOpenAI: 회사명\n챗GPT(ChatGPT): AI 응용프로그램 (AI 챗팅 서비스)\nOpenAI API: OpenAI 인공지능 모형을 활용하는 API 서비스\n\nOpenAI 회사가 제시하고 있는 기준에 맞춰 따라줘야 OpenAI API를 사용할 수 있다. 이를 위해 먼저 회원가입을 통한 계정을 생성하고 본인 인증과 과금을 위한 API KEY를 생성하고 프로그래밍 언어에서 쉽게 사용할 수 있도록 파이썬의 경우 openai 패키지를 설치하고 문법에 맞춰 코드를 작성하면 된다.\n\n\n계정생성\nAPI KEY 발급\nopenai 패키지 설치\n헬로월드\n보안강화\n\n\n\nOpenAI 계정 생성 웹사이트에서 계정을 생성한다.\n\n\n\n\n\n그림 28.3: OpenAI 계정 생성\n\n\n\n\nAPI keys 웹사이트에서 API KEY를 발급받는다.\n\n\n\n\n\n그림 28.4: API 키 생성\n\n\n\n\nAPI Reference 안내에 따라 openai 패키지를 설치한다.\n$ pip install openai\n\n\nAPI키를 직접 파이썬 프로그램에 명시하고 결과를 확인한다.\n\nimport openai\n\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"OpenAI API가 뭔가요?\"\n)\n\nprint(response)\n\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\nOpenAI API\\ub294 \\uc778\\uacf5\"\n    }\n  ],\n  \"created\": 1689745304,\n  \"id\": \"cmpl-7duESieoaT985f4IKPskfcYQ3AH7F\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 14,\n    \"prompt_tokens\": 15,\n    \"total_tokens\": 29\n  }\n}\n\n\npython-dotenv 패키지를 사용하여 API KEY를 환경변수로 관리한다. .env 파일에 OpenAI 발급받은 API KEY를 저장하고 파이썬 프로그램에서 .env 파일을 읽어서 API KEY를 사용한다. 버전 관리 시스템에 .env 파일을 포함시키지 않도록 .gitignore 파일에 추가한다. 이와 같은 방식으로 API KEY를 관리하면 API KEY가 노출되는 것을 방지할 수 있다.\n\nimport requests\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n#&gt; True\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"OpenAI API가 뭔가요?\"\n)\n\nprint(response[\"choices\"][0]['text'])\n\nOpenAI API는 OpenAI가 \n\n\n\n\n\n\n그림 28.1: 파이썬 설치 과정\n그림 28.2: RStudio 부조종사 설정방법\n그림 28.3: OpenAI 계정 생성\n그림 28.4: API 키 생성",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#footnotes",
    "href": "gpt_python.html#footnotes",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "",
    "text": "Github Copilot integration with RStudio #10148↩︎",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html",
    "href": "gpt_prompt_ds.html",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "",
    "text": "29.1 챗GPT 수행가능 작업\n챗GPT는 사무노동자의 업무를 대신할 수 있으나 모든 업무를 담당할 수는 없다. 기본적으로 챗GPT가 수행 가능한 업무 범위는 사람이 직접 할 수 없는 일을 챗GPT에 지시하면 안 된다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#챗gpt-수행가능-작업",
    "href": "gpt_prompt_ds.html#챗gpt-수행가능-작업",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "",
    "text": "챗GPT 초기에는 정확한 응답결과가 요구되는 경우 사용이 권장되지 않았으나, GPT-4 출시와 챗GPT 플러그인 도입으로 수학계산 등 연산작업과 프로그래밍을 통한 계산작업에도 적용영역을 넓히고 있다.\n응답품질을 사람이 직접 확인할 수 있는 경우 챗GPT를 사용한다. 하지만 여전히 환각 문제가 해결되지 않았고, 성, 나이, 직업, 인종, 종교 등에 대한 편향성이 존재하며, 부정적인 텍스트가 응답으로 나오는 경우 책임자가 이를 걸러내야 한다.\n주민번호, 코드, 공정수율 등 민감데이터(Sensitive Data)를 프롬프트에 포함하여 챗GPT에 전달할 경우 보안 등의 문제가 발생할 수 있으므로 사용을 자제해야 한다.\n콘텐츠 저작권 이슈가 없는 경우는 적극 활용하되, 저작권 문제가 발생할 수 있는 경우에는 주의를 기울이고 필요시 법률자문 등을 받아 후속 업무를 진행한다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#데이터-과학-프롬프트",
    "href": "gpt_prompt_ds.html#데이터-과학-프롬프트",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.2 데이터 과학 프롬프트",
    "text": "29.2 데이터 과학 프롬프트\n챗GPT의 등장으로 데이터 과학 결과물 생성을 위해 과거에는 구글링, 미트업, cheatsheet 등을 데이터 과학자가 직접 파악하고 반영했다면, 이제는 챗GPT와 상호작용하여 결과물을 얻어내는 새로운 선택지가 생겼다 (Gencay 2023). 데이터를 가져와서 데이터 정제, 탐색적 데이터분석, 시각화, 기계학습, 배포 과정을 거치게 되는데, 각 단계별로 데이터 과학 질문에 대한 챗GPT 정답을 살펴보도록 하자.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질의응답-함수",
    "href": "gpt_prompt_ds.html#질의응답-함수",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.3 질의응답 함수",
    "text": "29.3 질의응답 함수\n먼저, OpenAI API 키를 환경변수에서 로드하고, OpenAI 클라이언트를 초기화한다. answer_question 함수는 데이터 과학 질문을 입력받아 GPT-4 모델에 전달하고, 모델결과를 반환한다. 이 함수에 “데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?”라는 질문을 전달하고, 모델 응답결과를 출력한다.\n\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nclient = OpenAI(\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n)\n\ndef answer_question(text):\n    prompt = f\"당신은 R 언어를 사용하는 데이터 과학자로 다음 질문을 친절히 설명해주세요: {text}\"\n    \n    try:\n        response = client.chat.completions.create(\n            model='gpt-4',\n            messages=[\n                {\"role\": \"user\", \n                 \"content\": prompt }\n            ]\n        )\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        return str(e)\n\n# Example usage\nanswer_text = answer_question(\"데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\")\nprint(answer_text)\n\n동일한 프롬프트에 모형을 gpt-3.5-turbo와 현존하는 가장 높은 성능을 보이고 있는 gpt-4를 사용하여 결과를 비교해보자. 토큰 당 달리 과금되기 때문에 이점을 염두에 두고 데이터 과학 “데이터 탐색”에 대한 두 모형의 차이를 살펴보면 도움이 된다.\n\n\nGPT-3.5\n데이터 탐색은 데이터를 분석하기 전에 데이터셋을 살펴보고 이해하는 단계를 말합니다. 데이터 탐색 단계에서는 데이터의 구조, 패턴, 이상치, 결측값 등을 파악하고 데이터의 특성을 이해하기 위해 시각화와 통계 분석을 활용합니다.\n데이터 과학에서 데이터 탐색은 매우 중요한 단계로, 데이터의 특성과 패턴을 파악하는 데 도움을 줍니다. 데이터 탐색을 통해 데이터의 잠재적인 문제나 패턴을 발견할 수 있고, 데이터 전처리나 모델링 단계에서 필요한 추가적인 작업을 수행할 수 있습니다.\n또한, 데이터 탐색을 통해 데이터의 관계나 인사이트를 발견하고 이를 시각화하여 이해하기 쉽게 전달할 수 있습니다. 따라서 데이터 탐색은 데이터 과학의 핵심적인 부분이며, 데이터를 더 잘 이해하고 분석할 수 있는 기반을 제공합니다. 이러한 데이터 탐색 과정을 R 언어를 사용하여 다양한 시각화 기법과 통계 분석을 통해 진행할 수 있습니다.\n\n\n\n\nGPT-4\n데이터 탐색 혹은 탐색적 데이터 분석(Exploratory Data Analysis, EDA)는 데이터를 열어보고 분석하는 단계입니다. 그래프나 통계량을 사용하여 수집된 데이터의 구조 및 관계를 파악하는 데 사용됩니다.\n데이터 탐색의 목적은 데이터의 주요 특성을 찾아내고, 구조를 이해하거나 특정 변수 사이의 관계를 확인하거나, 예상치 못한 패턴이나 이상치를 찾는 것입니다.\n데이터 과학에서 데이터 탐색은 매우 중요합니다. 일단 분석 전에 데이터의 특성을 이해하는 것은 추론 및 모델링 과정에서 중요한 역할을 합니다. 데이터 탐색을 통해 이상치를 발견할 수 있고, 이를 통해 데이터 클리닝 및 전처리를 이행할 수 있습니다. 또한, 변수들간의 관계 및 패턴을 분석함으로써, 이후의 분석 방향을 결정하거나 가설을 설정하는데 도움이 됩니다.\n예를 들어, 어떤 상점의 판매 데이터를 분석한다고 가정해봅시다. 데이터 탐색 과정에서 고객 연령대별로 구매 패턴이 유의미하게 차이가 난다면 이를 분석에 반영하여 연령대별 마케팅 전략을 수립하는데 도움이 될 수 있습니다. 또한, 특정 상품이 후불결제 시 결제 실패율이 높다는 이상치를 발견하였다면, 이 문제를 해결하기 위한 방안을 마련하는데 도움이 됩니다. 또한 데이터 탐색을 통해 데이터의 품질 및 적절성을 평가할 수 있으며, 데이터 과학자가 보다 의미있는 결과와 인사이트를 도출하는데 기여하게 됩니다.\n\n\n\n두 모형의 결과를 비교해보면 gpt-4가 더 자세하고 구체적인 내용을 제공하는 것을 확인할 수 있다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#프롬프트",
    "href": "gpt_prompt_ds.html#프롬프트",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.4 프롬프트",
    "text": "29.4 프롬프트\n탐색적 데이터 분석에 초점을 맞춰 다음 질문에 대해서 챗GPT 응답결과를 정리해보자.\n\n데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\n데이터 탐색을 위한 인기있는 R 패키지는 무엇이며, 어떻게 사용되나요?\nR을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\nt-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\ntibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\ntibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\ntibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\ntibble과 ggplot2를 사용하여 산점도와 라인 차트를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n\n\n\n\ngraph LR\n    subgraph 이해하기[\"이해하기\"]\n        모형 --&gt; 시각화\n        변환 --&gt; 모형\n        시각화 --&gt; 변환\n    end\n    import --&gt; 깔끔화 --&gt; 이해하기\n    이해하기 --&gt; 의사소통\n\n    classDef modern fill:#fff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\n    classDef emphasize fill:#8CBDE3,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\n    classDef subgraphStyle fill:#f0f8ff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:15px;\n\n    class import,깔끔화,변환,모형,시각화,의사소통 modern;\n    class 이해하기 subgraphStyle;\n\n\n\n\n\n\n\n29.4.1 질문 1\n\nlibrary(reticulate)\nquestion_01 &lt;- \"데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\"\n\nanswer_01 &lt;- py$answer_question(question_01)\n\n\n\n질문\n답변\n\n\n\n데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\n\n\n데이터 탐색 혹은 탐색적 데이터 분석(Exploratory Data Analysis, EDA)는 데이터를 열어보고 분석하는 단계입니다. 그래프나 통계량을 사용하여 수집된 데이터의 구조 및 관계를 파악하는 데 사용됩니다.\n데이터 탐색의 목적은 데이터의 주요 특성을 찾아내고, 구조를 이해하거나 특정 변수 사이의 관계를 확인하거나, 예상치 못한 패턴이나 이상치를 찾는 것입니다.\n데이터 과학에서 데이터 탐색은 매우 중요합니다. 일단 분석 전에 데이터의 특성을 이해하는 것은 추론 및 모델링 과정에서 중요한 역할을 합니다. 데이터 탐색을 통해 이상치를 발견할 수 있고, 이를 통해 데이터 클리닝 및 전처리를 이행할 수 있습니다. 또한, 변수들간의 관계 및 패턴을 분석함으로써, 이후의 분석 방향을 결정하거나 가설을 설정하는데 도움이 됩니다.\n예를 들어, 어떤 상점의 판매 데이터를 분석한다고 가정해봅시다. 데이터 탐색 과정에서 고객 연령대별로 구매 패턴이 유의미하게 차이가 난다면 이를 분석에 반영하여 연령대별 마케팅 전략을 수립하는데 도움이 될 수 있습니다. 또한, 특정 상품이 후불결제 시 결제 실패율이 높다는 이상치를 발견하였다면, 이 문제를 해결하기 위한 방안을 마련하는데 도움이 됩니다. 또한 데이터 탐색을 통해 데이터의 품질 및 적절성을 평가할 수 있으며, 데이터 과학자가 보다 의미있는 결과와 인사이트를 도출하는데 기여하게 됩니다.\n\n\n\n\n29.4.2 질문 2\n\nquestion_02 &lt;- \"R을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_02 &lt;-  py$answer_question(question_02)\n\n\n\n질문\n답변\n\n\n\nR을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n물론입니다. R 언어를 활용한 간단한 데이터 탐색 스크립트를 제공하겠습니다.\n우선, 내장 데이터셋인 “mtcars”을 사용하여 살펴보겠습니다.\n# 데이터 불러오기\ndata(mtcars)\n\n# 데이터의 상위 6개 행 보기\nhead(mtcars)\n\n# 데이터의 기본적인 구조 보기\nstr(mtcars)\n\n# 간단한 요약통계 보기\nsummary(mtcars)\n\n# 변수 간 상관관계 보기\ncor(mtcars)\n\n# mpg 변수에 대한 히스토그램 생성\nhist(mtcars$mpg)\n\n# 변수 간 산점도 행렬 생성\npairs(mtcars)\n위의 예시는 mtcars 데이터셋을 불러와 파일의 상위 6줄을 보는 것으로 시작합니다. 그 신 dataset의 전반적인 구조 (변수의 이름, 유형, 데이터 셋에서 행의 수 등) 를 확인하고, 요약 통계를 제공합니다. 그 다음으로, 변수 간 상관관계를 설명하며 mpg 변수를 가진 히스토그램을 그립니다. 마지막으로, 산점도 행렬을 생성하여 변수들 사이의 관계를 한눈에 보여줍니다.\n이러한 단계들은 기본적인 데이터 탐색 과정에서 중요하며, 상황에 따라 추가분석이 필요한 경우가 있습니다. 이는 데이터에 대한 조사나, 누락 된 값의 확인, 이상치 검사 등이 포함될 수 있습니다.\n\n\n\n\n29.4.3 질문 3\n\nquestion_03 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_03 &lt;- py$answer_question(question_03)\n\n\n\n질문\n답변\n\n\n\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\nPCA (Principal Component Analysis)는 데이터의 차원 축소를 위한 테크닉 중 하나입니다. 이는 관찰된 변수들 간의 상관관계를 기반으로 데이터의 차원을 축소하는 방법입니다.\nPCA를 사용하여 R에서 변수의 관계를 탐색하려면 다음과 같이 수행할 수 있습니다:\n\n데이터 전처리: 분석을 위한 데이터는 종종 어떤 변수도 누락되지 않도록 전처리되어야 합니다. 또한, PCA는 숫자 변수에 대해 작동하므로 범주형 데이터는 적절하게 변환되어야 합니다.\n스케일링: PCA는 상관행렬이나 공분산 행렬에 대해 수행되므로 모든 변수가 동일한 스케일을 가져야 합니다. 이를 위해 scale() 함수를 사용하여 데이터를 정규화 할 수 있습니다.\nPCA 수행: prcomp() 또는 princomp() 함수를 사용하여 PCA를 수행하고 결과를 저장할 수 있습니다.\n\npca &lt;- prcomp(data, scale = TRUE)\n\n결과 시각화 및 해석: summary() 함수와 biplot() 함수를 사용하여 결과를 해석하고 시각화 할 수 있습니다. summary(pca)를 사용하여 설명된 분산의 백분율을 검토해 보고, biplot(pca)를 사용하여 기여도를 시각화 할 수 있습니다.\n\nsummary(pca)\nbiplot(pca)\n이렇게 하면 각 주성분이 데이터의 분산을 어느 정도 설명하는지 그리고 각 변수가 주성분에 어떻게 기여하는지 쉽게 이해할 수 있습니다. 이 정보를 통해 변수간의 관계를 분석하고 이해할 수 있습니다.\n\n\n\n\n29.4.4 질문 4\n\nquestion_04 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_04 &lt;- py$answer_question(question_04)\n\n\n\n질문\n답변\n\n\n\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\nPCA(Principal Component Analysis, 주성분 분석)는 많은 변수에서 가장 중요한 패턴을 찾을 수 있도록 도와주는 통계적 기법으로, 이를 통해 데이터의 차원을 축소할 수 있습니다. R에서는 ‘prcomp’ 또는 ‘princomp’ 함수를 사용하여 PCA를 수행할 수 있습니다.\n추상적인 개념이지만 구체적인 예제를 통해 설명해보겠습니다.\n\n데이터 준비: 먼저, 분석할 데이터셋을 로드하고 정제합니다. 모든 변수는 수치형이어야 하며, 결측치가 없어야 합니다.\n데이터 정규화: PCA는 변수의 스케일에 민감하므로, 데이터는 진행하기 전에 반드시 정규화(normalization)되어야 합니다. R에서는 scale() 함수를 사용해서 모든 변수를 같은 스케일로 조정할 수 있습니다.\nPCA 수행: prcomp() 함수를 사용하여 PCA를 실행합니다. 예를 들어, 데이터 프레임이 df라면 아래와 같이 코드를 작성할 수 있습니다. pca_result &lt;- prcomp(df, center = TRUE, scale. = TRUE) 이 함수는 PCA 결과를 반환하며, 각 주성분의 중요도, 각 원래 변수가 주성분에 얼마나 기여하는지 (rotation 또는 loadings라고 함) 등을 제공합니다.\n결과 분석: summary() 함수를 사용하여 각 주성분이 데이터의 얼마나 큰 비율을 설명하는지 확인할 수 있습니다. summary(pca_result) 또한 R에서 제공하는 다양한 시각화 기능 (예: biplot)을 사용하여 결과를 시각화하고 분석할 수 있습니다. 이를 통해 변수 간의 관계를 더 자세히 조사하거나, 데이터를 더 낮은 차원으로 투영하여 차원 축소를 수행할 수 있습니다.\n\n따라서 PCA는 데이터의 차원을 줄이면서도 가장 중요한 정보를 유지하는 데 도움이 되며, 이는 특히 데이터의 패턴을 탐색하거나 시각화할 때 유용합니다.\n\n\n\n\n29.4.5 질문 5\n\nquestion_05 &lt;- \"t-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_05 &lt;- py$answer_question(question_05)\n\n\n\n질문\n답변\n\n\n\nt-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n네, 말씀하신대로 R 언어를 사용하여 t-SNE, PCA 및 클러스터링을 사용하여 데이터의 변수 간 관계를 탐색하는 샘플 스크립트를 아래에 제공합니다. 이 예시에서는 내장 데이터 세트인 ’iris’를 사용합니다.\n# 필요한 패키지를 불러오기\nlibrary(tidyverse)\nlibrary(Rtsne)\nlibrary(cluster)\nlibrary(factoextra)\n\n# 데이터 불러오기\ndata(\"iris\")\n\n# PCA를 이용한 변수 간 관계 탐색\npca_iris &lt;- prcomp(iris[, 1:4], scale = TRUE)\n\n# PCA 결과 확인\nfviz_pca_ind(pca_iris, geom.ind='point',\n             pointshape = 21, pointsize = 2, \n             fill.ind = iris$Species, palette = \"jco\", \n             addEllipses = TRUE, ellipse.level = 0.95)\n\n# t-SNE를 이용한 변수간 관계 탐색\ntsne_iris &lt;- Rtsne(iris[, 1:4], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500)\n\n# t-SNE 결과 확인 \ntsne_iris$Y %&gt;%\n  data.frame() %&gt;%\n  setNames(c(\"X\", \"Y\")) %&gt;%\n  mutate(Species = iris$Species) %&gt;%\n  ggplot(aes(X, Y, color = Species)) +\n  geom_point() +\n  theme_minimal()\n\n# 클러스터링을 이용한 변수간 관계 탐색\ndist_iris &lt;- dist(iris[, 1:4])\nhclust_iris &lt;- hclust(dist_iris)\ncluster_iris &lt;- cutree(hclust_iris, k = 3)\n\n# 클러스터링 결과 확인\ntable(cluster_iris, iris$Species)\nplot(hclust_iris)\nrect.hclust(hclust_iris, k = 3, border = \"red\")\nt-SNE와 PCA는 고차원 데이터를 시각화하기 위해 사용되며, 이를 통해 변수들 사이의 관계를 기하학적으로 이해할 수 있습니다. 클러스터링은 데이터 세트를 비슷한 특성을 가진 그룹으로 분류할 때 사용하며, 이를 통해 관계를 파악할 수 있습니다. 출력 그래프와 함께 이들 결과를 해석하면 더 깊은 인사이트를 얻을 수 있습니다.\n\n\n\n\n29.4.6 질문 6\n\nquestion_06 &lt;- \"tibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\"\n\n# answer_06 &lt;- py$answer_question(question_06)\n\n\n\n질문\n답변\n\n\n\ntibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\n\n\nR 언어에서 tibble과 ggplot2 패키지는 시계열 데이터에서 패턴과 추세를 식별하는데 사용할 수 있는 유용한 도구입니다.\n\n첫 번째로, tibble은 R에서 데이터 프레임을 다루기 위한 새로운 데이터 타입입니다. 기본 R 데이터 프레임의 단점을 보완하면서, 동일한 구조와 작업을 유지합니다. 그러므로 tibble은 아주 큰 데이터셋에서도 처리 속도가 빠르며, 시계열 데이터를 포함한 모든 종류의 데이터를 쉽게 다룰 수 있게 해줍니다.\n\n예를 들어, 우리는 다음과 같은 방법으로 시계열 데이터를 tibble로 변환할 수 있습니다:\n# 필요한 패키지 불러오기\nlibrary(tidyverse)\n\n# 일별 주가 데이터를 tibble로 변환\nstock_prices &lt;- as_tibble(stock_prices)\n\n다음으로, ggplot2는 ’The Grammar of Graphics’를 기반으로 하는 데이터 시각화 패키지입니다. 이를 이용해 시계열 데이터에서 패턴과 추세를 직관적으로 파악할 수 있습니다.\n\n예를 들어, ggplot2를 이용해 주가의 시간에 따른 변화를 그래프로 그릴 수 있습니다.\nggplot(data = stock_prices, aes(x = Date, y = Close)) +\n  geom_line() +\n  labs(title = \"Stock prices over time\", x = \"Date\", y = \"Close price\")\n여기서 geom_line() 함수는 시간에 따른 주가의 변화를 선 그래프로 그려줍니다. 이를 통해 데이터에서 패턴이나 트렌드를 시각적으로 파악할 수 있습니다.\n또한, ggplot2 패키지는 다양한 통계 변환 함수와 smoothers를 제공하여 시계열 데이터의 추세를 더 잘 나타낼 수 있습니다. 예를 들어, geom_smooth() 함수는 일련의 데이터 포인트에 적합한 부드러운 곡선을 추가하여 데이터의 전반적인 추세를 도식화해 줍니다.\n\n\n\n\n29.4.7 질문 7\n\nquestion_07 &lt;- \"tibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_07 &lt;- py$answer_question(question_07)\n\n\n\n질문\n답변\n\n\n\ntibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n네, 물론입니다. 다음의 예를 들어보겠습니다. 먼저 필요한 라이브러리를 로드합니다:\nlibrary(tidyverse)  # includes both tibble and ggplot2\n다음으로, 패턴이나 추세를 확인하려는 데이터를 예로 들어, 내장되어 있는 미국 여성의 생존 기대치를 보여주는 데이터셋인 women을 불러옵니다:\ndata(\"women\")\ntibble을 사용해 데이터를 보다 더 보기 쉽게 확인할 수 있습니다:\nwomen_tibble &lt;- as_tibble(women)\nprint(women_tibble)\nggplot2을 사용해 이 데이터의 패턴과 추세를 시각화하겠습니다. 본 예시에서는 women 데이터셋에서 나이에 따른 평균 체중을 나타내는 그래프를 그립니다:\nggplot(data = women_tibble, aes(x = height, y = weight)) +\n  geom_point(shape=1) +    # Use hollow circles\n  geom_smooth(method = lm, se = FALSE, color=\"red\") +  # Add linear regression line, without CI\n  labs(title = \"Age vs Weight in Women\", x = \"Height\", y = \"Weight\") # labels\n데이터를 plot해 본 결과, women 데이터셋에서 여성의 키와 몸무게 사이에 양의 상관 관계가 있음을 발견할 수 있습니다. geom_smooth 함수를 사용하면, 이 데이터의 추세를 더 명확하게 파악할 수 있습니다. 이는 ggplot2 패키지의 많은 기능 중 하나입니다.\n이런 식으로 tibble과 ggplot2를 활용하면, 데이터를 쉽게 관리하고 시각화하면서 여러 패턴과 추세를 찾아낼 수 있습니다. 이는 데이터 과학자가 데이터를 이해하는 데 매우 중요한 과정입니다.\n\n\n\n\n29.4.8 질문 8\n\nquestion_08 &lt;- \"tibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\"\n\n# answer_08 &lt;- py$answer_question(question_08)\n\n\n\n질문\n답변\n\n\n\ntibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\n\n\ntibble과 ggplot2는 R 언어에서 데이터를 다루고 시각화하는 데 널리 사용되는 도구입니다. 변수 간 관계를 탐색하는 일반적인 기법은 아래와 같습니다.\n\n\ntibble 사용:\ntibble은 데이터 프레임을 다루는 R 패키지로, 데이터 프레임을 더 유연하게 조작할 수 있도록 만들어진 향상된 데이터 프레임입니다. tibble을 사용하면, 데이터 요약 및 변환을 위한 dplyr 함수들을 사용할 수 있습니다.\n변수 간 관계를 탐색하는 tibble의 일반적인 방법은 ‘group_by’, ‘summarize’와 같은 dplyr 함수를 사용하여 특정 변수에 따라 데이터를 그룹 지어 분석하는 것입니다. ’filter’, ‘select’, ‘mutate’, ’arrange’와 같은 함수들도 유용하게 사용될 수 있습니다.\n\n\nggplot2 사용:\nggplot2는 R에서 가장 인기 있는 데이터 시각화 패키지 중 하나로, 데이터를 그래프로 그려 복잡한 데이터 패턴을 빠르게 이해할 수 있도록 합니다.\nggplot2를 사용해서 변수 간 관계를 탐색하는 일반적인 방법 중 하나는 산점도 (scatter plot)를 그리는 것입니다. 두 변수 간의 관계를 보여주는 가장 기본적인 그래프 형태입니다.\n또한, ggplot2의 ‘facet_wrap’ 또는 ‘facet_grid’ 기능을 사용하면, 한 변수의 서로 다른 수준에 따른 다른 변수의 분포를 여러 개의 작은 패널로 보여주는 그래프를 만들 수 있습니다.\n’geom_smooth’를 사용하면 추세선을 추가하여 변수 간 관계를 보다 명확하게 확인할 수도 있습니다.\n\n\n이외에도 histogram, boxplot, violin plot 등 다양한 그래프를 활용하여 변수 간 관계를 탐색할 수 있습니다. 주어진 데이터 특성과 질문에 따라 적절한 방법을 선택하면 됩니다.\n\n\n\n\n29.4.9 질문 9\n\nquestion_09 &lt;- \"tibble과 ggplot2를 사용하여 산점도와 선 그래프를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\"\n\n# answer_09 &lt;- py$answer_question(question_09)\n\n\n\n질문\n답변\n\n\n\ntibble과 ggplot2를 사용하여 산점도와 선 그래프를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\n\n\nR에서 ’tibble’이란 데이터 프레임을 보다 쉽게 다룰 수 있게 설계된 데이터 구조이며, ’ggplot2’는 복잡한 그래프를 효과적으로 생성할 수 있게 도와주는 패키지입니다.\n선 그래프와 산점도를 생성하여 변수 간의 관계를 탐색하는 방식은 아래와 같이 구성할 수 있습니다.\n먼저, 필요한 패키지를 설치하고 불러와야 합니다.\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\nlibrary(dplyr)\nlibrary(ggplot2)\n다음으로, tibble 형태의 데이터를 생성하거나 불러옵니다.\ndf &lt;- tibble::tibble(\n  x = rnorm(100),  # 100개의 표준 정규분포 난수 생성\n  y = 1.5 * x + rnorm(100)  # x값에 따른 y값 생성\n)\n이 데이터는 x와 y이라는 두 개의 변수로 이루어진 100개의 관측치를 담고 있습니다.\n그 후, ’ggplot2’를 사용하여 산점도를 생성할 수 있습니다.\nggplot(data = df, mapping = aes(x = x, y = y)) +\n  geom_point()  # 산점도 생성\n선 그래프를 생성하기 위해서는 ’geom_line()’을 사용합니다.\nggplot(data = df, mapping = aes(x = x, y = y)) +\n  geom_line()  # 선 그래프 생성\n이렇게 하면 x와 y의 관계를 산점도와 선 그래프로 시각적으로 표현할 수 있습니다. 각 점이나 선의 위치와 형태를 통해 변수들 간의 관계를 살펴볼 수 있게 됩니다.\n추가적으로, 여러 그래픽 파라미터를 조정하는 것으로 그래프를 보다 세밀하게 조정할 수 있습니다. 예를 들어, ‘labs()’, ‘theme()’, ‘geom_smooth()’, ‘facet_wrap()’ 등의 함수들을 조합하면 다양한 복잡한 그래프를 만들 수 있습니다.\n\n\n\n\n29.4.10 질문 10\n\nquestion_10 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_10 &lt;- py$answer_question(question_10)\n\n\n\n질문\n답변\n\n\n\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n주성분 분석(PCA)는 R에서 차원 축소를 수행하는 데 사용되는 기법 중 하나로, 변동이 가장 큰 데이터의 축을 찾아 높은 차원의 데이터를 낮은 차원으로 변환시키는 기법을 설명합니다.\n다음은 R에서 PCA를 수행하기 위한 간단한 단계입니다:\n\n데이터를 준비합니다. PCA는 연속형 변수에만 적용될 수 있으므로, 범주형 변수를 포함하고 있다면 이를 제거하거나 숫자로 변환해야 합니다.\n데이터 스케일링을 수행합니다. PCA는 변수의 범위에 따라 결과가 크게 달라질 수 있으므로, 데이터의 표준화 또는 정규화 과정이 필요합니다. R의 scale() 함수를 사용할 수 있습니다.\nprcomp() 또는 princomp() 함수를 사용하여 PCA를 수행합니다. 주성분의 개수를 지정하거나, 데이터에 총 변동성의 몇 퍼센트를 설명하도록 할지 결정할 수 있습니다.\nsummary() 함수를 사용하여 결과를 확인합니다. 여기에는 각 주성분이 설명하는 변동성의 비율, 누적 설명된 변동중 어느 정도의 비율을 차지하는 지 등의 정보가 포함되어 있습니다.\n필요에 따라 결과를 시각화합니다. 이를 통해 각 주성분이 어떤 변수와 강한 연관성을 보이는지 확인할 수 있습니다. biplot() 함수를 사용하면 주성분과 개별 변수 사이의 관계를 한눈에 볼 수 있습니다.\n\n이런 방법을 통해 데이터 세트의 차원을 축소하고, 변수 간의 패턴이나 관계를 더 잘 이해하고 시각화하는 데 도움이 될 수 있습니다.\n\n\n\n\n\n\n그림 29.1: AI 소통하는 챗팅 인터페이스\n그림 29.2: 챗GPT 사용이유\n그림 29.3: 데이터 과학자 업무방식 변화\n\n\n\nGencay, I. 2023. “ChatGPT Guide for Data Scientists: Top 40 Most Important Prompts”. Towards AI, 4월. https://pub.towardsai.net/chatgpt-guide-for-data-scientists-top-40-most-important-prompts-cdb911f3a427.\n\n\nHardie, Mary, 와/과 Swapan Saha. 2012. “Builders Perceptions of Lowest Cost Procurement and Its Impact on Quality”. Construction Economics and Building 9 (1): 1–8. https://doi.org/10.5130/ajceb.v9i1.3009.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_interpreter.html",
    "href": "gpt_interpreter.html",
    "title": "\n30  데이터프레임 분석\n",
    "section": "",
    "text": "30.1 광명시 인구현황 분석",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>데이터프레임 분석</span>"
    ]
  },
  {
    "objectID": "gpt_interpreter.html#광명시-인구현황-분석",
    "href": "gpt_interpreter.html#광명시-인구현황-분석",
    "title": "\n30  데이터프레임 분석\n",
    "section": "",
    "text": "30.1.1 데이터 수집\n행정안전부 주민등록 인구 및 세대현황 웹사이트에서 광명시 2023년 7월 기준 인구현황 데이터를 가져온다.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nraw_data &lt;- read_excel(\"data/202307_202307_연령별인구현황_월간.xlsx\", skip =3) \n\nraw_data |&gt; \n  janitor::clean_names(ascii = FALSE) |&gt; \n  select(1:10) |&gt; \n  head() |&gt; \n  gt::gt()\n\n\n\n\n\n\n행정기관코드\n행정기관\n총_인구수\n연령구간인구수_4\nx0세_5\nx1세_6\nx2세_7\nx3세_8\nx4세_9\nx5세_10\n\n\n\n4121000000\n경기도 경기도 광명시\n280,986\n280,986\n1,373\n1,470\n1,582\n1,588\n1,729\n1,924\n\n\n4121051000\n경기도 광명시 광명1동\n56\n56\n0\n0\n0\n0\n0\n0\n\n\n4121052000\n경기도 광명시 광명2동\n2,953\n2,953\n9\n7\n11\n5\n5\n8\n\n\n4121054000\n경기도 광명시 광명3동\n9,869\n9,869\n29\n21\n20\n20\n23\n24\n\n\n4121055000\n경기도 광명시 광명4동\n8,114\n8,114\n35\n29\n44\n27\n28\n35\n\n\n4121056000\n경기도 광명시 광명5동\n12,206\n12,206\n41\n40\n64\n51\n49\n60\n\n\n\n\n\n\n\n\n30.1.2 가공데이터\n\nlibrary(tidyverse)\n\npop_tbl &lt;- \n  read_rds(\"data/kwangmyung_pop.rds\")\n\npop_tbl |&gt; \n  head() |&gt; \n  gt::gt()\n\n\n\n\n\n\n시점\n행정동\n성별\n나이\n인구수\n\n\n\n2023-07-15\n광명1동\n남자\n0\n0\n\n\n2023-07-15\n광명1동\n남자\n1\n0\n\n\n2023-07-15\n광명1동\n남자\n2\n0\n\n\n2023-07-15\n광명1동\n남자\n3\n0\n\n\n2023-07-15\n광명1동\n남자\n4\n0\n\n\n2023-07-15\n광명1동\n남자\n5\n0\n\n\n\n\n\n\n\n\n30.1.3 챗GPT 데이터 변환\n데이터를 가공한 후 챗GPT Code Interpreter 분석용으로 변환시켜 데이터를 전송한다.\n\npop_tbl |&gt; \n  write_csv(\"data/kwangmyung_pop.csv\")\n\n\n\n\n\n\n그림 30.1: 코드 인터프리터 광명시 인구분석 작업흐름\n\n\n 광명시 인구",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>데이터프레임 분석</span>"
    ]
  },
  {
    "objectID": "gpt_interpreter.html#챗gpt-code-interpreter",
    "href": "gpt_interpreter.html#챗gpt-code-interpreter",
    "title": "\n30  데이터프레임 분석\n",
    "section": "\n30.2 챗GPT Code Interpreter",
    "text": "30.2 챗GPT Code Interpreter\n\n\n데이터 불러오기\n선그래프\n남녀별\n동별\n유권자 비율\n동별 유권자\n동별 유권자(정렬)\n동별 유권자(비율)\n동별 유권자(비교)\n노트북 내보내기\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 주피터 노트북",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>데이터프레임 분석</span>"
    ]
  },
  {
    "objectID": "gpt_interpreter.html#마무리-작업",
    "href": "gpt_interpreter.html#마무리-작업",
    "title": "\n30  데이터프레임 분석\n",
    "section": "\n30.3 마무리 작업",
    "text": "30.3 마무리 작업\n\n30.3.1 연도별 인구수\n\nlibrary(tidyverse)\n\npop_tbl &lt;- \n  read_rds(\"data/kwangmyung_pop.rds\")\n\npop_tbl |&gt; \n  mutate(시점 = ymd(시점)) |&gt; \n  mutate(연도 = year(시점)) |&gt; \n  group_by(연도) |&gt; \n  summarise(인구수 = sum(인구수)) |&gt; \n  ggplot(aes(x=연도, y=인구수)) +\n    geom_line(color=\"skyblue\", size=1.5) +\n    geom_point(color=\"blue\", size=3) +\n    labs(title=\"광명시 인구수 변화\", x=\"\", y=\"인구수(명)\",\n         caption = \"데이터 출처: 행정안전부 주민등록 인구 및 세대현황\") +\n    theme_minimal() +\n    scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n30.3.2 연도별 인구수\n\nlibrary(tidyverse)\n\npop_tbl &lt;- \n  read_rds(\"data/kwangmyung_pop.rds\")\n\npop_tbl |&gt; \n  mutate(시점 = ymd(시점)) |&gt; \n  mutate(연도 = year(시점)) |&gt; \n  group_by(연도) |&gt; \n  summarise(인구수 = sum(인구수)) |&gt; \n  ggplot(aes(x=연도, y=인구수)) +\n    geom_line(color=\"skyblue\", size=1.5) +\n    geom_point(color=\"blue\", size=3) +\n    labs(title=\"광명시 인구수 변화\", x=\"\", y=\"인구수(명)\",\n         caption = \"데이터 출처: 행정안전부 주민등록 인구 및 세대현황\") +\n    theme_minimal() +\n    scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n30.3.3 동별 인구수\n\n\npop_tbl |&gt; \n  mutate(시점 = ymd(시점)) |&gt; \n  mutate(연도 = year(시점)) |&gt; \n  group_by(연도, 행정동) |&gt; \n  summarise(인구수 = sum(인구수)) |&gt; \n  ggplot(aes(x=연도, y=인구수, color = 행정동, group=행정동)) +\n    geom_line(size=1.5) +\n    geom_point(color=\"blue\", size=3) +\n    labs(title=\"광명시 동별 인구수 변화\", x=\"\", y=\"인구수(명)\",\n         caption = \"데이터 출처: 행정안전부 주민등록 인구 및 세대현황\") +\n    theme_minimal() +\n    scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n30.3.4 유권자 비율\n\npop_tbl |&gt; \n  mutate(시점 = ymd(시점)) |&gt; \n  mutate(연도 = year(시점)) |&gt; \n  mutate(유권자 = ifelse(나이 &gt;=18, \"유권자\", \"비유권자\"),\n         유권자 = factor(유권자, levels = c(\"비유권자\", '유권자'))) |&gt; \n  group_by(유권자) |&gt; \n  summarise(인구수 = sum(인구수)) |&gt; \n  ggplot(aes(x=\"\", y=인구수, fill = 유권자)) +\n    geom_bar(width = 1, stat = \"identity\") +\n    coord_polar(\"y\")  +\n    labs(title=\"광명시 유권자 비율\",\n         caption = \"데이터 출처: 행정안전부 주민등록 인구 및 세대현황\",\n         fill = \"구분\") +\n    theme_minimal() +\n    scale_y_continuous(labels = scales::comma) +\n    scale_fill_manual(values = c(유권자 = \"blue\", 비유권자=\"red\"))\n\n\n\n\n\n\n\n\n30.3.5 내림차순 유권자수\n\npop_tbl |&gt; \n  mutate(시점 = ymd(시점)) |&gt; \n  mutate(연도 = year(시점)) |&gt; \n  filter(연도 == max(연도)) |&gt; \n  mutate(유권자 = ifelse(나이 &gt;=18, \"유권자\", \"비유권자\"),\n         유권자 = factor(유권자, levels = c(\"비유권자\", '유권자'))) |&gt; \n  group_by(행정동, 유권자) |&gt; \n  summarise(인구수 = sum(인구수)) |&gt; \n  mutate(유권자비율 = 인구수/sum(인구수)) |&gt; \n  ungroup() |&gt; \n    \n  # pivot_wider(names_from = 유권자, values_from = 인구수) |&gt; \n  # mutate(유권자비율 = 유권자/(비유권자+유권자)) |&gt; \n  # arrange(desc(유권자비율))\n\n  # 시각화 -----------------\n  ggplot(aes(x=fct_reorder2(행정동, 유권자, 유권자비율), y=유권자비율, fill = 유권자)) +\n    geom_col() +\n    coord_flip() +\n    labs(title=\"광명시 동별 유권자 비율\",\n         caption = \"데이터 출처: 행정안전부 주민등록 인구 및 세대현황\",\n         fill = \"구분\",\n         x = \"\") +\n    theme_minimal() +\n    scale_y_continuous(labels = scales::percent) +\n    scale_fill_manual(values = c(유권자 = \"blue\", 비유권자=\"red\")) +\n    theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n그림 30.1: 코드 인터프리터 광명시 인구분석 작업흐름",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>데이터프레임 분석</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Abu-Mostafa, Yaser S, Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012.\nLearning from Data. Vol. 4. AMLBook New York.\n\n\nBecker, Richard. 2018. The New s Language. CRC Press.\n\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in\nSpreadsheets.” The American Statistician 72 (1): 2–10.\nhttps://doi.org/10.1080/00031305.2017.1375989.\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science.\nLeanpub.\n\n\nChambers, J. M., and T. J. Hastie. 1992. Statistical Models in\ns. London: Chapman & Hall.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of\nStatistics and Data Visualization.\n\n\nGencay, I. 2023. “ChatGPT Guide for Data Scientists: Top 40 Most\nImportant Prompts.” Towards AI, April. https://pub.towardsai.net/chatgpt-guide-for-data-scientists-top-40-most-important-prompts-cdb911f3a427.\n\n\nHardie, Mary, and Swapan Saha. 2012. “Builders\nPerceptions of Lowest Cost Procurement and Its Impact on\nQuality.” Construction Economics and Building 9 (1):\n1–8. https://doi.org/10.5130/ajceb.v9i1.3009.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial\nData Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian\nBriese, and Markus Neteler. 2016. “OpenEO: A GDAL for Earth\nObservation Analytics.” 2016. https://r-spatial.org/2016/11/29/openeo.html.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data\nAnalysis.” Journal of Statistical Software 40: 1–29.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법.”\n프롭빅스(PROPBIX), no. 13 (September). http://www.kahps.org/.",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "gpt_rtutor.html",
    "href": "gpt_rtutor.html",
    "title": "31  AI 데이터 분석",
    "section": "",
    "text": "31.1 RTutor\n남 다코다 대학 스티븐 지(Steven Ge) 교수가 개인 프로젝트로 개발한 RTutor는 csv 와 같은 정형 데이터셋을 전달하고 자연어로 데이터 과학 작업을 수행할 수 있도록 개발된 Shiny와 OpenAI API가 결합된 웹 응용프로그램이다. 데이터셋을 업로드하고 영어나 한국어로 질문하거나 분석 요청을 하면 RTutor가 R 코드를 생성하여 실행을 한다. R 프로그래밍 언어에 대한 기본기가 없어도 자연어로 데이터 과학 업무를 수행할 수 있다.\nRTutor는 CSV, 엑셀과 같은 형식의 데이터셋을 전달받 df 데이터 프레임으로 자동 변환한다. 사용자가 자연스러운 언어로 분석 요청을 하면, OpenAI gpt-3.5-turbo, gpt-4-turbo를 통해 자연어을 R 코드로 변환시키고 이를 실행한다. 사용자는 통계량, 그래프, 표 등의 결과를 받게 되며 나중에 모아서 R 마크다운 파일로 다운로드할 수 있다.\nRTutor는 처음 R 언어에 초점을 두고 개발되었으나 현재는 파이썬도 지원하며 초기 비용문제 GPT-3.5-turbo에서 gpt-4-turbo도 지원하여 코드 품질이 높아졌다. 소스 코드는 GitHub에서 개발자들이 사용자 지정 통합 및 솔루션을 구축할 수 있도록 공개되어 있다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>AI 데이터 분석</span>"
    ]
  },
  {
    "objectID": "gpt_rtutor.html#rtutor",
    "href": "gpt_rtutor.html#rtutor",
    "title": "31  AI 데이터 분석",
    "section": "",
    "text": "그림 31.1: RTutor 실행화면",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>AI 데이터 분석</span>"
    ]
  },
  {
    "objectID": "gpt_rtutor.html#판다스ai",
    "href": "gpt_rtutor.html#판다스ai",
    "title": "31  AI 데이터 분석",
    "section": "31.2 판다스AI",
    "text": "31.2 판다스AI\nPandasAI는 자연어를 사용하여 다양한 데이터(CSV, XLSX, PostgreSQL, MySQL, BigQuery, Databrick, Snowflake 등)를 분석하는 파이썬 라이브러리다. 백엔드에 생성형 AI를 활용하여 데이터 탐색, 정제, 분석을 지원한다.\nPandasAI는 데이터 질의 기능 외에도 그래프를 통한 데이터 시각화, 결측값 처리를 통한 데이터셋 정제, 특성 생성을 통한 데이터 품질 향상 기능을 제공하여 데이터 과학자와 분석가를 위한 만능도구로 자리매김하고자 한다.\nPandasAI는 자연어 질의를 이해하고 해석하여 파이썬 코드와 SQL 쿼리로 변환하는 생성형 AI 모델을 사용한다. 그런 다음 코드를 사용하여 데이터와 상호 작용하고 결과를 사용자에게 반환한다.\n\n\n\n\n\n\ngraph LR\n   classDef default fill:#fff, stroke:#333, stroke-width:1px;\n\n   subgraph SmartDataframe\n       A[\"판다스&lt;br&gt;데이터프레임\"] --&gt; C[\"SmartDataframe&lt;br&gt;객체\"]\n       B[\"OpenAI LLM (gpt-4)\"] --&gt; C\n   end\n\n   subgraph prompt[\"프롬프트 입력\"]\n       D[자연어 질의] --&gt; E[프롬프트]\n   end\n\n   subgraph exec[코드 생성 및 실행]\n       direction LR\n       E --&gt; F[\"파이썬 &lt;br&gt;코드 생성\"]\n       F --&gt; G[코드 실행]\n       G --&gt; 결과\n   end\n\n   C --&gt; E\n\n\n\n\n그림 31.2: PandasAI 작업흐름도\n\n\n\n\n\nPandasAI 예제 코드에 들어 있는 판다스 데이터프레임을 생성하고, OpenAI를 통해 자연어 질의를 수행하는 예제를 살펴보자.\npandas와 pandasai를 사용해 데이터를 처리하고 OpenAI gpt-4 LLM으로 자연어를 파이썬 코드로 변경한 후 파이썬 코드를 실행하여 자연어 질의에 응답하는 방식으로 작성되었다. 먼저 필요한 라이브러리를 가졍고 .env 파일에서 OpenAI API 키를 로드한다. 이후 국가별 매출 데이터를 담은 데이터프레임을 만들고, OpenAI gpt-4 모델을 활용해 SmartDataframe 객체로 변환한다. 객체 chat 메소드를 사용해 “가장 매출이 많은 나라는? 한글로 번역해줘.”라는 질문에 대한 답변을 얻는다.\n\nimport pandas as pd\nfrom pandasai import SmartDataframe\nfrom pandasai.llm.openai import OpenAI\n\nfrom dotenv import load_dotenv, find_dotenv\nimport os\n\nload_dotenv()\n\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n\nsales_by_country = pd.DataFrame({\n    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n    \"sales\": [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]\n})\n\nllm = OpenAI(api_token=OPENAI_API_KEY, model=\"gpt-4\")\n\nsdf = SmartDataframe(sales_by_country, config={\"llm\": llm, \"verbose\": False})\n\nsdf.chat('가장 매출이 많은 나라는? 한글로 번역해줘.')\n#&gt; '가장 매출이 많은 나라는 China입니다.'\n\n“x 축 국가로 두고 매출 막대형 차트 그리기” 라는 자연어 질의를 이해하고 matplotlib 라이브러리를 사용해 막대형 차트를 생성하고 결과를 이미지 파일로 저장시킨다.\n\nsdf.chat('x 축 국가로 두고 매출 막대형 차트 그리기')\n#&gt; 'D:/tcs/gpt-ds/exports/charts/temp_chart.png'\n\n\n\n\nPandasAI는 막대그래프 실행결과\n\n\n\n\n\n그림 31.1: RTutor 실행화면\nPandasAI는 막대그래프 실행결과",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>AI 데이터 분석</span>"
    ]
  }
]