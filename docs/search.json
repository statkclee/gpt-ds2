[
  {
    "objectID": "ingest_file.html",
    "href": "ingest_file.html",
    "title": "25  파일 데이터",
    "section": "",
    "text": "25.1 유니코드와 UTF-8\n사람 간의 의사소통은 다양한 기호 체계를 통해 이루어진다. 영어 알파벳, 한글, 한자 등 문자가 의사소통에 사용되는 좋은 예이다. 디지털 환경에서 이러한 의사소통을 가능하게 하는 기술적 장치가 바로 문자 집합과 문자 인코딩 및 디코딩이다.\n컴퓨터 시스템은 이진수 바이트를 기본 단위로 사용한다. 바이트는 파일 형태로 묶이거나 네트워크를 통해 전송되어 다른 시스템에 도달한다. 이 데이터가 사람에게 의미 있는 정보로 전달되기 위해서는 인코딩(부호화)과 디코딩(복호화) 과정을 거쳐야 한다.\n컴퓨터 시스템은 데이터를 바이트(Byte) 형태로 처리한다. 이 바이트 데이터는 이진수, 즉 010101과 같은 형태로 표현되고, 바이트 데이터를 사람이 읽을 수 있는 문자로 변환하는 최초의 표준이 ASCII(아스키)다. 하지만 ASCII는 256개 문자만을 지원하기 때문에, CJK(중국, 일본, 한국)와 같은 동아시아 문화권에서는 그 한계가 명확하다. 이러한 한계를 해결하기 위해 유니코드(Unicode)가 도입되었다. 유니코드는 영문자는 물론이고 지구상의 거의 모든 문자와 기호를 디지털로 표현할 수 있는 방법을 제공한다.\n유니코드(Unicode)는 글자와 코드가 1:1 매핑되어 있는 단순한 코드표에 불과하고 산업 표준으로 일종의 국가 당사자 간 약속이다. 한글이 표현된 유니코드 영역도 위키백과 유니코드 영역에서 찾을 수 있다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#유니코드와-utf-8",
    "href": "ingest_file.html#유니코드와-utf-8",
    "title": "25  파일 데이터",
    "section": "",
    "text": "유니코드와 UTF-8\n\n\n\n\n\n\n\n인코딩 (Encoding)\n\n\n\n문자 인코딩(character encoding), 줄여서 인코딩은 사용자가 입력한 문자나 기호들을 컴퓨터가 이용할 수 있는 신호로 만드는 것을 말한다. 넓은 의미의 컴퓨터는 이러한 신호를 입력받고 처리하는 기계를 뜻하며, 신호 처리 시스템을 통해 이렇게 처리된 정보를 사용자가 이해할 수 있게 된다.\n\nAll text has a character encoding.\n\n\n\n\n25.1.1 인코딩 문제\n문자 인코딩은 컴퓨터가 텍스트를 바이트로 변환하거나 바이트를 텍스트로 변환하는 방법이다. 인코딩 과정에서는 다양한 문제가 발생할 수 있고, 그중 세 가지 문제가 많이 알려져 있다. 첫 번째는 ’두부(Tofu)’라 불리는 상황으로, 컴퓨터가 어떤 문자를 표현해야 할지 알지만, 화면에 어떻게 출력해야 할지 모르기 때문에 빈 사각형 상자로 표시된다. 두 번째는 ’문자 깨짐(Mojibake, 文字化け)’이다. 특히 일본어에서 자주 발생하며, 한 인코딩 방식으로 작성된 텍스트가 다른 인코딩 방식으로 해석될 때 문자가 깨지는 현상을 의미한다. 세 번째는 ’의문부호(Question Marks)’로, 특정 문자가 다른 문자로 변환될 때 발생된다. 문자 집합과 인코딩 궁합이 맞지 않을 때 발생하며, 데이터 손실과 오류도 야기된다.\n\n\n세 가지 인코딩 문제\n\n\n25.1.2 문자 집합\n아스키 코드\n디지털 글쓰기는 내용과 상관없이 결국 텍스트로 표현되고, 텍스트는 단지 문자이다. 하지만, 컴퓨터가 문자 하나를 어떻게 표현할까?\n1960년대 미국식 영문자를 컴퓨터로 표현하는 해결책은 간단했다. 알파벳 26개(대문자, 소문자), 숫자 10개, 구두점 몇 개, 그리고 전신을 보내던 시절에 제어를 위해 사용된 몇 개의 특수 문자(“새줄로 이동”, “본문 시작”, “경고음” 등)가 전부였다. 모두 합쳐도 128개보다 적어서, 아스키(ASCII) 위원회가 문자마다 7비트( \\(2^7\\) = 128)를 사용하는 인코딩으로 표준화했다. 1\n\n\n\n\n\n그림 25.1: 제어문자와 출력 가능한 아스키 문자표 알파벳 예시\n\n\n그림 25.1 는 아스키 문자표에서 제어문자 10개와 출력 가능한 아스키 문자표 중 영문 대문자 A-I까지 10개를 뽑아 사례로 보여준다. 즉, 문자표는 어떤 문자가 어떤 숫자에 해당하는지를 정의하고 있다.\n확장 아스키\n아스키(ASCII) 방식으로 숫자 2, 문자 q, 혹은 곡절 악센트 ^를 표현하는 데 충분하다. 하지만, 투르크어족 추바시어 ĕ, 그리스 문자 β, 러시아 키릴 문자 Я는 어떻게 저장하고 표현해야 할까? 7비트를 사용하면 0에서 127까지 숫자를 부여할 수 있지만, 8비트(즉, 1바이트)를 사용하게 되면 255까지 표현할 수 있다. 그렇다면, ASCII 표준을 확장해서 추가되는 128개 숫자에 대해 추가로 문자를 표현할 수 있게 된다.\n\n아스키: 0…127\n확장된 아스키: 128…255\n\n불행하게도, 영어 문자를 사용하지 않는 세계 곳곳에서 많은 사람들이 시도를 했지만, 방식도 다르고, 호환이 되지 않는 방식으로 작업이 되어, 결과는 엉망진창이 되었다. 예를 들어, 실제 텍스트가 불가리아어로 인코딩되었는데 스페인어 규칙을 사용해서 인코딩한 것으로 프로그램이 간주하고 처리될 경우 결과는 무의미한 횡설수설 값이 출력된다. 이와는 별도로 한중일(CJK) 동아시아 국가들을 비롯한 많은 국가들이 256개 이상의 기호를 사용한다. 왜냐하면 8비트로는 특히 동아시아 국가 문자를 표현하는 데 부족하기 때문이다.\n한글 완성형과 조합형\n1980년대부터 컴퓨터를 사용하신 분이라면 완성형과 조합형의 표준화 전쟁을 지켜봤을 것이고, 그 이면에는 한글 워드프로세서에 대한 주도권 쟁탈전이 있었던 것을 기억할 것이다. 결국 완성형과 조합형을 모두 포용하는 것으로 마무리되었지만, 여기서 끝난 게 아니다. 유닉스 계열에서 KSC5601을 표준으로 받아들인 EUC-KR과 90년대와 2000년대를 호령한 마이크로소프트 CP949가 있었다. 결국 대한민국 정부에서 주도한 표준화 전쟁은 유닉스/리눅스, 마이크로소프트 모두를 녹여내는 것으로 마무리되었고, 웹과 모바일 시대는 유니코드로 넘어가서 KSC5601이 유니코드의 원소로 들어가는 것으로 마무리되었다.\n이제 신경 쓸 것은 인코딩, 즉 utf-8만 신경 쓰면 된다. 그리고 남은 디지털 레거시 유산을 잘 처리하면 된다.\n\n\n\n\n\n\n유닉스/리눅스(EUC-KR), 윈도우(CP949)\n\n\n\nEUC-KR, CP949 모두 2바이트 한글을 표현하는 방식으로 동일점이 있지만, EUC-KR 방식은 KSC5601-87 완성형을 초기에 사용하였으나, KSC5601-92 조합형도 사용할 수 있도록 확장되었다. CP949는 확장 완성형으로도 불리며 EUC-KR에서 표현할 수 없는 한글 글자 8,822자를 추가한 것으로 마이크로소프트 코드페이지(Code Page) 949를 사용하면서 일반화되었다.\n\n\n유니코드\n1990년대에 나타나기 시작한 해결책을 유니코드(Unicode)라고 부른다. 예를 들어, 영어 A 대문자는 1바이트, 한글 가는 3바이트다. 유니코드는 정수값을 서로 다른 수만 개 문자와 기호를 표현하는 데 정의한다. ’A’는 U+0041, ’가’는 U+AC00과 같이 고유한 코드 포인트를 가진다. 하지만, 파일에 혹은 메모리에 문자열로 정수값을 저장하는 방식을 정의하지는 않는다.\n각 문자마다 8비트를 사용하던 방식에서 32비트 정수를 사용하는 방식으로 전환하면 되지만, 영어, 에스토니아어, 브라질 포르투갈어 같은 알파벳 언어권에는 상당한 공간 낭비가 발생된다. 접근 속도가 중요한 경우 메모리에 문자당 32비트를 종종 사용한다. 하지만, 파일에 데이터를 저장하거나 인터넷을 통해 전송하는 경우 대부분의 프로그램과 프로그래머는 이와는 다른 방식을 사용한다.\n다른 방식은 (거의) 항상 UTF-8으로 불리는 인코딩으로, 문자마다 가변 바이트를 사용한다. 하위 호환성을 위해, 첫 128개 문자(즉, 구 아스키 문자 집합)는 바이트 1개에 저장된다. 다음 1920개 문자는 바이트 2개를 사용해서 저장된다. 다음 61,000개는 바이트 3개를 사용해서 저장해 나간다.\n궁금하다면, 동작 방식이 다음 표에 나타나 있다. “전통적” 문자열은 문자마다 1바이트를 사용한다. 반대로, “유니코드” 문자열은 문자마다 충분한 메모리를 사용해서 어떤 텍스트 유형이든 저장한다. R, 파이썬 3.x에서 모든 문자열은 유니코드다. 엄청난 바이트를 읽어오거나 저장하여 내보내려고 할 때, 인코딩을 지정하는 것은 엄청난 고통이다.\n유니코드 문자열은 여는 인용부호 앞에 소문자 U를 붙여 표시한다. 유니코드 문자열을 바이트 문자열로 전환하려면, 인코딩을 명세해야만 된다. 항상 UTF-8을 사용해야 하고, 그 밖의 인코딩을 사용하는 경우 매우, 매우 특별히 좋은 사유가 있어야만 된다. 특별한 인코딩을 사용하는 경우 두 번 생각해 보라.\n\n\n아스키에서 유니코드로 진화과정\n\n컴퓨터가 처음 등장할 때 미국 영어권 중심 아스키가 아니고 4바이트로 전 세계 모든 글자를 표현할 수 있는 유니코드가 사용되었다면 한글을 컴퓨터에 표현하기 위한 지금과 같은 번거로움은 없었을 것이다. 돌이켜보면 초기 컴퓨터가 저장 용량 한계로 인해 유니코드가 표준으로 자리를 잡더라도 실용적인 이유로 인해서 한글을 컴퓨터에 표현하기 위한 다른 대안이 제시됐을 것도 분명해 보인다. 초창기 영어권을 중심으로 아스키 표준이 정립되어 현재까지 내려오고, 유니코드와 UTF-8 인코딩이 사실상 표준으로 자리 잡았으며, 그 사이 유닉스/리눅스 EUC-KR, 윈도우즈 CP949가 빈틈을 한동안 메우면서 역할을 담당했다.\n\n\n\n\n\n\n\n\n\n항목\nASCII (1963)\nEUC-KR (1980s)\nCP949 (1990s)\nUnicode (1991)\n\n\n\n범위\n128개의 문자\n2,350개의 한글 문자 등\n약 11,172개의 완성형 한글 문자 등\n143,859개의 문자 (버전 13.0 기준)\n\n\n비트 수\n7비트\n8~16비트\n8~16비트\n다양한 인코딩 방식 (UTF-8, UTF-16, UTF-32 등)\n\n\n표준\nANSI, ISO/IEC 646\nKS X 2901\n마이크로소프트\nISO/IEC 10646\n\n\n플랫폼\n다양한 시스템\nUNIX 계열, 일부 Windows\nWindows 계열\n다양한 플랫폼\n\n\n문자 집합\n영문 알파벳, 숫자, 특수 문자\n한글, 영문 알파벳, 숫자, 특수 문자\n한글, 한자, 영문 알파벳, 숫자, 특수 문자\n전 세계 언어, 특수 문자, 이모티콘 등\n\n\n확장성\n확장 불가능\n한정적\n더 많은 문자 지원\n높은 확장성\n\n\n국제성\n영어 중심\n한국어 중심\n한국어 중심\n다국어 지원\n\n\n유니코드 호환\n호환 가능 (U+0000 ~ U+007F)\n호환 불가, 변환 필요\n유니코드와 상호 변환 가능\n자체가 표준\n\n\nUTF-8\nUTF-8(Universal Coded Character Set + Transformation Format – 8-bit의 약자)은 유니코드 중에서 가장 널리 쓰이는 인코딩으로, 유니코드를 위한 가변 길이 문자 인코딩 방식 중 하나로 켄 톰프슨과 롭 파이크가 제작했다.\nUTF-8 인코딩의 가장 큰 장점은 아스키(ASCII), 라틴-1(ISO-8859-1)과 호환되어, 문서를 처리하는 경우 아스키, 라틴-1 문서를 변환 없이 그대로 처리할 수 있고 영어를 비롯한 라틴계열 문서로 저장할 때 용량이 매우 작다. 이러한 이유로 많은 오픈소스 소프트웨어와 데이터를 생산하는 미국을 비롯한 유럽 언어권에서 UTF-8이 많이 사용되고 있지만, 한글은 한 글자당 3바이트 용량을 차지한다.\n웹 표준 인코딩\n스마트폰의 대중화에 따라 더이상 윈도우 운영체제에서 사용되는 문자체계가 더이상 표준이 되지 못하고 여러 문제점을 야기함에 따라 유니코드 + UTF-8 체제가 대세로 자리잡고 있는 것이 확연히 나타나고 있다.\n2010년 구글에서 발표한 자료에 의하면 2010년 UTF-8 인코딩이 웹에서 주류로 부상하기 시작한 것이 확인되었다. (unicode2010?) 웹 기반 플롯 디지털 도구를 활용하여 그래프(WebPlotDigitizer)에서 데이터를 추출하여 시각화하면 유사한 결과를 시각적으로 표현할 수 있다. 2010년 이후 웹에서 가장 점유율이 높은 인코딩 방식은 UTF-8으로 W3Tech 웹 기술 조사(Web Technology Surveys)를 통해 확인할 수 있다. 여기서 주목할 점은, 프랑스어, 독일어, 스페인어와 같은 서유럽 언어의 문자와 기호를 표현하는 ISO-8859-1 인코딩, 종종 “Latin-1”으로 불리는 8비트 문자 인코딩이 현저히 줄고 있다는 점이다.\n\n\n2010 ~ 2012 웹에서 UTF-8 성장세",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#문자-집합",
    "href": "ingest_file.html#문자-집합",
    "title": "24  파일 데이터",
    "section": "\n24.2 문자 집합",
    "text": "24.2 문자 집합\n\n24.2.1 아스키 코드\n디지털 글쓰기는 내용과 상관없이 결국 텍스트로 표현되고, 텍스트는 단지 문자이다. 하지만, 컴퓨터가 문자 하나를 어떻게 표현할까?\n1960년대 미국식 영문자를 컴퓨터로 표현하는 해결책은 간단했다. 알파벳 26개(대문자, 소문자), 숫자 10개, 구두점 몇 개, 그리고 전신을 보내던 시절에 제어를 위해 사용된 몇 개의 특수 문자(“새줄로 이동”, “본문 시작”, “경고음” 등)가 전부였다. 모두 합쳐도 128개보다 적어서, 아스키(ASCII) 위원회가 문자마다 7비트( \\(2^7\\) = 128)를 사용하는 인코딩으로 표준화했다. 1\n\n\n\n\n\n그림 24.2: 제어문자와 출력 가능한 아스키 문자표 알파벳 예시\n\n\n그림 24.2 는 아스키 문자표에서 제어문자 10개와 출력 가능한 아스키 문자표 중 영문 대문자 A-I까지 10개를 뽑아 사례로 보여준다. 즉, 문자표는 어떤 문자가 어떤 숫자에 해당하는지를 정의하고 있다.\n\n24.2.2 확장 아스키\n아스키(ASCII) 방식으로 숫자 2, 문자 q, 혹은 곡절 악센트 ^를 표현하는 데 충분하다. 하지만, 투르크어족 추바시어 ĕ, 그리스 문자 β, 러시아 키릴 문자 Я는 어떻게 저장하고 표현해야 할까? 7비트를 사용하면 0에서 127까지 숫자를 부여할 수 있지만, 8비트(즉, 1바이트)를 사용하게 되면 255까지 표현할 수 있다. 그렇다면, ASCII 표준을 확장해서 추가되는 128개 숫자에 대해 추가로 문자를 표현할 수 있게 된다.\n\n아스키: 0…127\n확장된 아스키: 128…255\n\n불행하게도, 영어 문자를 사용하지 않는 세계 곳곳에서 많은 사람들이 시도를 했지만, 방식도 다르고, 호환이 되지 않는 방식으로 작업이 되어, 결과는 엉망진창이 되었다. 예를 들어, 실제 텍스트가 불가리아어로 인코딩되었는데 스페인어 규칙을 사용해서 인코딩한 것으로 프로그램이 간주하고 처리될 경우 결과는 무의미한 횡설수설 값이 출력된다. 이와는 별도로 한중일(CJK) 동아시아 국가들을 비롯한 많은 국가들이 256개 이상의 기호를 사용한다. 왜냐하면 8비트로는 특히 동아시아 국가 문자를 표현하는 데 부족하기 때문이다.\n\n24.2.3 한글 완성형과 조합형\n1980년대부터 컴퓨터를 사용하신 분이라면 완성형과 조합형의 표준화 전쟁을 지켜봤을 것이고, 그 이면에는 한글 워드프로세서에 대한 주도권 쟁탈전이 있었던 것을 기억할 것이다. 결국 완성형과 조합형을 모두 포용하는 것으로 마무리되었지만, 여기서 끝난 게 아니다. 유닉스 계열에서 KSC5601을 표준으로 받아들인 EUC-KR과 90년대와 2000년대를 호령한 마이크로소프트 CP949가 있었다. 결국 대한민국 정부에서 주도한 표준화 전쟁은 유닉스/리눅스, 마이크로소프트 모두를 녹여내는 것으로 마무리되었고, 웹과 모바일 시대는 유니코드로 넘어가서 KSC5601이 유니코드의 원소로 들어가는 것으로 마무리되었다.\n이제 신경 쓸 것은 인코딩, 즉 utf-8만 신경 쓰면 된다. 그리고 남은 디지털 레거시 유산을 잘 처리하면 된다.\n\n\n\n\n\n\n유닉스/리눅스(EUC-KR), 윈도우(CP949)\n\n\n\nEUC-KR, CP949 모두 2바이트 한글을 표현하는 방식으로 동일점이 있지만, EUC-KR 방식은 KSC5601-87 완성형을 초기에 사용하였으나, KSC5601-92 조합형도 사용할 수 있도록 확장되었다. CP949는 확장 완성형으로도 불리며 EUC-KR에서 표현할 수 없는 한글 글자 8,822자를 추가한 것으로 마이크로소프트 코드페이지(Code Page) 949를 사용하면서 일반화되었다.\n\n\n\n24.2.4 유니코드\n1990년대에 나타나기 시작한 해결책을 유니코드(Unicode)라고 부른다. 예를 들어, 영어 A 대문자는 1바이트, 한글 가는 3바이트다. 유니코드는 정수값을 서로 다른 수만 개 문자와 기호를 표현하는 데 정의한다. ’A’는 U+0041, ’가’는 U+AC00과 같이 고유한 코드 포인트를 가진다. 하지만, 파일에 혹은 메모리에 문자열로 정수값을 저장하는 방식을 정의하지는 않는다.\n각 문자마다 8비트를 사용하던 방식에서 32비트 정수를 사용하는 방식으로 전환하면 되지만, 영어, 에스토니아어, 브라질 포르투갈어 같은 알파벳 언어권에는 상당한 공간 낭비가 발생된다. 접근 속도가 중요한 경우 메모리에 문자당 32비트를 종종 사용한다. 하지만, 파일에 데이터를 저장하거나 인터넷을 통해 전송하는 경우 대부분의 프로그램과 프로그래머는 이와는 다른 방식을 사용한다.\n다른 방식은 (거의) 항상 UTF-8으로 불리는 인코딩으로, 문자마다 가변 바이트를 사용한다. 하위 호환성을 위해, 첫 128개 문자(즉, 구 아스키 문자 집합)는 바이트 1개에 저장된다. 다음 1920개 문자는 바이트 2개를 사용해서 저장된다. 다음 61,000개는 바이트 3개를 사용해서 저장해 나간다.\n궁금하다면, 동작 방식이 다음 표에 나타나 있다. “전통적” 문자열은 문자마다 1바이트를 사용한다. 반대로, “유니코드” 문자열은 문자마다 충분한 메모리를 사용해서 어떤 텍스트 유형이든 저장한다. R, 파이썬 3.x에서 모든 문자열은 유니코드다. 엄청난 바이트를 읽어오거나 저장하여 내보내려고 할 때, 인코딩을 지정하는 것은 엄청난 고통이다.\n유니코드 문자열은 여는 인용부호 앞에 소문자 U를 붙여 표시한다. 유니코드 문자열을 바이트 문자열로 전환하려면, 인코딩을 명세해야만 된다. 항상 UTF-8을 사용해야 하고, 그 밖의 인코딩을 사용하는 경우 매우, 매우 특별히 좋은 사유가 있어야만 된다. 특별한 인코딩을 사용하는 경우 두 번 생각해 보라.\n\n\n아스키에서 유니코드로 진화과정\n\n컴퓨터가 처음 등장할 때 미국 영어권 중심 아스키가 아니고 4바이트로 전 세계 모든 글자를 표현할 수 있는 유니코드가 사용되었다면 한글을 컴퓨터에 표현하기 위한 지금과 같은 번거로움은 없었을 것이다. 돌이켜보면 초기 컴퓨터가 저장 용량 한계로 인해 유니코드가 표준으로 자리를 잡더라도 실용적인 이유로 인해서 한글을 컴퓨터에 표현하기 위한 다른 대안이 제시됐을 것도 분명해 보인다. 초창기 영어권을 중심으로 아스키 표준이 정립되어 현재까지 내려오고, 유니코드와 UTF-8 인코딩이 사실상 표준으로 자리 잡았으며, 그 사이 유닉스/리눅스 EUC-KR, 윈도우즈 CP949가 빈틈을 한동안 메우면서 역할을 담당했다.\n\n\n\n\n\n\n\n\n\n항목\nASCII (1963)\nEUC-KR (1980s)\nCP949 (1990s)\nUnicode (1991)\n\n\n\n범위\n128개의 문자\n2,350개의 한글 문자 등\n약 11,172개의 완성형 한글 문자 등\n143,859개의 문자 (버전 13.0 기준)\n\n\n비트 수\n7비트\n8~16비트\n8~16비트\n다양한 인코딩 방식 (UTF-8, UTF-16, UTF-32 등)\n\n\n표준\nANSI, ISO/IEC 646\nKS X 2901\n마이크로소프트\nISO/IEC 10646\n\n\n플랫폼\n다양한 시스템\nUNIX 계열, 일부 Windows\nWindows 계열\n다양한 플랫폼\n\n\n문자 집합\n영문 알파벳, 숫자, 특수 문자\n한글, 영문 알파벳, 숫자, 특수 문자\n한글, 한자, 영문 알파벳, 숫자, 특수 문자\n전 세계 언어, 특수 문자, 이모티콘 등\n\n\n확장성\n확장 불가능\n한정적\n더 많은 문자 지원\n높은 확장성\n\n\n국제성\n영어 중심\n한국어 중심\n한국어 중심\n다국어 지원\n\n\n유니코드 호환\n호환 가능 (U+0000 ~ U+007F)\n호환 불가, 변환 필요\n유니코드와 상호 변환 가능\n자체가 표준\n\n\n\n24.2.5 UTF-8\nUTF-8(Universal Coded Character Set + Transformation Format – 8-bit의 약자)은 유니코드 중에서 가장 널리 쓰이는 인코딩으로, 유니코드를 위한 가변 길이 문자 인코딩 방식 중 하나로 켄 톰프슨과 롭 파이크가 제작했다.\nUTF-8 인코딩의 가장 큰 장점은 아스키(ASCII), 라틴-1(ISO-8859-1)과 호환되어, 문서를 처리하는 경우 아스키, 라틴-1 문서를 변환 없이 그대로 처리할 수 있고 영어를 비롯한 라틴계열 문서로 저장할 때 용량이 매우 작다. 이러한 이유로 많은 오픈소스 소프트웨어와 데이터를 생산하는 미국을 비롯한 유럽 언어권에서 UTF-8이 많이 사용되고 있지만, 한글은 한 글자당 3바이트 용량을 차지한다.\n\n24.2.6 웹 표준 인코딩\n스마트폰의 대중화에 따라 더이상 윈도우 운영체제에서 사용되는 문자체계가 더이상 표준이 되지 못하고 여러 문제점을 야기함에 따라 유니코드 + UTF-8 체제가 대세로 자리잡고 있는 것이 확연히 나타나고 있다.\n2010년 구글에서 발표한 자료에 의하면 2010년 UTF-8 인코딩이 웹에서 주류로 부상하기 시작한 것이 확인되었다. (unicode2010?) 웹 기반 플롯 디지털 도구를 활용하여 그래프(WebPlotDigitizer)에서 데이터를 추출하여 시각화하면 유사한 결과를 시각적으로 표현할 수 있다. 2010년 이후 웹에서 가장 점유율이 높은 인코딩 방식은 UTF-8으로 W3Tech 웹 기술 조사(Web Technology Surveys)를 통해 확인할 수 있다. 여기서 주목할 점은, 프랑스어, 독일어, 스페인어와 같은 서유럽 언어의 문자와 기호를 표현하는 ISO-8859-1 인코딩, 종종 “Latin-1”으로 불리는 8비트 문자 인코딩이 현저히 줄고 있다는 점이다.\n\n\n2010 ~ 2012 웹에서 UTF-8 성장세",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#데이터-종류",
    "href": "ingest_file.html#데이터-종류",
    "title": "24  파일 데이터",
    "section": "\n24.4 데이터 종류",
    "text": "24.4 데이터 종류\n\n\n\n\n\ngraph TB\n\nsubgraph 가져오기[\"가져오기(Import)\"]\n\n    스프레드쉬트 --&gt; 핸들러\n    데이터베이스 --&gt; 핸들러\n    아스키 --&gt; 핸들러\n    웹데이터 --&gt; 핸들러\n    핸들러 --&gt; 데이터프레임\n\n    subgraph 아스키[\"&lt;strong&gt;아스키 파일&lt;/strong&gt;\"]\n        데이터입력[데이터 입력]\n        csv[CSV 파일]\n        tsv[TSV 파일]\n        고정길이파일[고정길이 파일]\n    end\n\nend\n\nclassDef modern fill:#f0f0f0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\nclassDef emphasize fill:#d0d0d0,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\nclassDef subgraphStyle fill:#e0e0e0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:20px;\n\nclass csv,데이터입력,tsv,고정길이파일,스프레드쉬트,데이터베이스,웹데이터,핸들러 modern\nclass 데이터프레임 emphasize\nclass 아스키,가져오기 subgraphStyle\n\n\n\n\n그림 24.3: 다양한 데이터 종류",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#아스키-파일",
    "href": "ingest_file.html#아스키-파일",
    "title": "25  파일 데이터",
    "section": "\n25.2 아스키 파일",
    "text": "25.2 아스키 파일\n아스키 파일은 텍스트 파일로, 데이터를 저장하는 가장 기본적인 형태이다. R에서 데이터프레임으로 다양한 데이터를 가져올 때, 아스키 파일은 CSV(Comma-Separated Values) 파일, TSV(Tab-Separated Values) 파일, 고정길이 파일 등 다양한 형식으로 존재한다. CSV 파일은 쉼표로 구분된 값들로 이루어진 텍스트 파일이며, TSV 파일은 탭으로 구분된 값들로 이루어진 텍스트 파일이다. 고정길이 파일은 각 필드가 고정된 길이를 가지는 텍스트 파일이다. 또한, R에서는 데이터를 직접 입력하여 데이터프레임을 생성할 수도 있다. 아스키 파일을 데이터프레임으로 가져올 때는 read.csv(), read.table(), read.fwf() 등의 함수를 사용하며, 데이터를 직접 입력할 때는 열중심 혹은 행중심에 따라 tibble(), tribble() 함수를 사용한다.\n\n\n\n\n\ngraph TB\n\nsubgraph 가져오기[\"&lt;strong&gt;가져오기&lt;/strong&gt;\"]\n\n    스프레드쉬트 --&gt; 핸들러\n    데이터베이스 --&gt; 핸들러\n    아스키 --&gt; 핸들러\n    웹데이터 --&gt; 핸들러\n    핸들러 --&gt; 데이터프레임\n\n    subgraph 아스키[\"&lt;strong&gt;아스키 파일&lt;/strong&gt;\"]\n        데이터입력[데이터 입력]\n        csv[CSV 파일]\n        tsv[TSV 파일]\n        고정길이파일[고정길이 파일]\n    end\n\nend\n\nclassDef modern fill:#f0f0f0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\nclassDef emphasize fill:#d0d0d0,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\nclassDef subgraphStyle fill:#e0e0e0,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:20px;\n\nclass csv,데이터입력,tsv,고정길이파일,스프레드쉬트,데이터베이스,웹데이터,핸들러 modern\nclass 데이터프레임 emphasize\nclass 아스키,가져오기 subgraphStyle\n\n\n\n\n그림 25.2: 다양한 데이터 종류",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#자료형",
    "href": "ingest_file.html#자료형",
    "title": "25  파일 데이터",
    "section": "\n25.4 자료형",
    "text": "25.4 자료형\n가장 많이 사용되는 콤마 구분자 아스키 파일(.csv) 파일로 불러오면서 함께 고민해야 하는 사항이 바로 자료형이다.\nR에서는 다양한 자료형을 지원한다. 가장 기본적인 자료형은 숫자형, 문자형, 범주형, 논리형이다. 숫자형은 정수형과 실수형으로 구분되며, 문자형은 문자열을 저장하는 자료형이다. 논리형은 참과 거짓을 나타내는 자료형이고, 범주형은 내부적으로 정수로 저장되지만 한정된 범주를 갖는 문자형으로 표현된다. 그외에도 날짜와 시간을 저장하는 자료형, 지도정보를 담고 있는 자료형, 이미지 정보를 담고 있는 자료형 등 다양한 자료형이 있다.\nreadr 패키지 spec() 함수를 사용하면 아스키 파일을 불러읽어 오면서 각 열의 자료형을 확인할 수 있다. spec() 함수에서 출력한 각 열 자료형이 정답은 아니지만 나름 최선의 추정으로 각 열의 자료형을 살펴본 후 최종 열별 자료형을 지정하는데 도움이 되는 것은 사실이다.\n\nspec( read_csv(\"data/file/nine_penguins.csv\") )\n#&gt; cols(\n#&gt;   species = col_character(),\n#&gt;   island = col_character(),\n#&gt;   bill_length_mm = col_double(),\n#&gt;   flipper_length_mm = col_double(),\n#&gt;   body_mass_g = col_double(),\n#&gt;   sex = col_character(),\n#&gt;   year = col_double()\n#&gt; )\n\nreadr 패키지 col_types 인자를 사용하여 각 열의 자료형을 지정할 수 있다. col_types 인자에는 cols() 함수를 사용하여 각 열의 자료형을 지정한다. cols() 함수에는 col_factor(), col_character(), col_double(), col_integer(), col_logical() 함수를 사용하여 각 열의 자료형을 지정한다. col_factor() 함수는 범주형 자료형을 지정할 때 사용하며, col_character() 함수는 문자형 자료형을 지정할 때 사용한다. col_double() 함수는 실수형 자료형을 지정할 때 사용하며, col_integer() 함수는 정수형 자료형을 지정할 때 사용한다. col_logical() 함수는 논리형 자료형을 지정할 때 사용한다.\nspec() 함수가 텍스트로 된 열은 모두 문자형(col_character())으로 인식하였지만, species, sex 열은 범주형 자료형으로 지정하는 것이 더 적절하다. bill_length_mm, flipper_length_mm, body_mass_g, year 열은 실수형, 정수형 자료형으로 지정하는 것이 적절하다고 판단되어 다음과 같이 .csv 파일을 불러오면서 각 열의 자료형도 함께 지정한다.\n\npenguins_tbl &lt;- read_csv(\"data/file/nine_penguins.csv\",\n         col_types = cols(\n            species = col_factor(level = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")),\n            island = col_character(),\n            bill_length_mm = col_double(),\n            flipper_length_mm = col_double(),\n            body_mass_g = col_double(),\n            sex = col_factor(levels = c(\"female\", \"male\")),\n            year = col_integer()\n          )\n)\n\npenguins_tbl\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island    bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;fct&gt;     &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 Adelie    Biscoe              35                 190        3450 female  2008\n#&gt; 2 Adelie    Torgersen           39.1               181        3750 male    2007\n#&gt; 3 Adelie    Dream               35.6               191        3175 female  2009\n#&gt; 4 Gentoo    Biscoe              46.1               211        4500 female  2007\n#&gt; 5 Gentoo    Biscoe              48.2               221        5100 male    2008\n#&gt; 6 Gentoo    Biscoe              49.8               229        5950 male    2009\n#&gt; 7 Chinstrap Dream               49.8               198        3675 female  2009\n#&gt; 8 Chinstrap Dream               50                 196        3900 male    2007\n#&gt; 9 Chinstrap Dream               45.5               196        3500 female  2008",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#공공-데이터",
    "href": "ingest_file.html#공공-데이터",
    "title": "25  파일 데이터",
    "section": "\n25.6 공공 데이터",
    "text": "25.6 공공 데이터\n공공데이터포털을 비롯한 많은 정부기관에서 제공하는 데이터는 대부분 EUC-KR로 인코딩되어 있다. 이유는 여러가지가 있겠지만 가장 큰 이유는 아마도 엑셀에서 .csv 파일을 열었을 때 한글이 깨지는 민원을 처리하기 위함이 아닐까 싶다. 정형 .csv 파일 형태로 데이터를 받게 되면 먼저 인코딩을 확인해야 한다. readr 패키지의 guess_encoding() 함수를 사용하면 파일의 인코딩을 확인할 수 있다.\n공공데이터포털 인천광역시_정류장별 이용승객 현황 데이터를 다운로드 받아 로컬 파일로 저장한 후 인코딩을 확인한다.\n\nlibrary(readr)\n\nfile_path &lt;- \"data/file/인천광역시_정류장별 이용승객 현황_20220630.csv\"\nguess_encoding(file_path)\n#&gt; # A tibble: 4 × 2\n#&gt;   encoding confidence\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 EUC-KR         1   \n#&gt; 2 GB18030        0.81\n#&gt; 3 Big5           0.48\n#&gt; 4 EUC-JP         0.3\n\n따라서, 이를 바로 read_csv() 함수로 읽을 경우 오류가 발생된다. 왜냐하면 read_csv() 함수는 인코딩을 UTF-8을 기본으로 가정하고 있기 때문이다.\n\nread_csv(file_path)\n#&gt; Error in nchar(x, \"width\"): invalid multibyte string, element 1\n\n따라서, read_csv() 함수를 사용할 때는 locale 인수를 사용하여 인코딩을 지정해주어야 한다. “EUC-KR”로 인코딩을 지정하면 파일을 오류없이 읽을 수 있다.\n\nincheon_bus &lt;- spec(read_csv(file_path, locale = locale(encoding = \"EUC-KR\")))\nincheon_bus |&gt; names() |&gt; dput()\n#&gt; c(\"cols\", \"default\", \"delim\")\n\n데이터 가져오기는 데이터 분석의 첫 단계로, 외부 데이터를 R로 불러오는 과정으로 첫단추가 이후 이어질 분석단계에서 중요한 역할을 한다.\n먼저, 파일 형식에 따라 적절한 함수를 선택해야 한다. 텍스트 파일은 read.csv, read.table 등의 함수를 사용하고, 엑셀 파일은 readxl 패키지의 read_excel 함수를 사용한다. 특히, 인코딩도 이 단계에서 반듯이 확인해야 한다.\n데이터 전처리 단계에서는 구분자와 헤더 유무를 확인하고, 자료형과 칼럼명을 결정해야 한다. 결측값 처리를 위해 na = 옵션을 사용할 수 있고, 필요에 따라 특정 행/열을 선택하는 등의 추가 옵션을 설정할 수 있다.\n전처리 과정을 거쳐 최종적으로 데이터프레임을 생성하게 된다. 다소 번거럽더라도 데이터를 가져오는 단계에서 전처리 과정을 충실히 수행하게 되면 이후 dplyr, tidyr 패키지 등을 활용해 다양한 데이터 조작 및 시각화를 수월하게 할 수 있다.\n\nfile_path &lt;- \"data/file/인천광역시_정류장별 이용승객 현황_20220630.csv\"\n\nincheon_bus &lt;- read_csv(file_path, locale = locale(encoding = \"EUC-KR\"),\n                        skip = 1,\n                        na = c(\"---\", \"\"),\n                        col_names = c(\"정류소명\", \"정류소_id\", \"승차건수_총합계\", \n                                     \"하차건수_총합계\",\"승차건수_카드\", \"하차건수_카드\",\n                                     \"승차건수_현금\", \"일평균_승하차건수\"),\n                       col_types = cols(\n                         정류소명 = col_character(),\n                         정류소_id = col_double(),\n                         승차건수_총합계 = col_double(),\n                         하차건수_총합계 = col_double(),\n                         승차건수_카드 = col_double(),\n                         하차건수_카드 = col_double(),\n                         승차건수_현금 = col_double(),\n                         일평균_승하차건수 = col_double()\n                       ))  \nincheon_bus\n#&gt; # A tibble: 6,386 × 8\n#&gt;    정류소명             정류소_id 승차건수_총합계 하차건수_총합계 승차건수_카드\n#&gt;    &lt;chr&gt;                    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1 (구)국제여객터미널       35051              95            1923            21\n#&gt;  2 (구)국제여객터미널          NA            2512              43          2465\n#&gt;  3 (주)경동세라믹스         89146             341              26           335\n#&gt;  4 (주)경인양행앞           42096             945             923           938\n#&gt;  5 (주)경인양행앞           42097            1322            3536          1294\n#&gt;  6 (주)대한특수금속         39050            1243              89          1238\n#&gt;  7 (주)두남                 39135             147              29           147\n#&gt;  8 (주)세모입구(린나이…     37585            1410            1517          1404\n#&gt;  9 (주)세모입구(린나이…     40893             307             556           304\n#&gt; 10 (주)스킨이데아           89388             147             148           147\n#&gt; # ℹ 6,376 more rows\n#&gt; # ℹ 3 more variables: 하차건수_카드 &lt;dbl&gt;, 승차건수_현금 &lt;dbl&gt;,\n#&gt; #   일평균_승하차건수 &lt;dbl&gt;\n\n지금까지 작업한 전반적인 작업흐름은 그림 25.3 에 대략적으로 나와있다. 공공데이터포털에서 다운로드 받은 인천광역시_정류장별 이용승객 현황_20220630.csv은 EUC-KR로 인코딩 되어 있고 헤더를 갖고 있으며 쉼표로 구분되어 있다. 결측치는 없으나 임의로 --- 으로 정류장 한 곳을 달리 표현하여 na = c(\"---\", \"\")로 결측값 처리를 하였다.\n\n\n\n\n\ngraph LR\n    subgraph \"&lt;strong&gt;파일 형식 결정&lt;/strong&gt;\"\n    A[파일 형식 결정] --&gt; |\"read.csv, read.table 등\"| B[인코딩 확인]\n    A --&gt; |\"readxl::read_excel 등\"| B\n    end\n\n    subgraph \"&lt;strong&gt;데이터 전처리&lt;/strong&gt;\"\n    C[구분자 확인] --&gt; |\"쉼표, 탭 등\"| B\n    D[헤더 유무] --&gt; |\"header = TRUE/FALSE\"| B\n    B --&gt; E[자료형 및&lt;br&gt; 칼럼명 결정]\n    F[결측값 처리] --&gt; |\"na 옵션\"| E\n    G[추가 옵션] --&gt; |\"특정 행/열 선택 등\"| E\n    end\n\nE --&gt; H[데이터프레임&lt;br&gt;생성]\n\nstyle A fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle B fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle C fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle D fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle E fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle F fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle G fill:#c0c0c0,stroke:#333,stroke-width:2px\nstyle H fill:#c0c0c0,stroke:#333,stroke-width:2px\n\n\n\n\n그림 25.3: 인천광역시 정류장별 이용승객 현황 데이터 데이터프레임 가져오는 과정",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#텍스트-바로-읽기",
    "href": "ingest_file.html#텍스트-바로-읽기",
    "title": "24  파일 데이터",
    "section": "\n24.7 텍스트 바로 읽기",
    "text": "24.7 텍스트 바로 읽기\n파일 크기가 적은 경우 즉, 눈으로 식별가능한 크기를 갖는 아스키 파일을 .csv, .txt 등의 형식으로 저장한 후 readr 패키지 read_csv(), read_table(), read_delim() 등의 함수로 불러오는 것이 오히려 적절하지 못한 경우가 있다.\ntibble() 혹은 tribble() 함수를 사용해서 인라인 데이터를 생성하는 것이 더 효율적일 수 있다. 다음과 같이 쇼핑몰 초창기 고객 주문 데이터를 R로 불러와서 분석하는 경우를 살펴보자.\n주문일자,주문번호,고객번호,상품명,상품범주,주문금액\n\"2023-05-19 13:45:32\",203451,A20193,\"슬림핏 반팔 티셔츠\",의류,21800\n2023/05/19 14:23:11,203452,B10582,\"여성용 스니커즈, 240mm\",,68000\n\"2023.05.19 16:05:49\",203453,\"C30281\",\"진공 보온병, 500ml\",주방용품,\"35,600\"\n2023-05-20 09:18:22, 203454,\"D18734\",\"\"귀걸이\"세트 (실버)\",액세서리,112000\n2023-05-20 11:36:58,,E42097,남성용 슬림 진 (32인치),의류,54900\n2023/05/21 08:02:44,\"203,456\",F61052,\"무선 게이밍 마우스\",전자기기,\"\"88,700\"\"\n2023.05.21 15:30:05,203457,,,,42300\n데이터가 크지 않기 때문에 칼럼 혹은 행 기준으로 데이터프레임으로 불러올 수 있다. 먼저 tibble() 함수를 사용해서 데이터프레임을 생성한다. 실무에서 결측값도 있고 주문금액에 천단위 , 구분자도 포함되어 있고 날짜 형식도 다양하게 표현되어 있을 수 있다.\n\nlibrary(tibble)\n\norders &lt;- tribble(\n  ~주문일자, ~주문번호, ~고객번호, ~상품명, ~상품범주, ~주문금액,\n  \"2023-05-19 13:45:32\", \"203451\", \"A20193\", \"슬림핏 반팔 티셔츠\", \"의류\", \"21800\",\n  \"2023/05/19 14:23:11\", \"203452\", \"B10582\", \"여성용 스니커즈, 240mm\", NA, \"68000\",\n  \"2023.05.19 16:05:49\", \"203453\", \"C30281\", \"진공 보온병, 500ml\", \"주방용품\", \"35,600\",\n  \"2023-05-20 09:18:22\", \"203454\", \"D18734\", \"귀걸이세트 (실버)\", \"액세서리\", \"112000\",\n  \"2023-05-20 11:36:58\", NA, \"E42097\", \"남성용 슬림 진 (32인치)\", \"의류\", \"54900\",\n  \"2023/05/21 08:02:44\", \"203,456\", \"F61052\", \"무선 게이밍 마우스\", \"전자기기\", \"88,700\",\n  \"2023.05.21 15:30:05\", \"203457\", NA, NA, NA, \"42300\"\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300\n\ntibble() 함수는 벡터를 기준으로 칼럼을 생성하고 이를 tibble() 함수로 결합하여 데이터프레임을 생성한다.\n\nlibrary(tibble)\n\norders &lt;- tibble(\n 주문일자 = c(\"2023-05-19 13:45:32\", \"2023/05/19 14:23:11\", \"2023.05.19 16:05:49\", \n             \"2023-05-20 09:18:22\", \"2023-05-20 11:36:58\", \"2023/05/21 08:02:44\", \n             \"2023.05.21 15:30:05\"),\n 주문번호 = c(\"203451\", \"203452\", \"203453\", \"203454\", NA, \"203,456\", \"203457\"),\n 고객번호 = c(\"A20193\", \"B10582\", \"C30281\", \"D18734\", \"E42097\", \"F61052\", NA),\n 상품명 = c(\"슬림핏 반팔 티셔츠\", \"여성용 스니커즈, 240mm\", \"진공 보온병, 500ml\", \n           \"귀걸이세트 (실버)\", \"남성용 슬림 진 (32인치)\", \"무선 게이밍 마우스\", NA),\n 상품범주 = c(\"의류\", NA, \"주방용품\", \"액세서리\", \"의류\", \"전자기기\", NA),\n 주문금액 = c(\"21800\", \"68000\", \"35,600\", \"112000\", \"54900\", \"88,700\", \"42300\")\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#footnotes",
    "href": "ingest_file.html#footnotes",
    "title": "25  파일 데이터",
    "section": "",
    "text": "미국정보교환표준부호(American Standard Code for Information Interchange, ASCII)는 영문 알파벳을 사용하는 대표적인 문자 인코딩으로 컴퓨터와 통신 장비를 비롯한 문자를 사용하는 많은 장치에서 사용되며, 대부분의 문자 인코딩이 아스키에 기초하고 있다.↩︎",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "llm_python.html",
    "href": "llm_python.html",
    "title": "\n30  쿼토 파이썬 환경\n",
    "section": "",
    "text": "아나콘다를 설치하고 conda 가상환경을 설정한다. 가상환경 이름은 envs로 설정하고 데이터 과학, 인공지능을 위한 기본 파이썬 패키지도 가상환경 안에 설치한다.\n$ conda create --prefix ./envs python=3.11 numpy seaborn pandas matplotlib scikit-learn transformers\n$ conda activate ./envs\n$ which python\n파이썬(python.exe)를 R 환경에 연결시키기 위해 정확한 경로명을 reticulate::conda_list() 함수로 확인한다.\n\nreticulate::conda_list()\n\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 아래 내용을 추가한다.\nSys.setenv(RETICULATE_PYTHON=\"C:\\\\chatGPT4ds\\\\envs\\\\python.exe\")\n\nlibrary(reticulate)\npy_config()\n\npython:         D:/tcs/chatGPT4ds/envs/python.exe\nlibpython:      D:/tcs/chatGPT4ds/envs/python311.dll\npythonhome:     D:/tcs/chatGPT4ds/envs\nversion:        3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          D:/tcs/chatGPT4ds/envs/Lib/site-packages/numpy\nnumpy_version:  1.26.3\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n31 감성분석\n\nfrom transformers import pipeline\n\nprompt = \"The ambience was good, food was quite good.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n\n[{'label': '4 stars', 'score': 0.5752392411231995}]",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>쿼토 파이썬 환경</span>"
    ]
  },
  {
    "objectID": "local_llm.html",
    "href": "local_llm.html",
    "title": "31  오라마 설치",
    "section": "",
    "text": "Ollama 설치\nstatkclee@dl:/mnt/d/tcs/chatGPT4ds/llm$ curl https://ollama.ai/install.sh | sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&gt;&gt;&gt; Downloading ollama...\n100  8422    0  8422    0     0  18348      0 --:--:-- --:--:-- --:--:-- 18348\n######################################################################## 100.0%##O=#  #                                 ######################################################################## 100.0%\n&gt;&gt;&gt; Installing ollama to /usr/local/bin...\n&gt;&gt;&gt; Adding ollama user to render group...\n&gt;&gt;&gt; Adding current user to ollama group...\n&gt;&gt;&gt; Creating ollama systemd service...\n&gt;&gt;&gt; NVIDIA GPU installed.\n&gt;&gt;&gt; The Ollama API is now available at 0.0.0.0:11434.\n&gt;&gt;&gt; Install complete. Run \"ollama\" from the command line.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>오라마 설치</span>"
    ]
  },
  {
    "objectID": "lang_gpt.html",
    "href": "lang_gpt.html",
    "title": "32  챗GPT 자연어",
    "section": "",
    "text": "33 챗GPT 시대 데이터 분석\n\nOpenAI 챗GPT Code Interpreter 플러그인\n노터블(Notable): EDA & ETL Made Easy (SQL, Python, & R)\n오픈소스 GPT-Code UI\nR\n\nRTutor.ai, GitHub 저장소\nhttps://chatlize.ai/\n\n\n\n\n34 Code Interpreter\n\n1단계2단계3단계4단계5단계 (데이터+프롬프트)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n35 Notable.ai\n\n\n36 심슨 패러독스\n\n챗GPT Code Interpreter : 채팅 이력\nJupyter Notebook 다운로드: penguin_analysis.ipynb\npenguin_analysis.ipynb → penguin_analysis.qmd\n\n명령어: $ quarto convert penguin_analysis.ipynb\n\n쿼토 컴파일: 바로가기",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>챗GPT 자연어</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Abu-Mostafa, Yaser S, Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012.\nLearning from Data. Vol. 4. AMLBook New York.\n\n\nBecker, Richard. 2018. The New s Language. CRC Press.\n\n\nCaffo, Brian. 2015. Advanced Linear Models for Data Science.\nLeanpub.\n\n\nChambers, J. M., and T. J. Hastie. 1992. Statistical Models in\ns. London: Chapman & Hall.\n\n\nDibia, Victor. 2023. “LIDA: A Tool for Automatic\nGeneration of Grammar-Agnostic Visualizations and Infographics Using\nLarge Language Models.” In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 3:\nSystem Demonstrations), edited by Danushka Bollegala, Ruihong\nHuang, and Alan Ritter, 113–26. Toronto, Canada: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2023.acl-demo.11.\n\n\nFriendly, Michael. 2023. HistData: Data Sets from the History of\nStatistics and Data Visualization.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial\nData Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian\nBriese, and Markus Neteler. 2016. “OpenEO: A GDAL for Earth\nObservation Analytics.” 2016. https://r-spatial.org/2016/11/29/openeo.html.\n\n\nStack\", \"Enigma of the. 2023. “The Future of Data Analysis: 10\nChatGPT Prompts You Should Start Using Today.”\nMedium.com, December. https://medium.com/ai-in-plain-english/the-future-of-data-analysis-10-chatgpt-prompts-you-should-start-using-today-39734b701e43.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data\nAnalysis.” Journal of Statistical Software 40: 1–29.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법.”\n프롭빅스(PROPBIX), no. 13 (September). http://www.kahps.org/.",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "ingest_file.html#데이터-입력방식",
    "href": "ingest_file.html#데이터-입력방식",
    "title": "25  파일 데이터",
    "section": "\n25.3 데이터 입력방식",
    "text": "25.3 데이터 입력방식\n파일 크기가 적은 경우 즉, 눈으로 식별가능한 크기를 갖는 아스키 파일을 .csv, .txt 등의 형식으로 저장한 후 readr 패키지 read_csv(), read_table(), read_delim() 등의 함수로 불러오는 것이 오히려 적절하지 못한 경우가 있다.\ntibble() 혹은 tribble() 함수를 사용해서 인라인 데이터를 생성하는 것이 더 효율적일 수 있다. 다음과 같이 쇼핑몰 초창기 고객 주문 데이터를 입력하여 R로 불러와서 분석하는 방법를 살펴보자.\n주문일자,주문번호,고객번호,상품명,상품범주,주문금액\n\"2023-05-19 13:45:32\",203451,A20193,\"슬림핏 반팔 티셔츠\",의류,21800\n2023/05/19 14:23:11,203452,B10582,\"여성용 스니커즈, 240mm\",,68000\n\"2023.05.19 16:05:49\",203453,\"C30281\",\"진공 보온병, 500ml\",주방용품,\"35,600\"\n2023-05-20 09:18:22, 203454,\"D18734\",\"\"귀걸이\"세트 (실버)\",액세서리,112000\n2023-05-20 11:36:58,,E42097,남성용 슬림 진 (32인치),의류,54900\n2023/05/21 08:02:44,\"203,456\",F61052,\"무선 게이밍 마우스\",전자기기,\"\"88,700\"\"\n2023.05.21 15:30:05,203457,,,,42300\n데이터가 크지 않기 때문에 칼럼 혹은 행 기준으로 데이터프레임으로 불러올 수 있다. 먼저 tibble() 함수를 사용해서 데이터프레임을 생성한다. 실무에서 결측값도 있고 주문금액에 천단위 , 구분자도 포함되어 있고 날짜 형식도 다양하게 표현되어 있을 수 있다.\n\nlibrary(tibble)\n\norders &lt;- tribble(\n  ~주문일자, ~주문번호, ~고객번호, ~상품명, ~상품범주, ~주문금액,\n  \"2023-05-19 13:45:32\", \"203451\", \"A20193\", \"슬림핏 반팔 티셔츠\", \"의류\", \"21800\",\n  \"2023/05/19 14:23:11\", \"203452\", \"B10582\", \"여성용 스니커즈, 240mm\", NA, \"68000\",\n  \"2023.05.19 16:05:49\", \"203453\", \"C30281\", \"진공 보온병, 500ml\", \"주방용품\", \"35,600\",\n  \"2023-05-20 09:18:22\", \"203454\", \"D18734\", \"귀걸이세트 (실버)\", \"액세서리\", \"112000\",\n  \"2023-05-20 11:36:58\", NA, \"E42097\", \"남성용 슬림 진 (32인치)\", \"의류\", \"54900\",\n  \"2023/05/21 08:02:44\", \"203,456\", \"F61052\", \"무선 게이밍 마우스\", \"전자기기\", \"88,700\",\n  \"2023.05.21 15:30:05\", \"203457\", NA, NA, NA, \"42300\"\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300\n\ntibble() 함수는 벡터를 기준으로 칼럼을 생성하고 이를 tibble() 함수로 결합하여 데이터프레임을 생성한다.\n\nlibrary(tibble)\n\norders &lt;- tibble(\n 주문일자 = c(\"2023-05-19 13:45:32\", \"2023/05/19 14:23:11\", \"2023.05.19 16:05:49\", \n             \"2023-05-20 09:18:22\", \"2023-05-20 11:36:58\", \"2023/05/21 08:02:44\", \n             \"2023.05.21 15:30:05\"),\n 주문번호 = c(\"203451\", \"203452\", \"203453\", \"203454\", NA, \"203,456\", \"203457\"),\n 고객번호 = c(\"A20193\", \"B10582\", \"C30281\", \"D18734\", \"E42097\", \"F61052\", NA),\n 상품명 = c(\"슬림핏 반팔 티셔츠\", \"여성용 스니커즈, 240mm\", \"진공 보온병, 500ml\", \n           \"귀걸이세트 (실버)\", \"남성용 슬림 진 (32인치)\", \"무선 게이밍 마우스\", NA),\n 상품범주 = c(\"의류\", NA, \"주방용품\", \"액세서리\", \"의류\", \"전자기기\", NA),\n 주문금액 = c(\"21800\", \"68000\", \"35,600\", \"112000\", \"54900\", \"88,700\", \"42300\")\n)\n\norders\n#&gt; # A tibble: 7 × 6\n#&gt;   주문일자            주문번호 고객번호 상품명                 상품범주 주문금액\n#&gt;   &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 2023-05-19 13:45:32 203451   A20193   슬림핏 반팔 티셔츠     의류     21800   \n#&gt; 2 2023/05/19 14:23:11 203452   B10582   여성용 스니커즈, 240mm &lt;NA&gt;     68000   \n#&gt; 3 2023.05.19 16:05:49 203453   C30281   진공 보온병, 500ml     주방용품 35,600  \n#&gt; 4 2023-05-20 09:18:22 203454   D18734   귀걸이세트 (실버)      액세서리 112000  \n#&gt; 5 2023-05-20 11:36:58 &lt;NA&gt;     E42097   남성용 슬림 진 (32인…  의류     54900   \n#&gt; 6 2023/05/21 08:02:44 203,456  F61052   무선 게이밍 마우스     전자기기 88,700  \n#&gt; 7 2023.05.21 15:30:05 203457   &lt;NA&gt;     &lt;NA&gt;                   &lt;NA&gt;     42300\n\ntribble(), tibble() 함수 모두 데이터 입력을 통해 orders 데이터프레임을 생성하였으나 결측값에 대한 처리와 자료형이 모두 문자형(&lt;chr&gt;)으로 되어 있어 후속작업을 위해 추가 데이터 정제 작업이 필수적이다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#다양한-파일형태",
    "href": "ingest_file.html#다양한-파일형태",
    "title": "25  파일 데이터",
    "section": "\n25.5 다양한 파일형태",
    "text": "25.5 다양한 파일형태\n\n25.5.1 파일 저장\n펭귄 데이터에서 종별로 3마리를 무작위로 추출해서 nine_penguins 데이터프레임을 만든 후에 다양한 형식 아스키 파일로 저장한다. 펭귄 9마리 데이터프레임으로 아스크 파일 형식으로 저장된 다양한 형태(탭 구분자, 콤마 구분자, 고정길이) 데이터를 불러오는 방법을 살펴본다. 구분자로 탭과 콤마가 가장 많이 사용되지만 경우에 따라서는 “;”, “:”, “|” 등 다양한 구분자를 사용할 수 있다.\n\nlibrary(palmerpenguins)\n\nnine_penguins &lt;- palmerpenguins::penguins |&gt; \n    drop_na() |&gt; \n    slice_sample(n = 3, replace = FALSE, by = species) |&gt; \n    select(-bill_depth_mm)\n\n탭 구분자\nwrite_delim() 함수에 delim 인자를 탭으로 명시하여 탭 구분자 아스키 파일로 저장하는 방법과 write_tsv() 함수를 사용하는 방법이 있다. 탭 구분자 파일로 저장하는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins |&gt; \n    # write_tsv(\"data/file/ASCII/nine_penguins.tsv\") |&gt; \n    write_delim(\"data/file/nine_penguins.txt\", delim = \"\\t\") \n\nspecies\tisland\tbill_length_mm\tflipper_length_mm\tbody_mass_g\tsex\tyear\nAdelie\tBiscoe\t38.6\t199\t3750\tfemale\t2009\nAdelie\tBiscoe\t40.5\t180\t3950\tmale\t2007\nAdelie\tDream\t37\t185\t3000\tfemale\t2007\nGentoo\tBiscoe\t54.3\t231\t5650\tmale\t2008\nGentoo\tBiscoe\t59.6\t230\t6050\tmale\t2007\nGentoo\tBiscoe\t44.5\t214\t4850\tfemale\t2009\nChinstrap\tDream\t49.3\t203\t4050\tmale\t2009\nChinstrap\tDream\t46.1\t178\t3250\tfemale\t2007\nChinstrap\tDream\t50.8\t210\t4100\tmale\t2009\nCSV 구분자\nCSV(Comma-Separated Values) 파일은 콤마 구분자를 사용하여 데이터를 저장하는 형식으로 모든 운영체제에서 특별한 별도 프로그램없이 열어볼 수 있다는 장점이 있어 호환성에서 큰 장점이 있지만 파일에 많은 정보가 담기게 되면 파일크기가 커져서 저장공간을 많이 차지한다는 단점이 있다. write_csv() 함수를 사용하여 콤마 구분자 아스키 파일로 저장하는 방법과 write_delim() 함수를 사용하는 방법이 있다. 콤마 구분자 파일로 저장하는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins |&gt; \n    write_csv(\"data/file/nine_penguins.csv\")\n\nspecies,island,bill_length_mm,flipper_length_mm,body_mass_g,sex,year\nAdelie,Biscoe,38.6,199,3750,female,2009\nAdelie,Biscoe,40.5,180,3950,male,2007\nAdelie,Dream,37,185,3000,female,2007\nGentoo,Biscoe,54.3,231,5650,male,2008\nGentoo,Biscoe,59.6,230,6050,male,2007\nGentoo,Biscoe,44.5,214,4850,female,2009\nChinstrap,Dream,49.3,203,4050,male,2009\nChinstrap,Dream,46.1,178,3250,female,2007\nChinstrap,Dream,50.8,210,4100,male,2009\n고정길이 파일\n고정길이 아스키 파일(Fixed-width ASCII file, FWF)은 데이터 저장 및 교환을 위해 초기 컴퓨팅 시대에 개발되었다. 당시에는 데이터 저장 공간이 제한적이었기 때문에 고정길이 파일은 구분자를 사용하지 않고 데이터를 더 촘촘하게 저장할 수 있었고, 하드웨어와 소프트웨어도 고정 길이 레코드 처리에 최적화되어 있었다.\n현재까지도 고정길이 파일은 레거시 시스템과의 호환성, 데이터 무결성 유지, 데이터 밀도 향상, 대용량 데이터 처리 성능 개선 등의 이유로 명맥을 유지하고 있으며, 의료 및 금융 분야에서 고정길이 파일을 데이터 교환 표준으로 활용하기도 한다.\n하지만, 고정길이 파일은 파일 구조를 이해하기 위해 별도 문서나 스키마 정의가 필요하고, 데이터 추가나 수정 시 레코드 길이 조정이 요구되는 단점이 크고, 구분자로 구분되는 구조화된 데이터 형식과 비교하면 사용 편의성이 크게 떨어진다.\nAdelie    Dream     37.6          181            3300     female   2007\nAdelie    Biscoe    35.3          187            3800     female   2007\nAdelie    Biscoe    37.8          174            3400     female   2007\nGentoo    Biscoe    47.4          212            4725     female   2009\nGentoo    Biscoe    49.1          220            5150     female   2008\nGentoo    Biscoe    47.5          209            4600     female   2008\nChinstrap Dream     40.9          187            3200     female   2008\nChinstrap Dream     47.6          195            3850     female   2008\nChinstrap Dream     46            195            4150     female   2007\n\n25.5.2 불러오기\n탭 구분자\nread_delim() 함수에 delim 인자를 탭으로 명시하여 탭 구분자 아스키 파일을 불러오는 방법과 read_tsv()` 함수를 사용하는 방법이 있다. 탭 구분자 파일을 불러오는 동일한 기능을 수행하지만 함수명에서 차이가 난다.\n\nnine_penguins &lt;- \n    # read_tsv(\"data/file/ASCII/nine_penguins.tsv\") |&gt; \n    read_delim(\"data/file/nine_penguins.txt\", delim = \"\\t\") \n\nCSV 구분자\n고정길이 파일\nreadr 패키지 read_fwf() 함수를 사용하여 고정길이 파일을 불러읽어오는 방식에서 fwf_widths 인자로 각 열의 길이를 지정하고 col_names 인자로 열 이름을 지정한다.\n\nnine_penguins_fwf &lt;-read_fwf(\"data/file/nine_penguins.fwf\",\n                             skip = 0,\n         col_positions = fwf_widths(c(10, 10, 14, 15, 9, 9, 5),\n           col_names = c(\"species\", \"island\", \"bill_length_mm\",\n                         \"flipper_length_mm\", \"body_mass_g\", \"sex\", \"year\")))\n\nnine_penguins_fwf\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Adelie    Dream            37.6               181        3300 female  2007\n#&gt; 2 Adelie    Biscoe           35.3               187        3800 female  2007\n#&gt; 3 Adelie    Biscoe           37.8               174        3400 female  2007\n#&gt; 4 Gentoo    Biscoe           47.4               212        4725 female  2009\n#&gt; 5 Gentoo    Biscoe           49.1               220        5150 female  2008\n#&gt; 6 Gentoo    Biscoe           47.5               209        4600 female  2008\n#&gt; 7 Chinstrap Dream            40.9               187        3200 female  2008\n#&gt; 8 Chinstrap Dream            47.6               195        3850 female  2008\n#&gt; 9 Chinstrap Dream            46                 195        4150 female  2007",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_file.html#다수-파일",
    "href": "ingest_file.html#다수-파일",
    "title": "25  파일 데이터",
    "section": "\n25.7 다수 파일",
    "text": "25.7 다수 파일\n다수 파일을 불러오는 경우를 상정하기 위해서 먼저 앞서 준비한 nine_penguins 데이터프레임을 재사용한다. split() 함수로 species 열을 기준으로 nine_penguins를 분할하여 분할된 데이터를 리스트 형태로 penguins_split에 저장한다. here() 함수로 data/file/ 폴더의 경로를 data_folder 변수에 저장한다. walk2() 함수로 penguins_split 리스트의 각 요소와 해당 요소의 이름을 순회하면서 write_csv() 함수를 사용해 각 분할된 데이터프레임을 CSV 파일로 저장한다. 코드 실행 결과, 프로젝트 디렉토리 내의 data/file/ 폴더에 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일이 생성되며, 각 파일에는 해당 펭귄 종 3마리 관측점 데이터가 저장된다.\n\n# species로 데이터 분할\npenguins_split &lt;- split(nine_penguins, nine_penguins$species)\n\n# 분할된 데이터를 CSV 파일로 저장\ndata_folder &lt;- here::here(\"data\", \"file\")\nwalk2(penguins_split, str_glue(\"penguin_{names(penguins_split)}\"), ~ write_csv(.x, here::here(data_folder, str_c(.y, \".csv\"))))\n\nlist.files() 함수로 data/file 폴더에 penguin으로 시작하는 .csv 파일 3개를 확인할 수 있다.\n\nlist.files(data_folder, pattern = \"^penguin\")\n#&gt; [1] \"penguin_Adelie.csv\"    \"penguin_Chinstrap.csv\" \"penguin_Gentoo.csv\"\n\n이제 데이터가 준비되었으니 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일을 읽어와서 하나의 데이터프레임으로 만들어보자. 동일한 자료구조를 갖는 아스키 파일은 시도, 시군구 데이터처럼 공간적으로 관리를 위해 구분되거나 일, 월, 분기, 년 처럼 시점을 달리하는 경우 관리 목적으로 구분되어 흔히 접하게 되는 데이터다.\ndata_folder 변수에 CSV 파일들이 저장된 폴더 경로를 지정한다. list.files() 함수로 data_folder 내의 모든 CSV 파일 경로를 csv_files 변수에 저장한다. map_df() 함수로 csv_files의 각 파일 경로에 대해 read_csv() 함수를 적용하여 CSV 파일을 읽어와 읽어온 데이터프레임으로 결합한다. 코드 실행 결과, data/file/ 폴더에 있는 penguin_Adelie.csv, penguin_Gentoo.csv, penguin_Chinstrap.csv 파일을 읽어와서 하나의 데이터프레임으로 결합한 penguins_tbl이 생성되며, 총 9 마리 펭귄 3종의 데이터가 포함되어 있다.\n\n# CSV 파일 경로 지정\ndata_folder &lt;- \"data/file/\"\ncsv_files &lt;- list.files(data_folder, pattern = \"^penguin\", full.names = TRUE)\n\n# CSV 파일들을 읽어와 데이터프레임 결합\npenguins_tbl &lt;- purrr::map_df(csv_files, read_csv)\n\npenguins_tbl\n#&gt; # A tibble: 9 × 7\n#&gt;   species   island bill_length_mm flipper_length_mm body_mass_g sex     year\n#&gt;   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Adelie    Biscoe           38.6               199        3750 female  2009\n#&gt; 2 Adelie    Biscoe           40.5               180        3950 male    2007\n#&gt; 3 Adelie    Dream            37                 185        3000 female  2007\n#&gt; 4 Chinstrap Dream            49.3               203        4050 male    2009\n#&gt; 5 Chinstrap Dream            46.1               178        3250 female  2007\n#&gt; 6 Chinstrap Dream            50.8               210        4100 male    2009\n#&gt; 7 Gentoo    Biscoe           54.3               231        5650 male    2008\n#&gt; 8 Gentoo    Biscoe           59.6               230        6050 male    2007\n#&gt; 9 Gentoo    Biscoe           44.5               214        4850 female  2009",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>파일 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html",
    "href": "ingest_spreadsheet.html",
    "title": "26  상용 데이터",
    "section": "",
    "text": "26.1 통계패키지\nSPSS, SAS, STATA는 널리 사용되는 통계 분석 소프트웨어 패키지로, 각각 고유한 파일 형식을 사용한다. 고유한 파일 형식을 갖게 되면 데이터가 통계 패키지 내부에서 원활히 동작할 수 있는 메타 정보를 담을 수 있고 속도 향상도 기대할 수 있다. 그러나 이러한 독점적인 파일 형식은 다른 통계 패키지와의 상호 운용성을 제한할 수 있고, 장기적으로 데이터 보존 및 이식성에 문제를 일으킬 수 있다.\nSPSS는 .sav 확장자를 사용하는 이진 파일 형식을 사용한다. .sav 파일은 데이터, 변수 레이블, 값 레이블 등의 메타데이터를 포함하고 있다. SPSS .por 확장자를 가진 파일은 다른 시스템으로 이식도 가능하다.\nSAS는 .sas7bdat 확장자를 사용하는 이진 파일 형식을 사용한다. .sas7bdat 파일은 데이터와 메타데이터를 모두 포함하며, SAS에서만 읽을 수 있다. SAS도 SPSS .por처럼 .xpt 확장자를 가진 다른 시스템에 이식 가능한 파일 형식도 지원한다.\nSTATA는 .dta 확장자를 사용하는 이진 파일 형식을 사용한다. .dta 파일에는 데이터, 변수 레이블, 값 레이블 등 메타데이터가 포함되어 있다. .dta 파일은 STATA에서만 읽을 수 있고 SAS, SPSS에서 읽을 수는 없다 하지만, ‘SAS STATA Transfer’ 프로시저를 ’SPSS Data Access Pack’을 구매하여 STATA 파일을 불러읽을 수 있으며, STATA에서 CSV 파일 형태로 내보낸 후 별도 프로시저나 팩없이 SPSS, SAS에서 불러읽을 수 있는 방법이 있다.\n하지만, 통계 패키지 간에 데이터를 교환하려면 일반적으로 .csv(쉼표로 분리된 값) 또는 .txt(탭으로 분리된 값) 형식과 같은 중간 파일 형식을 사용하는 과정에서 변수 레이블과 값 레이블과 같은 일부 메타데이터가 손실될 수 있다.\n따라서, 단기적으로 SAS/SPSS/STATA와 같은 독점 파일 형식이 제공하는 장점보다 개방형 파일 형식이 장기적으로 데이터 접근성과 재사용성을 높일 수 있다는 면에서 장점이 크다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html#통계패키지",
    "href": "ingest_spreadsheet.html#통계패키지",
    "title": "26  상용 데이터",
    "section": "",
    "text": "26.1.1 SPSS\n세종시에 위치한 한국보건사회연구원에서 조사하여 발표하는 한국복지패널데이터는 특이하게도 오픈 파일 형식만 제외하고 상용 통계 패키지가 있어야 열어볼 수 있는 SPSS, STATA, SAS 파일 형식으로 제공되고 있다. 총4가지 종류 파일을 제공하고 있지만 여기서는 다양한 파일 데이터를 불러오는 방법을 중심으로 살펴보기 때문에 가장 단순한 파일만 R 환경으로 불러오는 방법을 살펴보자.\n\n가구용데이터(SAS, SPSS, STATA):koweps_h17_2022_Beta1\n가구원용데이터(SAS, SPSS, STATA):koweps_p17_2022_Beta1\n복지인식설문용데이터(SAS, SPSS, STATA):koweps_wc17_2022_Beta1\n가구용, 가구원용, 복지인식설문용 머지데이터(SAS, SPSS, STATA):koweps_hpwc17_2022_Beta1\n\nSPSS 로 작성된 .sav 파일으로 R 환경으로 불러오기 위해서는 haven 패키지를 로드하여 SPSS (.sav) 데이터 파일을 R로 읽어온다. read_spss() 함수를 사용하여 “koweps_hpwc17_2022_Beta1.sav” 파일을 welfare_raw 데이터 프레임으로 저장한 후, map_chr() 함수를 사용하여 welfare_raw의 각 변수에 대해 attributes(.x)$label을 적용하여 변수의 레이블을 추출하고 후속 작업을 위해서 문자형 벡터로 변환시킨다.\nenframe() 함수를 사용하여 추출된 레이블을 데이터 프레임으로 변환하고, filter() 함수와 str_detect() 함수를 사용하여 “성별”, “종교”, “태어난 연도”, “혼인상태”, “가구원수”라는 키워드가 포함된 변수만 선택한다. pull() 함수를 사용하여 선택된 변수의 이름을 추출하고, setdiff() 함수를 사용하여 정규표현식 작성과정에서 함께 추출된”h1707_6aq6” 변수를 제외시킨 후 demo_vars 변수로 저장한다.\nwelfare_raw 데이터 프레임에서 select() 함수와 all_of() 함수를 사용하여 demo_vars에 해당하는 변수만 선택한 후 set_names() 함수를 사용하여 선택된 변수명을 “성별”, “종교”, “태어난 연도”, “혼인상태”, “가구원수”로 변경한다. str_split()과 dput()을 사용하여 변수 이름을 파이프(|)로 연산으로 한 명령어로 처리한다. janitor 패키지 clean_names() 함수를 사용하여 변수 이름을 깔끔하게 정리하는데, ascii = FALSE 옵션을 사용하여 한글 변수명을 유지한다.\n한국보건사회연구원에서 한국복지패널 데이터가 SPSS로 제공되고 있지만 상용 SPSS 패키지가 없더라도 R 환경에서 haven 패키지와 janitor 패키지를 활용하여 SPSS 데이터를 불러와서 본격적인 분석을 오픈 데이터 분석 및 통계 언어 R로 수행할 준비가 되었다.\n\nlibrary(haven) # install.packages(\"foreign\")\n\n# Read the .sav file\nwelfare_raw &lt;- read_spss(\"data/file/SPSS/koweps_hpwc17_2022_Beta1.sav\")\n\n## 관심 변수 추출\ndemo_vars &lt;- welfare_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt; \n    enframe() |&gt; \n    filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt; \n    pull(name) |&gt; \n    setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nwelfare_raw %&gt;%\n  select(all_of(demo_vars)) |&gt; \n    set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt; \n    janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows\n\n\n26.1.2 SAS\nSAS 통계패키지 koweps_hpwc17_2022_beta1.sas7bdat 파일을 작성된 동일한 한국보건사회연구원에서 한국복지패널 데이터도 haven 패키지를 사용하여 read_sas() 함수를 사용하여 SAS 데이터 파일(.sas7bdat)을 불러온다. 이후 코드는 앞서 SPSS 데이터를 R 인구통계 데이터프레임으로 변환시켜 가져온 것과 동일한 방법으로 진행된다. 즉, 코드를 재사용하게 된다.\n\nlibrary(haven) # install.packages(\"foreign\")\n\nsas_raw &lt;- read_sas(\"data/file/SAS/koweps_hpwc17_2022_Beta1.sas7bdat\")\n\n## 관심 변수 추출\nsas_vars &lt;- sas_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt;\n  enframe() |&gt;\n  filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt;\n  pull(name) |&gt;\n  setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nsas_raw %&gt;%\n  select(all_of(sas_vars)) |&gt;\n  set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt;\n  janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows\n\n\n26.1.3 STATA\nSTATA 통계패키지 koweps_hpwc17_2022_beta1.dta 파일은 SAS 버전과 동일한 한국복지패널 데이터다. R에서 haven 패키지 read_dta() 함수를 사용하여 STATA 데이터 파일(.dta)을 불러올 수 있다. 이후 코드는 앞서 SPSS, SAS 데이터를 R로 가져와 인구통계 데이터프레임으로 변환한 것과 동일한 방법으로 진행된다. 따라서 이전에 작성한 코드를 그대로 재사용할 수 있다.\n\nlibrary(haven) # install.packages(\"haven\")\n\n# STATA 파일 불러오기\nstata_raw &lt;- read_dta(\"data/file/STATA/Koweps_hpwc17_2022_beta1.dta\")\n\n## 관심 변수 추출\nstata_vars &lt;- stata_raw %&gt;%\n  map_chr(~attributes(.x)$label) |&gt;\n  enframe() |&gt;\n  filter(str_detect(value, \"성별|종교|(태어난 연도)|혼인상태|가구원수\")) |&gt;\n  pull(name) |&gt;\n  setdiff(\"h1707_6aq6\") # 기타소비-종교관련비 변수 제거\n\n## 인구통계학적 변수로 구성된 데이터셋\nstata_raw %&gt;%\n  select(all_of(stata_vars)) |&gt;\n  set_names(str_split(\"성별|종교|(태어난 연도)|혼인상태|가구원수\", \"\\\\|\")[[1]] |&gt; dput()) |&gt;\n  janitor::clean_names(ascii = FALSE)\n#&gt; c(\"성별\", \"종교\", \"(태어난 연도)\", \"혼인상태\", \"가구원수\"\n#&gt; )\n#&gt; # A tibble: 16,591 × 5\n#&gt;     성별  종교 태어난_연도 혼인상태 가구원수\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1     2        1945        2        1\n#&gt;  2     1     1        1948        2        2\n#&gt;  3     1     1        1942        3        1\n#&gt;  4     5     1        1962        1        1\n#&gt;  5     5     2        1963        1        1\n#&gt;  6     5     2        2003        5        1\n#&gt;  7     5     1        1927        1        1\n#&gt;  8     5     2        1934        1        1\n#&gt;  9     1     2        1940        2        1\n#&gt; 10     2     2        1970        3        1\n#&gt; # ℹ 16,581 more rows",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html#엑셀",
    "href": "ingest_spreadsheet.html#엑셀",
    "title": "26  상용 데이터",
    "section": "\n26.2 엑셀",
    "text": "26.2 엑셀\n엑셀에서 복잡한 데이터를 관리하는 방법 중 하나는 여러 시트에 데이터를 나누어 저장하는 것이다. readxl 패키지를 사용하여 엑셀 파일을 불러올 때 excel_sheets() 함수를 사용하여 엑셀 파일에 저장된 시트 이름을 확인할 수 있다. 이후 read_excel() 함수를 사용하여 원하는 시트를 불러올 수 있다. 제20대 국회의원 선거 지역구와 비례대표 당선자 명단을 담은 엑셀 파일을 불러오는 예제를 살펴보자. 국회의원 정수는 총 300 명이기 때문에 지역구 당선자와 비례 당선자를 하나로 묶어 각 당별로 당선자가 몇명인지 확인할 수 있다.\n\n\n\n\n\n그림 26.2: 제20대 국회의원 선거 당선자 명단\n\n\nreadxl 패키지의 excel_sheets() 함수를 사용하여 제20대와 제21대 국회의원선거 당선인 정보가 담긴 엑셀 파일 “data/국회의원_당선인명부.xlsx”에서 시트명을 추출하여 winners_sheetname 변수에 저장한다. read_excel() 함수를 사용하여 지역구 당선자 명단은 winners_sheetname의 첫 번째 시트에서, 비례대표 당선자 명단은 두 번째 시트에서 추출하여 각각 winners_precinct와 winners_prop 변수에 저장한다. bind_rows() 함수를 사용하여 지역구와 비례대표 당선자 명단을 하나로 합치고, 비례대표 당선자의 선거구명을 “비례대표”로 설정하여 지역구와 자료구조를 맞춘 후 winners 데이터프레임에 저장한다. count() 함수를 사용하여 소속정당명별 당선자수를 집계하고, 당선자수 순으로 정렬한다.\n\nlibrary(tidyverse)\nlibrary(readxl) \n\n# 엑셀 파일에서 시트명 추출\nwinners_sheetname &lt;- readxl::excel_sheets(\"data/국회의원_당선인명부.xlsx\")\nwinners_sheetname\n#&gt; [1] \"지역구20\"   \"비례대표20\" \"지역구21\"   \"비례대표21\"\n\n# 지역구와 비례 당선자 명단 추출\nwinners_precinct &lt;- readxl::read_excel(\"data/국회의원_당선인명부.xlsx\", sheet = winners_sheetname[1])\nwinners_prop &lt;- readxl::read_excel(\"data/국회의원_당선인명부.xlsx\", sheet = winners_sheetname[2])\n\n# 지역구와 비례대표 당선자 명단을 하나로 합침\nwinners &lt;- bind_rows(winners_precinct, winners_prop |&gt; \n                       mutate(선거구명 = \"비례대표\"))\n\n# 정당별 당선자수 집계\nwinners |&gt; \n  count(소속정당명, sort = TRUE, name = \"당선자수\")\n#&gt; # A tibble: 5 × 2\n#&gt;   소속정당명   당선자수\n#&gt;   &lt;chr&gt;           &lt;int&gt;\n#&gt; 1 더불어민주당      123\n#&gt; 2 새누리당          122\n#&gt; 3 국민의당           38\n#&gt; 4 무소속             11\n#&gt; 5 정의당              6",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_spreadsheet.html#구글시트",
    "href": "ingest_spreadsheet.html#구글시트",
    "title": "26  상용 데이터",
    "section": "\n26.3 구글시트",
    "text": "26.3 구글시트\n스프레드시트는 데이터 입력, 저장, 분석 및 시각화를 위해 널리 사용되는 소프트웨어 도구다. 하지만, 데이터 입력과 저장 측면에 초점을 맞추어 보면 오류도 많다. 흔히 보게 되는 엑셀 파일의 문제점 중 하나는 엉망진창(messy) 데이터다. 논문(Broman 와/과 Woo 2018)에 제시된 엉망진창 데이터 중 하나를 뽑아 깔끔한 데이터로 변환해보자.\n\n\n\n\n\n그림 26.3: 엉망진창(messy) 데이터를 깔끔한(tidy) 데이터 변환\n\n\n논문 데이터를 OCR 과정을 거친 후에 누구나 실습할 수 있도록 구글시트 messy_data로 공유되어 있다. 구글시트는 엑셀과 유사한 스프레드시트 프로그램이다. 구글시트에 저장된 데이터를 불러오기 위해서는 googlesheets4 패키지를 사용한다. googlesheets4 패키지를 사용하여 구글 시트의 엉망진창(messy) 데이터를 불러온다. read_sheet() 함수로 데이터를 읽어오고, set_names() 함수로 열 이름을 설정한다. pivot_longer() 함수를 사용하여 엉망진창 데이터를 깔끔한(tidy) 데이터로 변환한다. starts_with(\"week\") 열을 선택하고, names_sep 인수로 “_”를 기준으로 열 이름을 분리한다. names_to와 values_to 인수를 사용하여 새로운 열 이름과 값을 지정한다. select(), rename(), mutate() 함수를 사용하여 필요한 열을 선택하고 이름을 변경한다. 최종적으로 깔끔한 데이터 형태로 변환된 데이터프레임을 얻는다.\n\nlibrary(googlesheets4)\n\n# messy 데이터 불러오기\nsheet_url &lt;- \"https://docs.google.com/spreadsheets/d/19L3ASfmWTCd1YikWHIHeWKdK29nXz-hhwktFU8qLAYc/\"\n\nmessy_import &lt;- read_sheet(sheet_url, skip = 1, \n                         col_types = \"ccccccccccc\" )\nmessy_data &lt;- messy_import |&gt; \n  set_names(c(\"Mouse_ID\", \"SEX\", \"week_4_date\", \"week_4_weight\", \n              \"week_4_glucose\", \"week_6_date\", \"week_6_weight\", \n              \"week_6_glucose\", \"week_8_date\", \"week_8_weight\", \n              \"week_8_glucose\"))\n\n# messy 데이터를 tidy 데이터로 변환\ntidy_data &lt;- messy_data %&gt;%\n  mutate(across(everything(), as.character)) |&gt; \n  pivot_longer(cols = starts_with(\"week\"),\n               names_to = c(\"week\", \"type\", \".value\"),\n               values_to = c(\"date\", \"glucose\", \"weight\"),\n               names_sep = \"_\")  |&gt; \n  select(-week) |&gt; \n  rename(week = type) |&gt; \n  mutate(mouse_id = Mouse_ID,\n         sex = SEX) %&gt;%\n  select(mouse_id, sex, week, date, glucose, weight)\n\ntidy_data\n#&gt; # A tibble: 15 × 6\n#&gt;    mouse_id sex   week  date       glucose weight\n#&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; \n#&gt;  1 3005     M     4     3/30/2007  635     19.3  \n#&gt;  2 3005     M     6     4/11/2007  460.7   31    \n#&gt;  3 3005     M     8     4/27/2007  530.2   39.6  \n#&gt;  4 3017     M     4     10/6/2006  202.4   25.9  \n#&gt;  5 3017     M     6     10/19/2006 384.7   45.1  \n#&gt;  6 3017     M     8     11/3/2006  458.7   57.2  \n#&gt;  7 3434     F     4     11/22/2006 238.9   26.6  \n#&gt;  8 3434     F     6     12/6/2006  378     45.9  \n#&gt;  9 3434     F     8     12/22/2006 409.8   56.2  \n#&gt; 10 3449     M     4     1/5/2007   121     27.5  \n#&gt; 11 3449     M     6     1/19/2007  191.3   42.9  \n#&gt; 12 3449     M     8     2/2/2007   182.5   56.7  \n#&gt; 13 3499     F     4     1/5/2007   220.2   19.8  \n#&gt; 14 3499     F     6     1/19/2007  556.9   36.6  \n#&gt; 15 3499     F     8     2/2/2007   446     43.6\n\n깔끔한 데이터를 구글시트의 tidy_data 시트에 저장한다. sheet_write() 함수를 사용하여 깔끔한 데이터를 구글시트에 저장한다. 이때 해당 시트에 권한이 부여되어야만 데이터를 저장할 수 있다. 권한이 부여되지 않은 경우에는 그림 26.4 와 같이 편집자 권한을 부여하면 깔끔한 데이터가 구글시트에 저장된다.\n\n\n\n\n\n그림 26.4: 구글시트 messy_data 권한부여\n\n\n\ntidy_data |&gt; \n  sheet_write(ss = sheet_url, sheet = \"tidy_data\")\n✔ Writing to mess_data.\n✔ Writing to sheet tidy_data.\n\n\n\n\n\nBroman, Karl W., 와/과 Kara H. Woo. 2018. “Data Organization in Spreadsheets”. The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>상용 데이터</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 데이터 과학",
    "section": "",
    "text": "서문",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#책의-구성",
    "href": "index.html#책의-구성",
    "title": "챗GPT 데이터 과학",
    "section": "책의 구성",
    "text": "책의 구성",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#감사의-글",
    "href": "index.html#감사의-글",
    "title": "챗GPT 데이터 과학",
    "section": "감사의 글",
    "text": "감사의 글\n\n이 책이 탄생할 수 있도록 도움을 주신 여러분께 깊은 감사의 마음을 표합니다.\n공익법인 한국 R 사용자회가 없었다면 데이터 과학분야 챗GPT 시리즈가 세상에 나오지 못했을 것입니다. 한국 R 사용자회의 유충현 회장님, 신종화 사무처장님, 홍성학 감사님, 올해부터 새롭게 공익법인 한국 R 사용자를 이끌어주실 형환희 회장님께 감사드립니다.\n또한 이 책은 2014년 처음 몸담게 된 소프트웨어 카펜트리 그렉 윌슨 박사님과 Python for Informatics 저자인 미시건 대학 찰스 세브란스 교수님을 비롯한 전세계 수많은 익명의 기여자들의 노력과 지원이 있었고, 서울 R 미트업에서 발표해주시고 참여해주신 수많은 분들이 격려와 영감을 주셨기에 가능했습니다.\n이 책이 출간되는데 있어 이들 모든 분들의 도움 없이는 어려웠을 것입니다. 그동안의 관심과 지원에 깊은 감사를 드리며, 이 책이 데이터 과학의 발전과 독자들에게 도움이 될 수 있기를 바라는 마음으로 마무리하겠습니다.\n\n2024년 4월 속초 영금정\n이광춘",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "ingest_database.html",
    "href": "ingest_database.html",
    "title": "24  데이터베이스",
    "section": "",
    "text": "graph LR\n    subgraph 가져오기[\"가져오기\"]\n        ascii[아스키 파일] --&gt; 핸들러_가져오기\n        스프레드시트[\"스프레드시트/통계패키지\"] --&gt; 핸들러_가져오기\n        데이터베이스 --&gt; 핸들러_가져오기\n        통계패키지 --&gt; 핸들러_가져오기    \n        웹[\"웹 데이터\"] --&gt; 핸들러_가져오기\n    end\n\n    핸들러_가져오기[\"핸들러\"] --&gt; import[가져오기]\n    \n    subgraph 이해하기[\"이해하기\"]\n        모형 --&gt; 시각화\n        변환 --&gt; 모형\n        시각화 --&gt; 변환\n    end\n\n    import --&gt; 깔끔화 --&gt; 이해하기\n    이해하기 --&gt; 의사소통\n\nclassDef modern fill:#fff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\nclassDef emphasize fill:#8CBDE3,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\nclassDef subgraphStyle fill:#f0f8ff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:15px;\n\nclass 깔끔화,변환,모형,시각화,의사소통,ascii,웹,스프레드시트,데이터베이스,통계패키지,핸들러_가져오기 modern;\nclass import,데이터베이스 emphasize;\nclass 이해하기,가져오기 subgraphStyle;\n\n\n\n\n\n그림 24.1: 데이터 과학 작업흐름도\n\n\n\n\n\n\n\n파일 형태 데이터는 로컬 컴퓨터에 저장되어 있어 데이터를 불러오기 쉽지만, 데이터의 크기가 커지면 로컬 컴퓨터 저장공간이 부족하여 서버에 데이터를 저장하게 된다. 데이터베이스에 저장된 데이터는 데이터베이스 관리 시스템(DBMS)을 통해 관리되며, 여러 사용자와 개발자들에게 데이터 관련 서비스를 제공하기 위해 인증단계와 권한을 달리 부여하여 활용된다.\n데이터베이스에서 R로 데이터를 가져오는 자세한 기술적인 사항은 “챗GPT SQL” 책을 참조한다. 대표적인 정형 데이터베이스에서 데이터를 가져오는 방법을 주로 다루고 있고, 특히 DuckDB는 속도를 중요시하는 데이터 과학자에게 추천한다.ㅏ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“챗GPT SQL” 책\n\n\n\n이 책은 데이터베이스와 SQL에 대한 기본적인 내용을 다루고 있다. 데이터베이스와 SQL에 대한 기본적인 지식을 습득하고 싶은 분들에게 추천한다.\n📘 교보 POD 종이책: https://bit.ly/3OJmMeT  📗 교보 전자책: https://bit.ly/48fujZD  🌐 웹사이트: https://bit.ly/48gYn7d  💻 소스코드: https://bit.ly/3wrgeuP",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터베이스</span>"
    ]
  },
  {
    "objectID": "ingest_database.html#tidyverse-workflow",
    "href": "ingest_database.html#tidyverse-workflow",
    "title": "24  데이터 사이언스 운영체제",
    "section": "26.1 tidyverse 작업 흐름",
    "text": "26.1 tidyverse 작업 흐름\ntidyverse의 핵심적인 내용은 다양한 형태의 데이터를 가져와서 최종 산출물을 사람과 기계가 커뮤니케이션할 수 있는 형태로 제작하는 과정을 추상화한 것으로 이해할 수 있다. 시각화(Visualization)는 데이터에 대한 통찰력(insight)과 탄성, 놀라움을 줄 수 있지만, 확장성(Scalability) 측면에서는 한계가 명확히 존재하게 되는데 이는 사람이 작업 흐름 루프에 포함되기 때문이다. 반대로 모형(Model)은 자동화와 확장성에는 장점이 있지만, 주어진 모형 틀 안에서만 이뤄지기 때문에 통찰력, 놀라움, 탄성을 주지는 못하는 아쉬움이 있다. 따라서, tidyverse는 시각화와 모형을 통해 통찰력과 함께 자동화에 대한 부분도 충분히 반영한 체계적인 작업 흐름을 제시하고 있는데, 이를 관통하는 핵심적인 개념이 파이프(pipe)로 이미 이런 개념은 유닉스 파이프 연산자를 통해 검증되었다.\n\n\n\nTidyverse 데이터 사이언스 작업흐름",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "ingest_database.html#tidy-data",
    "href": "ingest_database.html#tidy-data",
    "title": "24  데이터 사이언스 운영체제",
    "section": "26.2 깔끔한 데이터",
    "text": "26.2 깔끔한 데이터\n기존 소프트웨어와 다르게 tidyverse는 데이터를 중심으로 다루기 때문에 깔끔한 데이터(tidy data)에 대한 이해도 추가로 필요로 한다. 깔끔한 데이터(tidy data)의 정의는 데이터를 통해 정보를 추출하고, 인사이트를 도출하기 위해서 시각화를 하고, 데이터를 모형으로 자동화하고, 커뮤니케이션을 위한 웹앱을 개발하고 발표 보고서를 작성할 때 수월한 자료구조를 갖는 데이터를 의미한다. 엄밀한 의미로 깔끔한 데이터를 전산학 데이터베이스 이론을 가져와서 설명할 수도 있지만, 비전산 전공자의 관점에서 풀어보자면 깔끔한 데이터가 준비되면 정제작업과 변형, 모형 개발, 시각화, 보고서 작성을 원활히 할 수 있는 반면 엉망진창인 데이터(messy data)는 그렇지 않은 데이터로 볼 수 있다.\n깔끔한 데이터는 특정한 구조를 갖추고 있는데 변수는 열(column)이고, 관측점은 행(row)이며, 관측 단위에 대한 형태는 데이터셋 즉, 테이블(table)로 구성된다.\n깔끔한 데이터 원칙은 전산학 코드(Codd) 박사의 관계대수(Relational Algebra)와 깊은 관련이 있어, 통계학 전공자들은 해당 데이터셋에서 관측점과 변수를 각각 식별하는 작업을 쉽게 생각하지만, 일반적으로 변수와 관측점을 정확하게 정의하는 것이 보통 어려운 것은 아니다. 따라서, 행과 행보다는 변수 간 기능적 관계(functional relationship)를 기술하는 것이 더 쉽고, 칼럼 그룹 집단 간 비교보다 관측점 그룹 집단 사이 비교를 하는 것이 더 쉽다.\n깔끔한 데이터(tidy data)는 데이터셋의 의미를 구조에 매칭하는 표준적인 방식으로 이와 같이 데이터가 구조화되면, 데이터 분석, 조작, 시각화, 모형 작업을 수월히 진행할 수 있다.\n\n각 변수가 칼럼이 된다.\n각 관측점은 행이 된다.\n관측 단위에 대한 형태는 테이블로 구성한다.\n\n\n\n\n저장 구분\n의미\n\n\n\n\n테이블/파일(table/file)\n데이터셋 (dataset)\n\n\n행(row)\n관측점 (observation)\n\n\n열(column)\n변수 (variable)\n\n\n\n깔끔하지 않는 데이터(messy data)는 위와는 다른 형태의 데이터를 지칭한다. 전산학에서 말하는 코드 제3 정규형이지만, 통계적 언어로 다시 표현한 것이다. 또한, 깔끔한 데이터는 R같은 벡터화 프로그래밍 언어에 특히 잘 맞는다. 왜냐하면 동일한 관측점에 대한 서로 다른 변수 값이 항상 짝으로 매칭되는 것을 보장하기 때문이다.\n시각화와 모형을 새로 개발하는 데 별도 자료구조(data structure)를 다시 창조하는 대신에 가능하면 기존 자료구조를 재사용하는 것을 원칙으로 삼고 있다. ggplot2, dplyr, tidyr을 포함한 대다수 R 패키지는 칼럼에 변수, 행에 관측점을 갖는 직사각형 형태 데이터셋을 가정하고 있다. 그리고, 일부 패키지는 특정한 변수 자료형에 집중한다. stringr은 문자열, lubridate는 날짜/시간, forcats는 요인 자료형에 집중하고 있지만 모두 기존 자료구조 재사용을 염두에 두고 있는 것도 사실이다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "ingest_database.html#tidyverse-functional-programming",
    "href": "ingest_database.html#tidyverse-functional-programming",
    "title": "24  데이터 사이언스 운영체제",
    "section": "26.3 함수형 프로그래밍",
    "text": "26.3 함수형 프로그래밍\n복잡한 문제를 해결하는 강력한 전략은 레고 블록처럼 다수의 간단한 조각으로 나누고 이를 조합하는 것이다. 단, 각 조각은 격리되어 쉽게 파악되며, 다른 조각과 조합할 수 있는 표준 위에서 성립되어야 한다. tidyverse 밑바탕에는 이런 전략이 파이프 연산자를 통해 구현되어 있고, 파이프 연산자(예를 들어, %&gt;%)로 단순한 함수를 조합하여 시스템 전체의 힘을 극대화시킨다.\n이를 위해서는 무엇보다 레고 블록 같은 패키지 혹은 함수가 동일한 인터페이스 표준을 준수해야만 한다. %&gt;% 연산자를 통해 많은 패키지에 걸쳐 동작되도록 만드려면, 함수를 작성할 때 다음 원칙을 준수하여 작성하면 된다.\n\n함수를 가능하면 단순하게 작성한다. 일반적으로 각 함수는 한 가지 작업을 매우 잘해야 되고, 한 문장으로 함수 존재 목적을 기술할 수 있어야 한다.\n변형(transformation)과 부작용(side-effect)을 섞지 말아야 한다. 함수가 객체를 반환하거나, 부작용을 일으키는 둘 중 하나만 동작하게 만든다.\n함수명은 동사로 작성해야 한다. 하지만, 다수의 함수가 동일한 동사를 사용하는 경우는 예외로 한다. 예를 들어 modify, add, compute 등을 들 수 있다. 이런 경우 반복되는 동사가 중복되지 않도록 명사에 집중한다. ggplot2가 좋은 예인데, 기존 플롯에 좌표, 점, 범례 등 거의 모든 함수가 추가되기 때문이다.\n\nR은 데이터를 위해 개발된 함수형 언어를 근본에 두고 있지만, 객체 지향 언어(OOP)나 다른 언어 패러다임과 싸우려고 하지 말고 적극적으로 받아들이라고 충고하고 있는데, 이것이 의미하는 바는 다음과 같다.\n\n상태 불변 객체: 작성된 코드에 대한 추론이 쉬워진다.\nS3, S4에서 제공하는 제네릭 함수: 상태 변형 가능한 상태가 필요하다면, 파이프 내부에서 구현한다.\nfor 루프를 추상화한 도구: apply 함수 가족과 purrr 맵(map) 함수\n\n데이터 사이언스에서 병목점으로 문제가 발생되는 곳은 공통적으로 컴퓨터 실행 시간(computing time)이 아니라 사람의 생각(thinking time)의 시간이다. 따라서, 함수명을 작성할 때 생각이 잘 연상되는 이름으로 작명하고 시간을 적절히 안분하고, 명시적이며 긴 명칭을 변수명, 함수명, 객체명에 사용하고, 짧은 명칭은 가장 중요한 이름으로 아껴서 사용한다. RStudio 소스 편집기의 자동 완성 기능을 사용하는 경우 접두어가 접미어보다 왜 중요한지 알 수 있고, stringr, xml2, rvest 패키지를 살펴보면 접두어에 일관된 명칭을 부여한 장점을 알 수 있다.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "ingest_database.html#footnotes",
    "href": "ingest_database.html#footnotes",
    "title": "24  데이터 사이언스 운영체제",
    "section": "",
    "text": "Hadley Wickham (2017-11-13), “The tidy tools manifesto”↩︎",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>데이터 사이언스 운영체제</span>"
    ]
  },
  {
    "objectID": "ingest_web.html",
    "href": "ingest_web.html",
    "title": "26  웹 데이터",
    "section": "",
    "text": "26.1 엑셀\n제21대 국회의원 선거 당선인 명부 데이터를 엑셀 형태로 구하려면, 선관위 자료공간 웹사이트 게시판을 통해 엑셀 파일을 다운로드할 수 있다.\n다운로드 받은 엑셀 데이터를 가져오는 코드를 목록 26.1 와 같이 작성할 수 있다. 먼저, library(readxl)과 library(tidyverse)를 통해 필요한 패키지를 로드한다. nec_sheets &lt;- readxl::excel_sheets(...)는 엑셀 파일의 시트 이름을 가져오는 코드로 엑셀 파일에서 가져올할 시트를 선택한다. winner_raw &lt;- readxl::read_excel(...)는 선택한 시트의 데이터를 읽어오는 코드로 당선인 명부 데이터를 R 환경으로 불러온다.\n이후, winner_raw |&gt; count(소속정당명, name = \"당선인수\", sort = TRUE)는 파이프 연산자(|&gt;)를 사용하여 데이터를 조작하여 정당별로 집계하고, 당선인 수를 계산한 후, 결과를 내림차순으로 정렬하여 당선인 수를 쉽게 파악할 수 있다. janitor::adorn_totals(where = \"row\", name = \"합계\")는 janitor 패키지 함수로, 집계 결과에 총합 행을 추가하여 전체 당선인 수를 확인할 수 있다.\nlibrary(readxl)\nlibrary(tidyverse)\n\nnec_sheets &lt;- readxl::excel_sheets(\"data/nec/제21대_국회의원선거(재보궐선거_포함)_당선인명부.xlsx\")\n\nwinner_raw &lt;- readxl::read_excel(\"data/nec/제21대_국회의원선거(재보궐선거_포함)_당선인명부.xlsx\", sheet = nec_sheets[1])\n\nwinner_raw |&gt; \n  count(소속정당명, name = \"당선인수\", sort = TRUE) |&gt; \n    janitor::adorn_totals(where = \"row\", name = \"합계\")  \n#&gt;    소속정당명 당선인수\n#&gt;  더불어민주당      163\n#&gt;    미래통합당       84\n#&gt;        무소속        5\n#&gt;        정의당        1\n#&gt;          합계      253\n\n\n목록 26.1: 제21대 국회의원 선거 당선인 명부 데이터 불러오기",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>웹 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_web.html#엑셀",
    "href": "ingest_web.html#엑셀",
    "title": "26  웹 데이터",
    "section": "",
    "text": "그림 26.2: 선관위 제21대 국회의원 당선인 명부 엑셀파일",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>웹 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_web.html#api",
    "href": "ingest_web.html#api",
    "title": "26  웹 데이터",
    "section": "\n26.2 API",
    "text": "26.2 API\n공공데이터포털 중앙선거관리위원회 당선인정보 API를 활용하여 당선인 정보를 프로그래밍을 통해 직접 가져올 수 있다. 아래아한글로 작성된 당선인 정보 조회 서비스 API 명세서를 참고하여 Java, Javascript, C#, PHP, Curl, Objective-C, Python, Nodejs, R 언어로 예제 코드가 작성되어 있어 사용자가 편리하게 API를 활용할 수 있다.\n\n\n\n\n\n그림 26.3: 국가선거정보 - 당선인 정보 조회 서비스 API 명세서\n\n\n공공데이터포털 API 끝점(Endpoint)와 API KEY를 발급받고 API 서비스 신청을 하였다면 다음 단계로 아래아한글 당선인 정보 조회 서비스 API 명세서 내용을 참고하여 코드를 작성한다.\n\n\n\n\n\ngraph LR\n\n  subgraph API 파악\n    A[공공데이터포털&lt;br&gt;API 발급] --&gt; B[API 명세서&lt;br&gt;참고]\n  end\n  \n  subgraph 스크립트 작성\n    B --&gt; C[당선인 정보 API 호출&lt;br&gt;스크립트 작성]\n    B --&gt; D[선거구 정보 API 호출&lt;br&gt;스크립트 작성]\n  end\n  \n  subgraph 함수 작성\n    C --&gt; E[당선인 정보 API 호출&lt;br&gt;함수 제작]\n    D --&gt; F[선거구 정보 API 호출&lt;br&gt;함수 제작]\n  end\n  \n  subgraph 데이터 가져오기 및 저장\n    F --&gt; G[선거구 데이터프레임&lt;br&gt;제작]\n    G --&gt; H[선거구 당선인&lt;br&gt;데이터프레임 제작]\n    H --&gt; I[데이터 저장]\n  end\n  \nE --&gt; H\n\nlinkStyle 8 stroke:red,stroke-width:4px\n\nstyle A fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle B fill:#f0f0f0,stroke:#333,stroke-width:2px\nstyle C fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle D fill:#e0e0e0,stroke:#333,stroke-width:2px\nstyle E fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle F fill:#d0d0d0,stroke:#333,stroke-width:2px\nstyle G fill:#c0c0c0,stroke:#333,stroke-width:2px\nstyle H fill:#c0c0c0,stroke:#333,stroke-width:2px\nstyle I fill:#c0c0c0,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n공공데이터포털에서 제공하는 API를 활용하여 당선인 정보와 선거구 정보를 수집하고 분석에 용이한 형태로 가공하는 과정은 다음과 같다. API 사용을 위해 활용신청과 API KEY 발급을 진행한 후 명세서를 참고하여 API 호출 방법과 반환되는 데이터 형식을 파악한다.\n당선인 정보와 선거구 정보를 가져오기 위한 API 호출 스크립트를 작성하고, 반복을 줄이고 재사용성을 높이기 위해 함수로 변환하는 작업을 수행한다. 선거구 데이터프레임에서 기본정보 즉, 시도명과 선거구명을 작성한 함수 get_winner() 에 전달하여 선거구별 당선인 정보를 데이터프레임으로 가져온다.\n\n26.2.1 스크립트\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\n\nresponse &lt;- GET(\"http://apis.data.go.kr/9760000/WinnerInfoInqireService2/getWinnerInfoInqire\",\n                query = list(sgId = \"20200415\",\n                              sgTypecode = \"2\",\n                              sdName = \"서울특별시\",\n                              sggName = \"종로구\",\n                              pageNo = 1,\n                              numOfRows = 10,\n                              resultType = \"json\",\n                              serviceKey = Sys.getenv('DATA_GO_DECODE_KEY')))\n\nprint(status_code(response))\n#&gt; [1] 200\n\nresponse_list &lt;- content(response, \"text\") |&gt; \n  fromJSON()\n\nresponse_tbl &lt;- response_list$response$body$items$item\n\nresponse_tbl |&gt; \n    select(sgId, sggName, sdName, giho, jdName, name)\n\n#&gt;       sgId sggName     sdName giho       jdName   name\n#&gt; 1 20200415  종로구 서울특별시    1 더불어민주당 이낙연\n\n\n26.2.2 함수\n\nget_winner &lt;- function(sdName = \"서울특별시\", sggName = \"종로구\") {\n    response &lt;- GET(\"http://apis.data.go.kr/9760000/WinnerInfoInqireService2/getWinnerInfoInqire\",\n                query = list(sgId = \"20200415\",\n                              sgTypecode = \"2\",\n                              sdName = sdName,\n                              sggName = sggName,\n                              pageNo = 1,\n                              numOfRows = 1000,\n                              resultType = \"json\",\n                              serviceKey = Sys.getenv('DATA_GO_DECODE_KEY')))\n\n    response_list &lt;- content(response, \"text\") |&gt; \n      fromJSON()\n    \n    response_tbl &lt;- response_list$response$body$items$item |&gt; \n        select(sgId, sggName, sdName, giho, jdName, name)\n    \n    return(response_tbl)\n}\n\nget_winner(\"서울특별시\", \"종로구\")\n\n#&gt;       sgId sggName     sdName giho       jdName   name\n#&gt; 1 20200415  종로구 서울특별시    1 더불어민주당 이낙연\n\n\n26.2.3 선거구\n중앙선거관리위원회 코드정보 API를 활용하여 선거구 정보를 프로그래밍을 통해 가져올 수 있다. 당선인 명부 데이터를 불러올 때 선거구 정보가 필수적이라 이 과정을 생략할 수는 없다. 당선인 정보와 동일하기 때문에 스크립트 제작과정은 생략하고 명세서에 나와 있는 내용을 바탕으로 R 코드를 작성해서 선거구 데이터프레임을 제작한다.\n\nget_precinct &lt;- function(pageNo = 1) {\n  response &lt;- GET(\"http://apis.data.go.kr/9760000/CommonCodeService/getCommonSggCodeList\",\n              query = list(sgId = \"20200415\",\n                            sgTypecode = \"2\",\n                            pageNo =  pageNo,\n                            numOfRows = 1000,\n                            resultType = \"json\",\n                            serviceKey = Sys.getenv('DATA_GO_DECODE_KEY')))\n  \n  response_list &lt;- content(response, \"text\") |&gt; \n    fromJSON()\n  \n  response_tbl &lt;- response_list$response$body$items$item\n  \n  return(response_tbl)\n}\n\nprecinct_raw &lt;- tibble(page = 1:3) |&gt; \n  mutate(data = map(page, get_precinct)) \n\nprecinct_tbl &lt;- precinct_raw |&gt; \n  unnest(data)\n\nprecinct_tbl  |&gt; \n  select(sgId, sdName, sggName, sggJungsu)\n\n#&gt; # A tibble: 30 × 4\n#&gt;    sgId     sdName     sggName      sggJungsu\n#&gt;    &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;    \n#&gt;  1 20200415 서울특별시 종로구       1        \n#&gt;  2 20200415 서울특별시 중구성동구갑 1        \n#&gt;  3 20200415 서울특별시 중구성동구을 1        \n#&gt;  4 20200415 서울특별시 용산구       1        \n#&gt;  5 20200415 서울특별시 광진구갑     1        \n#&gt;  6 20200415 서울특별시 광진구을     1        \n#&gt;  7 20200415 서울특별시 동대문구갑   1        \n#&gt;  8 20200415 서울특별시 동대문구을   1        \n#&gt;  9 20200415 서울특별시 중랑구갑     1        \n#&gt; 10 20200415 서울특별시 중랑구을     1        \n#&gt; # ℹ 20 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\n26.2.4 선거구 당선인\n\nwinners_raw &lt;- precinct_tbl |&gt; \n  mutate(winner = map2(sdName, sggName, get_winner))\n\nwinners_tbl &lt;-winners_raw |&gt; \n  select(winner) |&gt; \n  unnest(winner)\n\nwinners_tbl\n\n#&gt; # A tibble: 253 × 6\n#&gt;    sgId     sggName      sdName     giho  jdName       name  \n#&gt;    &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; \n#&gt;  1 20200415 종로구       서울특별시 1     더불어민주당 이낙연\n#&gt;  2 20200415 중구성동구갑 서울특별시 1     더불어민주당 홍익표\n#&gt;  3 20200415 중구성동구을 서울특별시 1     더불어민주당 박성준\n#&gt;  4 20200415 용산구       서울특별시 2     미래통합당   권영세\n#&gt;  5 20200415 광진구갑     서울특별시 1     더불어민주당 전혜숙\n#&gt;  6 20200415 광진구을     서울특별시 1     더불어민주당 고민정\n#&gt;  7 20200415 동대문구갑   서울특별시 1     더불어민주당 안규백\n#&gt;  8 20200415 동대문구을   서울특별시 1     더불어민주당 장경태\n#&gt;  9 20200415 중랑구갑     서울특별시 1     더불어민주당 서영교\n#&gt; 10 20200415 중랑구을     서울특별시 1     더불어민주당 박홍근\n#&gt; # ℹ 243 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\n26.2.5 데이터 저장\n\nwinners_tbl |&gt; \n  write_csv(\"data/21st_election_winners.csv\")",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>웹 데이터</span>"
    ]
  },
  {
    "objectID": "ingest_web.html#동적-웹-페이지",
    "href": "ingest_web.html#동적-웹-페이지",
    "title": "26  웹 데이터",
    "section": "\n26.3 동적 웹 페이지",
    "text": "26.3 동적 웹 페이지\n중앙선거관리위원회 선거통계시스템에서 제공하는 제21대 국회의원선거 당선인 명단을 R과 RSelenium을 활용한 동적 웹 크롤링을 통해 수집하고 정제하는 과정을 살펴본다. RSelenium 패키지를 설치하고 불여우(Firefox) 브라우저를 제어하기 위해 드라이버를 설치하고 선관위 웹사이트로 이동한다.\n다음으로 CSS 선택자(selector)를 이용하여 선거유형, 선거명, 선거코드, 시도 등의 조회조건을 순차적으로 선택하고 클릭 이벤트를 실행하여 검색조건을 완성한 후 검색 버튼을 클릭하여 해당 조건에 맞는 당선인 명단이 포함된 HTML 표를 브라우저에 렌더링한다. 마지막 단계로, 다시 CSS 선택자로 해당 표를 선택하고 getElementAttribute() 함수를 통해 HTML 표를 추출한 다음, rvest 패키지 read_html(), html_table() 함수를 이용하여 HTML 표를 데이터프레임으로 변환하여 tibble 형태로 크롤링 작업을 마무리 힌다.\n과거 RSelenium을 install.packages() 명령어를 통해서 CRAN에서 다운로드를 할 수는 없었으나 이제 CRAN, RSelenium 에서 직접 설치가 가능하고 GitHub rOpenSci 저장소에서 devtools로 설치한다.\n\n# devtools::install_github(\"ropensci/binman\")\n# devtools::install_github(\"ropensci/wdman\")\n# devtools::install_github(\"ropensci/RSelenium\")\n\nlibrary(RSelenium)\nlibrary(tidyverse)\n\n# 1. 데이터 ----\n\nrem_driver &lt;- rsDriver(browser = \"firefox\", port = 4568L)\nremdrv_client &lt;- rem_driver[[\"client\"]]\nremdrv_client$navigate(\"http://info.nec.go.kr/main/showDocument.xhtml?electionId=0000000000&topMenuId=EP&secondMenuId=EPEI01\")\n\n# 선거유형 \"electionType2\" 선택 후 클릭\nelectionType2 &lt;- remdrv_client$findElement(using = \"css selector\", \"#electionType2\")\nelectionType2$clickElement()\n\n# 조회조건: 제21대 선택\nelectionName &lt;- remdrv_client$findElement(using = \"css selector\", \"#electionName &gt; option:nth-child(2)\")\nelectionName$clickElement()\n\n# 조회조건: 제21대 선택 &gt; 국회의원선거\nelectionCode &lt;- remdrv_client$findElement(using = \"css selector\", \"#electionCode &gt; option:nth-child(2)\")\nelectionCode$clickElement()\n\n# 조회조건: 제21대 선택 &gt; 국회의원선거 &gt; 시도 &gt; 서울특별시\ncityCode &lt;- remdrv_client$findElement(using = \"css selector\", \"#cityCode &gt; option:nth-child(2)\")\ncityCode$clickElement()\n\n# 검색 실행\nrunButton &lt;- remdrv_client$findElement(using = \"css selector\", \"#searchBtn\")\nrunButton$clickElement()\n\n# HTML 표 --&gt; 데이터프레임 변환\nwinner_html &lt;- remdrv_client$findElement(\"css\", \"#table01\")$getElementAttribute(\"outerHTML\")[[1]]\n\nwinner_table &lt;- read_html(winner_html) %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  .[[1]] \n\nremdrv_client$close()\n\nwinner_table |&gt; \n  select(-직업, -학력, -경력)\n\n#&gt; # A tibble: 49 × 6\n#&gt;    선거구명     정당명       `성명(한자)`   성별  `생년월일(연령)` `득표수(득표율)`\n#&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;           \n#&gt;  1 종로구       더불어민주당 이낙연(李洛淵) 남    1952.12.20(67세) 54,902(58.38)   \n#&gt;  2 중구성동구갑 더불어민주당 홍익표(洪翼杓) 남    1967.11.20(52세) 70,387(54.25)   \n#&gt;  3 중구성동구을 더불어민주당 박성준(朴省俊) 남    1969.04.23(50세) 64,071(51.96)   \n#&gt;  4 용산구       미래통합당   권영세(權寧世) 남    1959.02.24(61세) 63,891(47.80)   \n#&gt;  5 광진구갑     더불어민주당 전혜숙(全惠淑) 여    1955.05.05(64세) 56,608(53.68)   \n#&gt;  6 광진구을     더불어민주당 고민정(高旼廷) 여    1979.08.23(40세) 54,210(50.37)   \n#&gt;  7 동대문구갑   더불어민주당 안규백(安圭伯) 남    1961.04.29(58세) 51,551(52.72)   \n#&gt;  8 동대문구을   더불어민주당 장경태(張耿態) 남    1983.10.12(36세) 55,230(54.54)   \n#&gt;  9 중랑구갑     더불어민주당 서영교(徐瑛敎) 여    1964.11.11(55세) 55,185(57.76)   \n#&gt; 10 중랑구을     더불어민주당 박홍근(朴洪根) 남    1969.10.08(50세) 74,131(59.28)   \n#&gt; # ℹ 39 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n\n\n그림 26.4: 동적 웹 페이지 데이터 추출과정\n\n\n\n\n\n\n\n\n자바(Java) 설치 오류\n\n\n\nRSelenium 패키지를 사용하기 위해서는 자바(Java)가 설치되어 있어야 한다. 데이터 과학 PC (Java) - 윈도우 블로그 게시글을 참조하거나 library(multilinguer); install_java() 함수를 사용해서 설치하여 사용할 수 있다.\n\njava_check()에서 다음과 같은 에러가 발생했습니다: PATH to JAVA not found. Please check JAVA is installed.",
    "crumbs": [
      "**7부** 가져오기",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>웹 데이터</span>"
    ]
  },
  {
    "objectID": "comm_map.html",
    "href": "comm_map.html",
    "title": "\n20  지도 공간정보\n",
    "section": "",
    "text": "20.1 도구의 진화\nGIS, 좌표계, 파일 형식, 계층(Layer), 교차분석 등 주요개념을 바탕으로 도구 진화과정을 살펴보자. 초기 PC가 보급되면서 1980년대 GIS 시스템은 사일로 형태 고가 시스템이었으며, 1990년대에는 1980년대 개발된 GIS 시스템간 연결이 시작되면서 스파게티 현상이 심해지면서 새로운 애플리케이션과 파일 형식이 도입되는데 필요 이상의 낭비가 심해지는 구조가 되었다.\n2000년대 GDAL(Geospatial Data Abstraction Layer)이 등장하여 개발자가 각 파일 형식별로 서로 다른 드라이버를 작성하는 대신 GDAL 클라이언트 드라이버만 개발하면 되기 때문에 일대 혁신이 일어났다. 2010년대는 인공위성을 통한 대량의 데이터가 넘쳐나며 이를 저장하기 위해서 클라우드 서비스가 우후죽순처럼 생겨났고 각기 다른 파편화된 진화과정이 일어났다. 2020년대 넘어서면서 1990년대와 유사한 상황이 재현되었고, 이를 해결하고 새로운 전환점을 만들기 위해 OpenEO가 제시되며 사용자와 데이터 센터 사이에서 새로운 표준으로 자리잡게 되면서 R, 파이썬 등 데이터 과학 프로그래밍 언어를 통한 공간정보 가치 창출이 용이해졌다. (Pebesma 기타 2016)",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_map.html#도구의-진화",
    "href": "comm_map.html#도구의-진화",
    "title": "\n20  지도 공간정보\n",
    "section": "",
    "text": "GIS 도구 진화과정",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_map.html#gis-생태계",
    "href": "comm_map.html#gis-생태계",
    "title": "\n20  지도 공간정보\n",
    "section": "\n20.2 GIS 생태계",
    "text": "20.2 GIS 생태계\n인간은 오랜 역사 동안 다양한 도구를 개발하고 사용해 왔으며, GIS에서도 도구는 단순한 계산부터 복잡한 데이터 분석, 시각화를 통한 의사결정지원, 최근 거대언어모형(LLM) 인공지능까지 다양한 형태로 기여하고 있다.\n현재 직면한 문제를 해결하는데 과거 도구를 사용하는 것은 마치 철기시대에 석기시대 도구를 사용하는 것과 다름이 없다. 따라서, 데이터를 통해 가치를 만드는 데이터 과학자 입장에서 도구의 선택은 매우 중요할 수 밖에 없다.\n\n\n\n\n\n그림 20.2: sf 패키지와 의존성 - 화살표는 강한 의존성, 점선 화살표는 약한 의존성\n\n\n현재 R 공간정보 생태계는 공간정보 도구 진화과정을 몇차례 경험한 후에 그림 20.2 처럼 자리를 잡았다. 결론부터 들어가면 sf 패키지가 생태계의 중심으로 자리 잡았으며, 사용자와 개발자는 sf 패키지를 통해 상당수 공간정보 데이터 문제를 해결할 수 있게 되었다. 하지만, sf 패키지는 C/C++ 라이브러리에 크게 의존성하기 때문에 각 라이브러리를 살펴보는 것은 향후 GIS 개발자와 분석가로 현업에서 활약하는데 큰 도움이 될 것으로 보인다.\nGDAL, GEOS, PROJ, liblwgeom, s2geometry, NetCDF, udunits2는 C/C++ 라이브러리를 개발, 유지보수, 지원하는 커뮤니티가 존재하는 반면 R, 파이썬, 줄리아(Julia), 자바스크립트는 대화형 인터페이스를 통해 라이브러리를 활용하는 커뮤니티도 존재한다. (Pebesma 와/과 Bivand 2023)\n\n\nGDAL (Geospatial Data Abstraction Library) 라이브러리는 공간 데이터 처리에 있어 멀티툴의 대명사인 스위스군 칼(SAK, Swiss Army Knife)이라는 별명을 갖고 있고 100개가 넘는 다른 라이브러리와 연결되어 다양한 공간 데이터를 불러오고, 처리하고, 내보내는 기능을 제공한다.\n\nPROJ는 지도 투영 및 데이터 변환을 위한 라이브러리로, 하나의 좌표계에서 다른 좌표계로 좌표를 변환할 때 유용하다.\n\nGEOS (Geometry Engine Open Source)와 s2geometry 라이브러리는 기하학 연산에 사용하며, 길이, 면적, 거리를 측정하거나 연산작업에 사용되며 \\(R^2\\)로 표기되며 GEOS는 평평한 2차원 공간에, \\(S^2\\)로 표기되며 s2geometry는 구형 공간에 사용한다.\n\nNetCDF는 파일 형식이며 C 라이브러리로, 어떤 차원 배열도 정의할 수 있으며 특히 (기후) 모형개발 커뮤니티에서 널리 사용된다. Udunits2는 측정 단위의 데이터베이스 및 소프트웨어 라이브러리로, 단위의 변환과 파생 단위, 사용자 정의 단위를 처리한다. liblwgeom은 PostGIS 구성 요소로, GDAL, GEOS에서 누락된 몇 가지 루틴을 포함한다. (Pebesma 기타 2016)",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_map.html#사례",
    "href": "comm_map.html#사례",
    "title": "\n20  지도 공간정보\n",
    "section": "\n20.3 사례",
    "text": "20.3 사례\nsf 패키지가 추상화된 함수를 제공하기 때문에 R 공간정보 생태계에서 GDAL 라이브러리를 GIS 분석가가 직접적으로 다룰 일은 없다. 하지만, GIS 도구를 활용하여 2023년 7월 기준 시도별 인구수를 대한민국 지도위에 도식화하기 위해서는 R 공간정보 생태계를 구성하는 다양한 도구가 꼭 필요하다. 먼저, 지도 데이터를 다루기 위해 sf 패키지를 사용하고 통계청(KOSIS) 데이터를 처리하기 위해 tidyverse 패키지 도구를 활용하고 오픈지도 개발자가 공개한 geojson 파일을 결합하여 인구수를 시도 수준에서 시도별 인구수 색상을 달리하여 지도위에 시각화한다. 작성된 코드는 R로 작성되었지만 파이썬 진영에도 공간정보 생태계도 유사한 도구가 준비되어 있어 각자 사용하기 좋은 도구를 가지고 의미있는 결과물을 만들어내고 있다.\n\nlibrary(sf)\nlibrary(tidyverse)\nsf_use_s2(FALSE)\n\n## 지도\nkorea_map &lt;- read_sf(\"data/HangJeongDong_ver20230401.geojson\")\n\nsido_map &lt;- korea_map |&gt; \n  group_by(sidonm) |&gt; \n  summarise(geometry = sf::st_union(geometry))\n\n## 23년 7월 인구수(KOSIS) 행정구역별, 성별 인구수\npop_tbl &lt;- read_csv(\"data/행정구역_시군구_별__성별_인구수_20230831223248.csv\",\n         locale=locale(encoding=\"euc-kr\"), skip = 1) |&gt; \n  set_names(c(\"sidonm\", \"인구수\")) |&gt; \n  mutate(sidonm = if_else(sidonm == \"강원특별자치도\", \"강원도\", sidonm))\n\nsigo_gg &lt;- sido_map |&gt; \n  left_join(pop_tbl) |&gt; \n  ggplot() +\n    geom_sf(aes(geometry = geometry, fill = cut(인구수, 10)), show.legend = FALSE) +\n    ggrepel::geom_label_repel(aes(label = sidonm, geometry = geometry), \n                              size = 3, stat = \"sf_coordinates\") +\n    theme_void() +\n    scale_fill_brewer(palette = \"OrRd\")\n\nragg::agg_jpeg(\"images/GIS_tools.jpeg\",\n               width = 10, height = 7, units = \"in\", res = 600)\nsigo_gg\ndev.off()\n\n\n\nGIS 도구 활용 대한민국 시도별 인구수",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_map.html#마무리",
    "href": "comm_map.html#마무리",
    "title": "\n20  지도 공간정보\n",
    "section": "\n20.4 마무리",
    "text": "20.4 마무리\n공간정보 개발자와 사용자가 개발하는 코드는 기계보다 사람 친화적으로 바뀌었으며, 효과적인 디버그와 신속한 개발을 위한 파이프 철학도 도입되어 생산성 향상이 비약적으로 높아졌고 도구의 추상화 수준도 대폭 향상되었다. IoT와 인공위성을 통해 엄청난 공간정보 데이터가 축적되고 있지만, GIS 도구가 꾸준히 발전하면서 이제 누구나 이러한 도구를 활용하여 공간정보 데이터를 통해 의미있는 산출물을 제작하고 도구도 개발할 수 있게 되었고, 다른 한편으로는 도구 없이 도구에 대한 이해없이 프롭테크를 논하는 것조차 의미없는 시대로 접어들고 있다.\n\n\n\n\nPebesma, Edzer, 와/과 Roger Bivand. 2023. Spatial Data Science: With Applications in R. Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPebesma, Edzer, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian Briese, 와/과 Markus Neteler. 2016. “OpenEO: a GDAL for Earth Observation Analytics”. 2016년. https://r-spatial.org/2016/11/29/openeo.html.\n\n\n이광춘. 2023. “공간정보의 역사 및 공간정보 처리기법”. 프롭빅스(PROPBIX), 호 13 (9월). http://www.kahps.org/.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>지도 공간정보</span>"
    ]
  },
  {
    "objectID": "comm_quarto.html",
    "href": "comm_quarto.html",
    "title": "22  쿼토",
    "section": "",
    "text": "쿼토를 사용하면 저자들은 코드, 문서 요소, 텍스트를 하나의 문서로 매끄럽게 통합할 수 있어데이터 분석 과정을 투명하고 재현 가능하게 공유할 수 있다.\n쿼토 문서는 마치 살아있는 것처럼 역동적이다. R, 파이썬, 줄리아, SQL, JS 코드를 수정하면 분석결과와 데이터 시각화가 자동으로 업데이트되고, 문장을 고치면 전체 문서의 흐름이 자연스럽게 되어 문서 작성이 한층 더 생생해지는 경험을 할 수 있고 무엇보다 문서의 외양보다 콘텐츠 자체 집중할 수 있다.\n쿼토는 보고서, 학술논문, 프레젠테이션, 대시보드 등 다양한 형식으로 문서를 제작할 수 있을 뿐만 아니라, PDF, Word, HTML, 슬라이드와 인터랙티브 웹페이지까지 개발할 수 있어 작성한 문서를 상황에 맞게 유연하게 공유할 수 있다.\n쿼토는 데이터 과학자에게 코딩과 글쓰기를 자유롭게 오가며 통합적으로 사고할 수 있는 환경을 제공하여 AI 데이터 과학 시대 최적의 커뮤니케이션 도구로 평가받고 있다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“챗GPT 쿼토” 책\n\n\n\n이 책은 쿼토(Quarto) 대한 기본적인 내용을 다루고 있다. 과학기술 디지털 글쓰기에 대한 기본적인 지식을 습득하고 싶은 분들에게 추천한다.\n📘 교보 POD 종이책: https://bit.ly/3wElKuk  📗 교보 전자책: https://bit.ly/3IkWE6f  🌐 웹사이트: https://bit.ly/3P7PVjX  💻 소스코드: https://bit.ly/3InK46m",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>쿼토</span>"
    ]
  },
  {
    "objectID": "comm_export.html",
    "href": "comm_export.html",
    "title": "\n23  저장하기\n",
    "section": "",
    "text": "23.1 그림 저장하기\nggsave 명령어를 사용해서, ggplot2에서 생성한 가장 최신 도식화 결과물을 저장하는 방법을 이미 살펴봤다. 다시 상기시키기 위해 명령어를 적어보면 다음과 같다:\n# eval: false\nggsave(\"My_most_recent_plot.pdf\")\nRStudio 내부에서 그림을 저장할 경우, ‘Plot’ 윈도우에서 ‘Export’ 버튼을 사용한다. 버튼을 클릭하면 .pdf, .png, .jpg 혹은 다른 이미지 형식으로 저장할지 선택 옵션이 제시된다.\n종종 ‘Plot’ 윈도우에 먼저 찍어보지 말고 도표를 저장하고 싶을 때도 있다. 아마도 여러 페이지에 걸친 PDF 문서를 생성하고 싶을 것이다. 예를 들어, 각각은 다른 도표로 말이다. 혹은 다수 파일에서 부분집합으로 데이터를 뽑아내고, 각 하위 데이터에 대해 도식화를 하고, 결과물을 도표로 저장하고자 한다. 하지만 각각에 대해 ‘Export’ 버튼을 클릭하려고 루프를 중단할 수는 없는 노릇이다.\n이런 경우 더 유연한 접근법을 사용할 수 있다. pdf 함수는 새로운 PDF 장치를 생성한다. pdf 함수에 여러 인자를 사용해서 크기와 해상도를 조절할 수 있다.\npdf(\"Life_Exp_vs_time.pdf\", width=12, height=4)\nggplot(data=gapminder, aes(x=year, y=lifeExp, colour=country)) +\n  geom_line() +\n  theme(legend.position = \"none\")\n\n# 그런 다음에는 반드시 PDF 장치를 끄는 것을 잊지 말아야 한다!\ndev.off()\n저장한 문서를 열어서 살펴본다.\njpeg, png 등의 명령어도 다양한 형식으로 문서를 저장하는 데 유사하게 사용할 수 있다.",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>저장하기</span>"
    ]
  },
  {
    "objectID": "comm_export.html#그림-저장하기",
    "href": "comm_export.html#그림-저장하기",
    "title": "\n23  저장하기\n",
    "section": "",
    "text": "도전과제\n\n\n\n\npdf 명령어를 다시 작성해서 PDF 파일에 두 번째 페이지를 생성한다. 각 창 패널에 대륙별로 데이터를 패싯 도표(힌트: facet_grid 사용)로 출력한다.\n\n\n\n\n\n\n\n해답\n\n\n\n\n\n\npdf(\"Life_Exp_vs_time.pdf\", width = 12, height = 4)\n\np &lt;- ggplot(data = gapminder, aes(x = year, y = lifeExp, colour = country)) +\n  geom_line() +\n  theme(legend.position = \"none\")\np\n\np + facet_grid(. ~continent)\n\ndev.off()",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>저장하기</span>"
    ]
  },
  {
    "objectID": "comm_export.html#데이터를-파일-저장",
    "href": "comm_export.html#데이터를-파일-저장",
    "title": "\n23  저장하기\n",
    "section": "\n23.2 데이터를 파일 저장",
    "text": "23.2 데이터를 파일 저장\n어느 시점이 되면 R에서 데이터를 내보내서 파일에 저장하기도 한다. 이런 목적으로 write.table 함수를 사용하는데, 앞서 살펴본 read.table 함수와 매우 유사하다.\n데이터 정제 스크립트를 생성하자. gapminder 데이터에서 Australia 호주만 집중한다:\n\naust_subset &lt;- gapminder[gapminder$country == \"Australia\",]\n\nwrite.table(aust_subset,\n  file=\"cleaned-data/gapminder-aus.csv\",\n  sep=\",\"\n)\n\n쉘로 다시 전환해서 모든 것이 정상인지 데이터를 살펴본다.\n\n$ head cleaned-data/gapminder-aus.csv\n\n음… 엄밀하게 보면 원하는 바는 아니다. 이 모든 인용부호는 어디서 왔을까? 또한 행 번호도 보이는데 무의미하다.\n도움말 파일을 살펴보고 파일에 저장하는 방식을 변경해 본다.\n\n? write.table\n\n기본 설정으로 데이터를 파일에 저장할 때 R은 자동으로 인용부호로 문자 벡터를 감싼다. 행과 열 명칭도 파일에 저장한다.\n다음과 같이 고쳐본다:\n\nwrite.table(\n  gapminder[gapminder$country == \"Australia\",],\n  file=\"cleaned-data/gapminder-aus.csv\",\n  sep=\",\", quote=FALSE, row.names=FALSE\n)\n\n쉘 기술을 사용해서 다시 데이터를 살펴본다:\n\n$ head cleaned-data/gapminder-aus.csv\n\n훨씬 좋아 보인다!\n\n\n\n\n\n\n도전과제\n\n\n\n1990년 이후 수집된 데이터를 gapminder 데이터에서 부분집합으로 구성하는 데이터 정제 스크립트를 작성한다. 상기 스크립트를 사용해서 작업한 새로운 부분집합 데이터를 cleaned-data/ 디렉터리에 파일로 저장한다.\n\n\n\n\n\n\n해답\n\n\n\n\n\n\nwrite.table(\n   gapminder[gapminder$year &gt; 1990, ],\n   file = \"cleaned-data/gapminder-after1990.csv\",\n   sep = \",\", quote = FALSE, row.names = FALSE\n)\n\n\n\n\n\n\n\n# 수업 저장소에 이 디렉터리가 포함되는 것을 원치 않기 때문에,\n# cleaned-data 디렉터리를 삭제한다.\nunlink(\"cleaned-data\", recursive=TRUE)",
    "crumbs": [
      "**6부** 커뮤니케이션",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>저장하기</span>"
    ]
  },
  {
    "objectID": "gpt_python.html",
    "href": "gpt_python.html",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "",
    "text": "28.1 파이썬 설치\n가장 최신 파이썬 3.12 버전 대신 안정성이 검증된 파이썬 3.11 버전 다운로드 웹사이트에서 운영체제에 맞게 설치한다. 설치 과정에서 Add Python 3.11 to PATH 옵션을 반드시 선택한다.\nlibrary(reticulate)\nrepl_python()\n\n#&gt; Python 3.11.8 (C:/Users/&lt;사용자명&gt;/AppData/Local/Programs/Python/Python311/python.exe)\n#&gt; Reticulate 1.35.0 REPL -- A Python interpreter in R.\n#&gt; Enter 'exit' or 'quit' to exit the REPL and return to R.\n#&gt; &gt;&gt;&gt;",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#rprofile-설정",
    "href": "gpt_python.html#rprofile-설정",
    "title": "\n28  파이썬 설치\n",
    "section": "\n28.2 .Rprofile 설정",
    "text": "28.2 .Rprofile 설정\n매번 R을 실행할 때마다 파이썬 가상환경을 활성화하는 것은 번거롭다. .Rprofile 파일에 RETICULATE_PYTHON 환경변수를 설정하게 되면, .Rprofile 파일은 R을 시작할 때 자동으로 파이썬 가상환경이 활성화된다.\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 아래 내용을 추가한다.\nSys.setenv(RETICULATE_PYTHON=\"D:/envs/gpt-ds/Scripts/python.exe\")\n\nlibrary(reticulate)\npy_config()\n\n#&gt; python:         D:/envs/gpt-ds/Scripts/python.exe\n#&gt; libpython:      C:/Users/statkclee/AppData/Local/Programs/Python/Python311/python311.dll\n#&gt; pythonhome:     D:/envs/gpt-ds\n#&gt; version:        3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          D:/envs/gpt-ds/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.4\n#&gt; \n#&gt; NOTE: Python version was forced by RETICULATE_PYTHON",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>파이썬 설치</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#감성분석",
    "href": "gpt_python.html#감성분석",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "\n28.2 감성분석",
    "text": "28.2 감성분석\n앞서 구축한 파이썬 가상환경에서 음식점 리뷰에 대한 감성분석을 수행하는 사례를 살펴본다. 먼저 transformers 라이브러리 pipeline을 사용하여 텍스트 분류 모델을 로드하고, 한국어 문장을 입력으로 받아 다국어 버트(bert) 모형으로 감성 분석을 수행한다. 사전 학습된 모델을 활용하여 자연어 처리 감성분석 작업을 수행하는 방법이 앞서 구축한 파이썬 가상환경에서 수행되고 있다.\n\nfrom transformers import pipeline\n\nprompt = \"분위기도 좋았고 음식도 아주 훌륭했어.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n#&gt; [{'label': '4 stars', 'score': 0.5038140416145325}]\n\n\n\n\n그림 28.1: 파이썬 설치 과정",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#파이썬-설치",
    "href": "gpt_python.html#파이썬-설치",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "",
    "text": "그림 28.1: 파이썬 설치 과정\n\n\n\n\n28.1.1 가상환경\n파이썬 가상환경을 생성하는 이유는 프로젝트별로 필요한 패키지를 분리하여 관리하여 충돌을 방지하기 위함이다. 파일명(--name) 방식과 경로명(--prefix) 방식이 있으며 서로 장단점이 있다. 여기서 경로명 방식과 reticulate 패키지를 활용하여 R 내부적으로 수행하는 방식을 소개한다. virtualenv_remove() 함수를 사용하여 가상환경을 삭제할 수 있다.\n\nvirtualenv_create(\"d:/envs/gpt-ds\")\n\n#&gt; virtualenv_remove(\"d:/envs/gpt-ds\")\n#&gt; Remove virtual environment 'd:/envs/gpt-ds'? [Y/n]: Y\n#&gt; Virtual environment \"d:/envs/gpt-ds\" removed.\n\nvirtualenv_list()\n#&gt; [1] \"gpt-ds\"\n\nuse_virtualenv() 함수를 사용하여 가상환경을 활성화하고 virtualenv_install() 함수를 사용하여 필요한 패키지를 설치한다. 감성분석을 위해서 transformers 패키지가 필요한데 torch, torchvision 의존성으로 함께 설치한다.\n\nlibrary(reticulate)\nuse_virtualenv(\"gpt-ds\", required = TRUE)\n\nvirtualenv_install(\"d:/envs/gpt-ds\", c(\"transformers\", \"torch\", \"torchvision\", \"torchtext\"))\n\n\n28.1.2 .Rprofile 설정\n매번 R을 실행할 때마다 파이썬 가상환경을 활성화하는 것은 번거롭다. .Rprofile 파일에 RETICULATE_PYTHON 환경변수를 설정하게 되면, .Rprofile 파일은 R을 시작할 때 자동으로 파이썬 가상환경이 활성화된다.\n\nusethis::edit_r_profile()\n\nusethis::edit_r_profile() 명령어를 통해 .Rprofile 파일을 열고 파이썬 가상환경 gpt-ds의 python.exe 경로를 RETICULATE_PYTHON 환경변수로 설정한다.\nSys.setenv(RETICULATE_PYTHON=\"D:/envs/gpt-ds/Scripts/python.exe\")\n다시 R 세션을 시작한 후 py_config() 함수를 통해 파이썬 환경을 확인하면 앞서 설정한 gpt-ds 가상환경이 해당 디렉토리로 지정된 것을 확인할 수 있다.\n\nlibrary(reticulate)\npy_config()\n\n#&gt; python:         D:/envs/gpt-ds/Scripts/python.exe\n#&gt; libpython:      C:/Users/statkclee/AppData/Local/Programs/Python/Python311/python311.dll\n#&gt; pythonhome:     D:/envs/gpt-ds\n#&gt; version:        3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          D:/envs/gpt-ds/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.4\n#&gt; \n#&gt; NOTE: Python version was forced by RETICULATE_PYTHON",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#감성분석-사례",
    "href": "gpt_python.html#감성분석-사례",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "\n28.2 감성분석 사례",
    "text": "28.2 감성분석 사례\n앞서 구축한 파이썬 가상환경에서 음식점 리뷰에 대한 감성분석을 수행하는 사례를 살펴본다. 먼저 transformers 라이브러리 pipeline을 사용하여 텍스트 분류 모델을 로드하고, 한국어 문장을 입력으로 받아 다국어 버트(bert) 모형으로 감성 분석을 수행한다. 사전 학습된 모델을 활용하여 자연어 처리 감성분석 작업을 수행하는 방법이 앞서 구축한 파이썬 가상환경에서 수행되고 있다.\n\nfrom transformers import pipeline\n\nprompt = \"분위기도 좋았고 음식도 아주 훌륭했어.\"\n\nclassifier = pipeline(\"text-classification\", \n                      model='nlptown/bert-base-multilingual-uncased-sentiment')\n\nprediction = classifier(prompt)\nprint(prediction)\n#&gt; [{'label': '4 stars', 'score': 0.5038140416145325}]",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html",
    "href": "gpt_prompt_ds.html",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "",
    "text": "29.1 질의응답 함수\n먼저, OpenAI API 키를 환경변수에서 로드하고, OpenAI 클라이언트를 초기화한다. answer_question 함수는 데이터 과학 질문을 입력받아 GPT-4 모델에 전달하고, 모델결과를 반환한다. 이 함수에 “데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?”라는 질문을 전달하고, 모델 응답결과를 출력한다.\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nclient = OpenAI(\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n)\n\ndef answer_question(text):\n    prompt = f\"당신은 R 언어를 사용하는 데이터 과학자로 다음 질문을 친절히 설명해주세요: {text}\"\n    \n    try:\n        response = client.chat.completions.create(\n            model='gpt-4',\n            messages=[\n                {\"role\": \"user\", \n                 \"content\": prompt }\n            ]\n        )\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        return str(e)\n\n# Example usage\nanswer_text = answer_question(\"데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\")\nprint(answer_text)\n동일한 프롬프트에 모형을 gpt-3.5-turbo와 현존하는 가장 높은 성능을 보이고 있는 gpt-4를 사용하여 결과를 비교해보자. 토큰 당 달리 과금되기 때문에 이점을 염두에 두고 데이터 과학 “데이터 탐색”에 대한 두 모형의 차이를 살펴보면 도움이 된다.\n두 모형의 결과를 비교해보면 gpt-4가 더 자세하고 구체적인 내용을 제공하는 것을 확인할 수 있다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#탐색적-데이터-분석",
    "href": "gpt_prompt_ds.html#탐색적-데이터-분석",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.2 탐색적 데이터 분석",
    "text": "29.2 탐색적 데이터 분석\n탐색적 데이터 분석에 초점을 맞춰 다음 질문에 대해서 챗GPT 응답결과를 정리해보자.\n\n데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\n데이터 탐색을 위한 인기있는 R 패키지는 무엇이며, 어떻게 사용되나요?\nR을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\nt-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\ntibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\ntibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\ntibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\ntibble과 ggplot2를 사용하여 산점도와 라인 차트를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n\n\n\n\ngraph LR\n    subgraph 이해하기[\"이해하기\"]\n        모형 --&gt; 시각화\n        변환 --&gt; 모형\n        시각화 --&gt; 변환\n    end\n    import --&gt; 깔끔화 --&gt; 이해하기\n    이해하기 --&gt; 의사소통\n\n    classDef modern fill:#fff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\n    classDef emphasize fill:#8CBDE3,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\n    classDef subgraphStyle fill:#f0f8ff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:15px;\n\n    class import,깔끔화,변환,모형,시각화,의사소통 modern;\n    class 이해하기 subgraphStyle;\n\n\n\n\n\n\n\n29.2.1 질문 1\n\nlibrary(reticulate)\nquestion_01 &lt;- \"데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\"\n\nanswer_01 &lt;- py$answer_question(question_01)\n\n\n\n\n질문\n데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\n\n\n답변\n데이터 탐색 혹은 탐색적 데이터 분석(Exploratory Data Analysis, EDA)는 데이터를 열어보고 분석하는 단계입니다. 그래프나 통계량을 사용하여 수집된 데이터의 구조 및 관계를 파악하는 데 사용됩니다.\n데이터 탐색의 목적은 데이터의 주요 특성을 찾아내고, 구조를 이해하거나 특정 변수 사이의 관계를 확인하거나, 예상치 못한 패턴이나 이상치를 찾는 것입니다.\n데이터 과학에서 데이터 탐색은 매우 중요합니다. 일단 분석 전에 데이터의 특성을 이해하는 것은 추론 및 모델링 과정에서 중요한 역할을 합니다. 데이터 탐색을 통해 이상치를 발견할 수 있고, 이를 통해 데이터 클리닝 및 전처리를 이행할 수 있습니다. 또한, 변수들간의 관계 및 패턴을 분석함으로써, 이후의 분석 방향을 결정하거나 가설을 설정하는데 도움이 됩니다.\n예를 들어, 어떤 상점의 판매 데이터를 분석한다고 가정해봅시다. 데이터 탐색 과정에서 고객 연령대별로 구매 패턴이 유의미하게 차이가 난다면 이를 분석에 반영하여 연령대별 마케팅 전략을 수립하는데 도움이 될 수 있습니다. 또한, 특정 상품이 후불결제 시 결제 실패율이 높다는 이상치를 발견하였다면, 이 문제를 해결하기 위한 방안을 마련하는데 도움이 됩니다. 또한 데이터 탐색을 통해 데이터의 품질 및 적절성을 평가할 수 있으며, 데이터 과학자가 보다 의미있는 결과와 인사이트를 도출하는데 기여하게 됩니다.\n\n\n:",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-2",
    "href": "gpt_prompt_ds.html#질문-2",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.3 질문 2",
    "text": "29.3 질문 2\n\nquestion_02 &lt;- \"R을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_02 &lt;-  py$answer_question(question_02)\n\n\n\n\n질문\nR을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n답변\n\ncat(\"물론입니다. R 언어를 활용한 간단한 데이터 탐색 스크립트를 제공하겠습니다.\\n\\n우선, 내장 데이터셋인 \\\"mtcars\\\"을 사용하여 살펴보겠습니다.\\n\\n```R\\n# 데이터 불러오기\\ndata(mtcars)\\n\\n# 데이터의 상위 6개 행 보기\\nhead(mtcars)\\n\\n# 데이터의 기본적인 구조 보기\\nstr(mtcars)\\n\\n# 간단한 요약통계 보기\\nsummary(mtcars)\\n\\n# 변수 간 상관관계 보기\\ncor(mtcars)\\n\\n# mpg 변수에 대한 히스토그램 생성\\nhist(mtcars$mpg)\\n\\n# 변수 간 산점도 행렬 생성\\npairs(mtcars)\\n```\\n\\n위의 예시는 mtcars 데이터셋을 불러와 파일의 상위 6줄을 보는 것으로 시작합니다. 그 신 dataset의 전반적인 구조 (변수의 이름, 유형, 데이터 셋에서 행의 수 등) 를 확인하고, 요약 통계를 제공합니다. 그 다음으로, 변수 간 상관관계를 설명하며 mpg 변수를 가진 히스토그램을 그립니다. 마지막으로, 산점도 행렬을 생성하여 변수들 사이의 관계를 한눈에 보여줍니다. \\n\\n이러한 단계들은 기본적인 데이터 탐색 과정에서 중요하며, 상황에 따라 추가분석이 필요한 경우가 있습니다.\\n이는 데이터에 대한 조사나, 누락 된 값의 확인, 이상치 검사 등이 포함될 수 있습니다.\")",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-3-1",
    "href": "gpt_prompt_ds.html#질문-3-1",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.4 질문 3",
    "text": "29.4 질문 3\n\nquestion_03 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_03 &lt;- py$answer_question(question_03)\n\n\n\n\n질문\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n답변\n\n#&gt; PCA (Principal Component Analysis)는 데이터의 차원 축소를 위한 테크닉 중 하나입니다. 이는 관찰된 변수들 간의 상관관계를 기반으로 데이터의 차원을 축소하는 방법입니다. \n#&gt; \n#&gt; PCA를 사용하여 R에서 변수의 관계를 탐색하려면 다음과 같이 수행할 수 있습니다:\n#&gt; \n#&gt; 1. 데이터 전처리: 분석을 위한 데이터는 종종 어떤 변수도 누락되지 않도록 전처리되어야 합니다. 또한, PCA는 숫자 변수에 대해 작동하므로 범주형 데이터는 적절하게 변환되어야 합니다.\n#&gt; \n#&gt; 2. 스케일링: PCA는 상관행렬이나 공분산 행렬에 대해 수행되므로 모든 변수가 동일한 스케일을 가져야 합니다. 이를 위해 `scale()` 함수를 사용하여 데이터를 정규화 할 수 있습니다.\n#&gt; \n#&gt; 3. PCA 수행: `prcomp()` 또는 `princomp()` 함수를 사용하여 PCA를 수행하고 결과를 저장할 수 있습니다.\n#&gt; \n#&gt; ```r\n#&gt; pca &lt;- prcomp(data, scale = TRUE)\n#&gt; ```\n#&gt; 4. 결과 시각화 및 해석: `summary()` 함수와 `biplot()` 함수를 사용하여 결과를 해석하고 시각화 할 수 있습니다. `summary(pca)`를 사용하여 설명된 분산의 백분율을 검토해 보고, `biplot(pca)`를 사용하여 기여도를 시각화 할 수 있습니다.\n#&gt; \n#&gt; ```r\n#&gt; summary(pca)\n#&gt; biplot(pca)\n#&gt; ```\n#&gt; \n#&gt; 이렇게 하면 각 주성분이 데이터의 분산을 어느 정도 설명하는지 그리고 각 변수가 주성분에 어떻게 기여하는지 쉽게 이해할 수 있습니다. 이 정보를 통해 변수간의 관계를 분석하고 이해할 수 있습니다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-4-1",
    "href": "gpt_prompt_ds.html#질문-4-1",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.5 질문 4",
    "text": "29.5 질문 4\n\nquestion_04 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_04 &lt;- py$answer_question(question_04)\n\n\n\n\n질문\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n답변\n\n#&gt; PCA(Principal Component Analysis, 주성분 분석)는 많은 변수에서 가장 중요한 패턴을 찾을 수 있도록 도와주는 통계적 기법으로, 이를 통해 데이터의 차원을 축소할 수 있습니다. R에서는 'prcomp' 또는 'princomp' 함수를 사용하여 PCA를 수행할 수 있습니다. \n#&gt; \n#&gt; 추상적인 개념이지만 구체적인 예제를 통해 설명해보겠습니다.\n#&gt; \n#&gt; 1. 데이터 준비: 먼저, 분석할 데이터셋을 로드하고 정제합니다. 모든 변수는 수치형이어야 하며, 결측치가 없어야 합니다.\n#&gt; \n#&gt; 2. 데이터 정규화: PCA는 변수의 스케일에 민감하므로, 데이터는 진행하기 전에 반드시 정규화(normalization)되어야 합니다. R에서는 scale() 함수를 사용해서 모든 변수를 같은 스케일로 조정할 수 있습니다.\n#&gt; \n#&gt; 3. PCA 수행: prcomp() 함수를 사용하여 PCA를 실행합니다. 예를 들어, 데이터 프레임이 df라면 아래와 같이 코드를 작성할 수 있습니다.\n#&gt;     ```\n#&gt;     pca_result &lt;- prcomp(df, center = TRUE, scale. = TRUE)\n#&gt;     ```\n#&gt;     이 함수는 PCA 결과를 반환하며, 각 주성분의 중요도, 각 원래 변수가 주성분에 얼마나 기여하는지 (rotation 또는 loadings라고 함) 등을 제공합니다.\n#&gt; \n#&gt; 4. 결과 분석: summary() 함수를 사용하여 각 주성분이 데이터의 얼마나 큰 비율을 설명하는지 확인할 수 있습니다. \n#&gt;     ```\n#&gt;     summary(pca_result)\n#&gt;     ```\n#&gt;     또한 R에서 제공하는 다양한 시각화 기능 (예: biplot)을 사용하여 결과를 시각화하고 분석할 수 있습니다. 이를 통해 변수 간의 관계를 더 자세히 조사하거나, 데이터를 더 낮은 차원으로 투영하여 차원 축소를 수행할 수 있습니다. \n#&gt; \n#&gt; 따라서 PCA는 데이터의 차원을 줄이면서도 가장 중요한 정보를 유지하는 데 도움이 되며, 이는 특히 데이터의 패턴을 탐색하거나 시각화할 때 유용합니다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-5-1",
    "href": "gpt_prompt_ds.html#질문-5-1",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.6 질문 5",
    "text": "29.6 질문 5\n\nquestion_05 &lt;- \"t-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_05 &lt;- py$answer_question(question_05)\n\n\n\n\n질문\nt-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n답변\n\n#&gt; 네, 말씀하신대로 R 언어를 사용하여 t-SNE, PCA 및 클러스터링을 사용하여 데이터의 변수 간 관계를 탐색하는 샘플 스크립트를 아래에 제공합니다. 이 예시에서는 내장 데이터 세트인 'iris'를 사용합니다.\n#&gt; \n#&gt; ```R\n#&gt; # 필요한 패키지를 불러오기\n#&gt; library(tidyverse)\n#&gt; library(Rtsne)\n#&gt; library(cluster)\n#&gt; library(factoextra)\n#&gt; \n#&gt; # 데이터 불러오기\n#&gt; data(\"iris\")\n#&gt; \n#&gt; # PCA를 이용한 변수 간 관계 탐색\n#&gt; pca_iris &lt;- prcomp(iris[, 1:4], scale = TRUE)\n#&gt; \n#&gt; # PCA 결과 확인\n#&gt; fviz_pca_ind(pca_iris, geom.ind='point',\n#&gt;              pointshape = 21, pointsize = 2, \n#&gt;              fill.ind = iris$Species, palette = \"jco\", \n#&gt;              addEllipses = TRUE, ellipse.level = 0.95)\n#&gt; \n#&gt; # t-SNE를 이용한 변수간 관계 탐색\n#&gt; tsne_iris &lt;- Rtsne(iris[, 1:4], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500)\n#&gt; \n#&gt; # t-SNE 결과 확인 \n#&gt; tsne_iris$Y %&gt;%\n#&gt;   data.frame() %&gt;%\n#&gt;   setNames(c(\"X\", \"Y\")) %&gt;%\n#&gt;   mutate(Species = iris$Species) %&gt;%\n#&gt;   ggplot(aes(X, Y, color = Species)) +\n#&gt;   geom_point() +\n#&gt;   theme_minimal()\n#&gt; \n#&gt; # 클러스터링을 이용한 변수간 관계 탐색\n#&gt; dist_iris &lt;- dist(iris[, 1:4])\n#&gt; hclust_iris &lt;- hclust(dist_iris)\n#&gt; cluster_iris &lt;- cutree(hclust_iris, k = 3)\n#&gt; \n#&gt; # 클러스터링 결과 확인\n#&gt; table(cluster_iris, iris$Species)\n#&gt; plot(hclust_iris)\n#&gt; rect.hclust(hclust_iris, k = 3, border = \"red\")\n#&gt; ```\n#&gt; t-SNE와 PCA는 고차원 데이터를 시각화하기 위해 사용되며, 이를 통해 변수들 사이의 관계를 기하학적으로 이해할 수 있습니다. 클러스터링은 데이터 세트를 비슷한 특성을 가진 그룹으로 분류할 때 사용하며, 이를 통해 관계를 파악할 수 있습니다.\n#&gt; 출력 그래프와 함께 이들 결과를 해석하면 더 깊은 인사이트를 얻을 수 있습니다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-6-1",
    "href": "gpt_prompt_ds.html#질문-6-1",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.7 질문 6",
    "text": "29.7 질문 6\n\nquestion_06 &lt;- \"tibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\"\n\n# answer_06 &lt;- py$answer_question(question_06)\n\n\n\n\n질문\ntibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\n\n\n답변\n\n#&gt; R 언어에서 tibble과 ggplot2 패키지는 시계열 데이터에서 패턴과 추세를 식별하는데 사용할 수 있는 유용한 도구입니다.\n#&gt; \n#&gt; 1. 첫 번째로, tibble은 R에서 데이터 프레임을 다루기 위한 새로운 데이터 타입입니다. 기본 R 데이터 프레임의 단점을 보완하면서, 동일한 구조와 작업을 유지합니다. 그러므로 tibble은 아주 큰 데이터셋에서도 처리 속도가 빠르며, 시계열 데이터를 포함한 모든 종류의 데이터를 쉽게 다룰 수 있게 해줍니다.\n#&gt; \n#&gt; 예를 들어, 우리는 다음과 같은 방법으로 시계열 데이터를 tibble로 변환할 수 있습니다:\n#&gt; \n#&gt; ```r\n#&gt; # 필요한 패키지 불러오기\n#&gt; library(tidyverse)\n#&gt; \n#&gt; # 일별 주가 데이터를 tibble로 변환\n#&gt; stock_prices &lt;- as_tibble(stock_prices)\n#&gt; ```\n#&gt; \n#&gt; 2. 다음으로, ggplot2는 'The Grammar of Graphics'를 기반으로 하는 데이터 시각화 패키지입니다. 이를 이용해 시계열 데이터에서 패턴과 추세를 직관적으로 파악할 수 있습니다.\n#&gt; \n#&gt; 예를 들어, ggplot2를 이용해 주가의 시간에 따른 변화를 그래프로 그릴 수 있습니다.\n#&gt; \n#&gt; ```r\n#&gt; ggplot(data = stock_prices, aes(x = Date, y = Close)) +\n#&gt;   geom_line() +\n#&gt;   labs(title = \"Stock prices over time\", x = \"Date\", y = \"Close price\")\n#&gt; ```\n#&gt; \n#&gt; 여기서 geom_line() 함수는 시간에 따른 주가의 변화를 선 그래프로 그려줍니다. 이를 통해 데이터에서 패턴이나 트렌드를 시각적으로 파악할 수 있습니다. \n#&gt; \n#&gt; 또한, ggplot2 패키지는 다양한 통계 변환 함수와 smoothers를 제공하여 시계열 데이터의 추세를 더 잘 나타낼 수 있습니다. 예를 들어, geom_smooth() 함수는 일련의 데이터 포인트에 적합한 부드러운 곡선을 추가하여 데이터의 전반적인 추세를 도식화해 줍니다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-7-1",
    "href": "gpt_prompt_ds.html#질문-7-1",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.8 질문 7",
    "text": "29.8 질문 7\n\nquestion_07 &lt;- \"tibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_07 &lt;- py$answer_question(question_07)\n\n\n\n\n질문\ntibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n답변\n\n#&gt; 네, 물론입니다. 다음의 예를 들어보겠습니다. 먼저 필요한 라이브러리를 로드합니다:\n#&gt; \n#&gt; ```R\n#&gt; library(tidyverse)  # includes both tibble and ggplot2\n#&gt; ```\n#&gt; \n#&gt; 다음으로, 패턴이나 추세를 확인하려는 데이터를 예로 들어, 내장되어 있는 미국 여성의 생존 기대치를 보여주는 데이터셋인 `women`을 불러옵니다:\n#&gt; \n#&gt; ```R\n#&gt; data(\"women\")\n#&gt; ```\n#&gt; \n#&gt; `tibble`을 사용해 데이터를 보다 더 보기 쉽게 확인할 수 있습니다:\n#&gt; \n#&gt; ```R\n#&gt; women_tibble &lt;- as_tibble(women)\n#&gt; print(women_tibble)\n#&gt; ```\n#&gt; \n#&gt; `ggplot2`을 사용해 이 데이터의 패턴과 추세를 시각화하겠습니다. 본 예시에서는 women 데이터셋에서 나이에 따른 평균 체중을 나타내는 그래프를 그립니다:\n#&gt; \n#&gt; ```R\n#&gt; ggplot(data = women_tibble, aes(x = height, y = weight)) +\n#&gt;   geom_point(shape=1) +    # Use hollow circles\n#&gt;   geom_smooth(method = lm, se = FALSE, color=\"red\") +  # Add linear regression line, without CI\n#&gt;   labs(title = \"Age vs Weight in Women\", x = \"Height\", y = \"Weight\") # labels\n#&gt; ```\n#&gt; 데이터를 plot해 본 결과, `women` 데이터셋에서 여성의 키와 몸무게 사이에 양의 상관 관계가 있음을 발견할 수 있습니다. `geom_smooth` 함수를 사용하면, 이 데이터의 추세를 더 명확하게 파악할 수 있습니다. 이는 `ggplot2` 패키지의 많은 기능 중 하나입니다.\n#&gt; \n#&gt; 이런 식으로 `tibble`과 `ggplot2`를 활용하면, 데이터를 쉽게 관리하고 시각화하면서 여러 패턴과 추세를 찾아낼 수 있습니다. 이는 데이터 과학자가 데이터를 이해하는 데 매우 중요한 과정입니다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-8-1",
    "href": "gpt_prompt_ds.html#질문-8-1",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.9 질문 8",
    "text": "29.9 질문 8\n\nquestion_08 &lt;- \"tibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\"\n\n# answer_08 &lt;- py$answer_question(question_08)\n\n\n\n\n질문\ntibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\n\n\n답변\n\n#&gt; tibble과 ggplot2는 R 언어에서 데이터를 다루고 시각화하는 데 널리 사용되는 도구입니다. 변수 간 관계를 탐색하는 일반적인 기법은 아래와 같습니다.\n#&gt; \n#&gt; 1. **tibble 사용:**\n#&gt; \n#&gt;    tibble은 데이터 프레임을 다루는 R 패키지로, 데이터 프레임을 더 유연하게 조작할 수 있도록 만들어진 향상된 데이터 프레임입니다. tibble을 사용하면, 데이터 요약 및 변환을 위한 dplyr 함수들을 사용할 수 있습니다. \n#&gt; \n#&gt;    변수 간 관계를 탐색하는 tibble의 일반적인 방법은 'group_by', 'summarize'와 같은 dplyr 함수를 사용하여 특정 변수에 따라 데이터를 그룹 지어 분석하는 것입니다. 'filter', 'select', 'mutate', 'arrange'와 같은 함수들도 유용하게 사용될 수 있습니다.\n#&gt; \n#&gt; 2. **ggplot2 사용:**\n#&gt; \n#&gt;    ggplot2는 R에서 가장 인기 있는 데이터 시각화 패키지 중 하나로, 데이터를 그래프로 그려 복잡한 데이터 패턴을 빠르게 이해할 수 있도록 합니다.\n#&gt; \n#&gt;    ggplot2를 사용해서 변수 간 관계를 탐색하는 일반적인 방법 중 하나는 산점도 (scatter plot)를 그리는 것입니다. 두 변수 간의 관계를 보여주는 가장 기본적인 그래프 형태입니다. \n#&gt; \n#&gt;    또한, ggplot2의 'facet_wrap' 또는 'facet_grid' 기능을 사용하면, 한 변수의 서로 다른 수준에 따른 다른 변수의 분포를 여러 개의 작은 패널로 보여주는 그래프를 만들 수 있습니다. \n#&gt; \n#&gt;    'geom_smooth'를 사용하면 추세선을 추가하여 변수 간 관계를 보다 명확하게 확인할 수도 있습니다.\n#&gt;    \n#&gt; 이외에도 histogram, boxplot, violin plot 등 다양한 그래프를 활용하여 변수 간 관계를 탐색할 수 있습니다. 주어진 데이터 특성과 질문에 따라 적절한 방법을 선택하면 됩니다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-9-1",
    "href": "gpt_prompt_ds.html#질문-9-1",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.10 질문 9",
    "text": "29.10 질문 9\n\nquestion_09 &lt;- \"tibble과 ggplot2를 사용하여 산점도와 선 그래프를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\"\n\n# answer_09 &lt;- py$answer_question(question_09)\n\n\n\n\n질문\ntibble과 ggplot2를 사용하여 산점도와 선 그래프를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\n\n\n답변\n\ncat(\"R에서 'tibble'이란 데이터 프레임을 보다 쉽게 다룰 수 있게 설계된 데이터 구조이며, 'ggplot2'는 복잡한 그래프를 효과적으로 생성할 수 있게 도와주는 패키지입니다. \\n\\n선 그래프와 산점도를 생성하여 변수 간의 관계를 탐색하는 방식은 아래와 같이 구성할 수 있습니다.\\n\\n먼저, 필요한 패키지를 설치하고 불러와야 합니다.\\n```R\\ninstall.packages(\\\"dplyr\\\")\\ninstall.packages(\\\"ggplot2\\\")\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n```\\n\\n다음으로, tibble 형태의 데이터를 생성하거나 불러옵니다.\\n```R\\ndf &lt;- tibble::tibble(\\n  x = rnorm(100),  # 100개의 표준 정규분포 난수 생성\\n  y = 1.5 * x + rnorm(100)  # x값에 따른 y값 생성\\n)\\n```\\n이 데이터는 x와 y이라는 두 개의 변수로 이루어진 100개의 관측치를 담고 있습니다.\\n\\n그 후, 'ggplot2'를 사용하여 산점도를 생성할 수 있습니다. \\n```R\\nggplot(data = df, mapping = aes(x = x, y = y)) +\\n  geom_point()  # 산점도 생성\\n```\\n\\n선 그래프를 생성하기 위해서는 'geom_line()'을 사용합니다.\\n```R\\nggplot(data = df, mapping = aes(x = x, y = y)) +\\n  geom_line()  # 선 그래프 생성\\n```\\n\\n이렇게 하면 x와 y의 관계를 산점도와 선 그래프로 시각적으로 표현할 수 있습니다. 각 점이나 선의 위치와 형태를 통해 변수들 간의 관계를 살펴볼 수 있게 됩니다. \\n\\n추가적으로, 여러 그래픽 파라미터를 조정하는 것으로 그래프를 보다 세밀하게 조정할 수 있습니다. 예를 들어, 'labs()', 'theme()', 'geom_smooth()', 'facet_wrap()' 등의 함수들을 조합하면 다양한 복잡한 그래프를 만들 수 있습니다.\")\n#&gt; R에서 'tibble'이란 데이터 프레임을 보다 쉽게 다룰 수 있게 설계된 데이터 구조이며, 'ggplot2'는 복잡한 그래프를 효과적으로 생성할 수 있게 도와주는 패키지입니다. \n#&gt; \n#&gt; 선 그래프와 산점도를 생성하여 변수 간의 관계를 탐색하는 방식은 아래와 같이 구성할 수 있습니다.\n#&gt; \n#&gt; 먼저, 필요한 패키지를 설치하고 불러와야 합니다.\n#&gt; ```R\n#&gt; install.packages(\"dplyr\")\n#&gt; install.packages(\"ggplot2\")\n#&gt; library(dplyr)\n#&gt; library(ggplot2)\n#&gt; ```\n#&gt; \n#&gt; 다음으로, tibble 형태의 데이터를 생성하거나 불러옵니다.\n#&gt; ```R\n#&gt; df &lt;- tibble::tibble(\n#&gt;   x = rnorm(100),  # 100개의 표준 정규분포 난수 생성\n#&gt;   y = 1.5 * x + rnorm(100)  # x값에 따른 y값 생성\n#&gt; )\n#&gt; ```\n#&gt; 이 데이터는 x와 y이라는 두 개의 변수로 이루어진 100개의 관측치를 담고 있습니다.\n#&gt; \n#&gt; 그 후, 'ggplot2'를 사용하여 산점도를 생성할 수 있습니다. \n#&gt; ```R\n#&gt; ggplot(data = df, mapping = aes(x = x, y = y)) +\n#&gt;   geom_point()  # 산점도 생성\n#&gt; ```\n#&gt; \n#&gt; 선 그래프를 생성하기 위해서는 'geom_line()'을 사용합니다.\n#&gt; ```R\n#&gt; ggplot(data = df, mapping = aes(x = x, y = y)) +\n#&gt;   geom_line()  # 선 그래프 생성\n#&gt; ```\n#&gt; \n#&gt; 이렇게 하면 x와 y의 관계를 산점도와 선 그래프로 시각적으로 표현할 수 있습니다. 각 점이나 선의 위치와 형태를 통해 변수들 간의 관계를 살펴볼 수 있게 됩니다. \n#&gt; \n#&gt; 추가적으로, 여러 그래픽 파라미터를 조정하는 것으로 그래프를 보다 세밀하게 조정할 수 있습니다. 예를 들어, 'labs()', 'theme()', 'geom_smooth()', 'facet_wrap()' 등의 함수들을 조합하면 다양한 복잡한 그래프를 만들 수 있습니다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질문-10-1",
    "href": "gpt_prompt_ds.html#질문-10-1",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.11 질문 10",
    "text": "29.11 질문 10\n\nquestion_10 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_10 &lt;- py$answer_question(question_10)\n\n\n\n\n질문\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n답변\n\n#&gt; 주성분 분석(PCA)는 R에서 차원 축소를 수행하는 데 사용되는 기법 중 하나로, 변동이 가장 큰 데이터의 축을 찾아 높은 차원의 데이터를 낮은 차원으로 변환시키는 기법을 설명합니다.\n#&gt; \n#&gt; 다음은 R에서 PCA를 수행하기 위한 간단한 단계입니다:\n#&gt; \n#&gt; 1. 데이터를 준비합니다. PCA는 연속형 변수에만 적용될 수 있으므로, 범주형 변수를 포함하고 있다면 이를 제거하거나 숫자로 변환해야 합니다.\n#&gt; \n#&gt; 2. 데이터 스케일링을 수행합니다. PCA는 변수의 범위에 따라 결과가 크게 달라질 수 있으므로, 데이터의 표준화 또는 정규화 과정이 필요합니다. R의 `scale()` 함수를 사용할 수 있습니다.\n#&gt; \n#&gt; 3. `prcomp()` 또는 `princomp()` 함수를 사용하여 PCA를 수행합니다. 주성분의 개수를 지정하거나, 데이터에 총 변동성의 몇 퍼센트를 설명하도록 할지 결정할 수 있습니다.\n#&gt; \n#&gt; 4. `summary()` 함수를 사용하여 결과를 확인합니다. 여기에는 각 주성분이 설명하는 변동성의 비율, 누적 설명된 변동중 어느 정도의 비율을 차지하는 지 등의 정보가 포함되어 있습니다.\n#&gt; \n#&gt; 5. 필요에 따라 결과를 시각화합니다. 이를 통해 각 주성분이 어떤 변수와 강한 연관성을 보이는지 확인할 수 있습니다. `biplot()` 함수를 사용하면 주성분과 개별 변수 사이의 관계를 한눈에 볼 수 있습니다.\n#&gt; \n#&gt; 이런 방법을 통해 데이터 세트의 차원을 축소하고, 변수 간의 패턴이나 관계를 더 잘 이해하고 시각화하는 데 도움이 될 수 있습니다.\n\n\n\n\n\n\n\n\n그림 29.1: 데이터 과학자 업무방식 변화\n\n\n\nGencay, I. 2023. “ChatGPT Guide for Data Scientists: Top 40 Most Important Prompts”. Towards AI, 4월. https://pub.towardsai.net/chatgpt-guide-for-data-scientists-top-40-most-important-prompts-cdb911f3a427.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#rstudio-부조종사",
    "href": "gpt_python.html#rstudio-부조종사",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "\n28.3 RStudio 부조종사",
    "text": "28.3 RStudio 부조종사\nGitHub Copilot은 Visual Studio Code를 비롯한 다양한 코드 편집기와 통합할 수 있는 AI 기반 코딩 도우미(Assistant)다. 부조종사(Copilot)는 개발자가 보다 효율적이고 정확하게 코드를 작성할 수 있도록 도와주는 지능형 도우미 기능 제공하기 위해 GitHub와 OpenAI가 공동으로 개발했다.\nCopilot은 머신 러닝 알고리즘을 사용하여 다른 개발자가 작성한 코드를 분석하고 학습한 다음 현재 코드베이스에 삽입할 수 있는 제안 및 코드 조각을 생성한다. 이 기능은 방대한 코드 코퍼스를 학습한 신경망을 사용하여 개발자가 작성할 가능성이 있는 다음 코드 줄을 예측하는 방식으로 작동한다.\n이 기술을 통해 Copilot은 현재 코드의 컨텍스트를 기반으로 전체 함수 또는 클래스를 제안하고 구문적으로 정확하고 모범 사례를 준수하는 코드를 생성할 수도 있다. 또한 Copilot은 생성하는 코드의 의미와 목적을 이해할 수 있으므로 새로운 아이디어를 빠르게 프로토타입으로 만들거나 문제에 대한 다양한 해결책을 모색해야 하는 개발자에게 유용한 도구다.\nGitHub Copilot은 Python, JavaScript, TypeScript, Ruby, Go, R 등 다양한 프로그래밍 언어와 원활하게 작동하도록 설계되었다. 또한 사용자 지정이 가능하므로 개발자가 특정 코드베이스에 대해 학습시켜 제안을 개선하고 더욱 정확하게 만들 수 있다.\nCopilot을 사용하면 얻을 수 있는 잠재적 이점은 상당하다. 코드 작성에 필요한 시간과 노력을 줄임으로써 개발자는 새로운 기능을 설계하거나 기존 기능을 개선하는 등 더 복잡한 작업에 집중할 수 있다. 또한 Copilot은 모범 사례를 따르고 구조적으로 건전하고 읽기 쉬운 코드를 생성하도록 프로그래밍되어 있으므로 오류와 버그를 줄이는 데 도움이 될 수 있다.\nR 사용자는 RStudio를 많이 사용했으나 최근 챗GPT, Github Copilot의 부상으로 개발방식에 변화가 생겨나고 있다. 하지만, RStudio가 곧 Copilot 지원하지는 않을 예정이다. RStudio는 무료 오픈 소스인 반면 Copilot은 Microsoft의 독점 기술이며, Microsoft는 공식 비공개 소스 소프트웨어 및 플러그인에서만 사용할 수 있도록 라이선스를 부여하고 있다. 시중에 존재하는 몇몇 타사 플러그인은 공식 플러그인에서 바이너리를 추출하여 작동하지만, RStudio IDE에는 이런 우회 편법적인 방법을 취하고 있지는 않고 있다.1\n이러한 기술적, 문화적 제약으로 인해 Microsoft와 Posit이 RStudio 내에서 Copilot을 허용하는 방법과 RStudio가 공개 데이터 및 기술을 사용하여 Copilot과 유사한 AI 프로그래밍 도우미를 구현하는 방법 등 다양한 방식을 검토한 후에 2023년 7월 RStudio IDE에 GitHub Copilot 통합 버전을 출시했다.\nRStudio 부조종사는 RStudio IDE에 GitHub Copilot을 통합한 버전으로, RStudio 사용자가 RStudio IDE에서 Copilot을 사용할 수 있도록 지원한다. RStudio 부조종사는 RStudio IDE의 기능과 GitHub Copilot의 기능을 통합하여 RStudio 사용자가 더욱 효율적으로 코드를 작성하고 디버깅할 수 있도록 도와준다. RStudio 부조종사는 RStudio IDE의 사용성을 향상시키고 RStudio 사용자가 더욱 효율적으로 코드를 작성할 수 있도록 도와준다.\n\n\n\n\n\n그림 28.2: RStudio 부조종사 설정방법",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#footnotes",
    "href": "gpt_python.html#footnotes",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "",
    "text": "Github Copilot integration with RStudio #10148↩︎",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_python.html#openai-api",
    "href": "gpt_python.html#openai-api",
    "title": "\n28  챗GPT 환경 구축\n",
    "section": "\n28.4 OpenAI API",
    "text": "28.4 OpenAI API\nOpenAI API는 OpenAI의 인공지능 모형을 활용하는 API 서비스로, 개발자가 쉽게 인공지능 모형을 활용할 수 있도록 제공된다. OpenAI API를 사용하면 개발자는 챗GPT, DALL-E, 위스퍼, 임베딩 등 OpenAI의 다양한 인공지능 언어, 음성, 시각 모형을 활용하여 다양한 인공지능 응용프로그램을 개발할 수 있다. OpenAI API는 RESTful API로 제공되며, 개발자는 HTTP 요청을 통해 OpenAI의 인공지능 모형을 활용할 수 있다. OpenAI, 챗GPT(ChatGPT), OpenAI API를 명확히 구분하는 것이 필요하다.\n\nOpenAI: 회사명\n챗GPT(ChatGPT): AI 응용프로그램 (AI 챗팅 서비스)\nOpenAI API: OpenAI 인공지능 모형을 활용하는 API 서비스\n\nOpenAI 회사가 제시하고 있는 기준에 맞춰 따라줘야 OpenAI API를 사용할 수 있다. 이를 위해 먼저 회원가입을 통한 계정을 생성하고 본인 인증과 과금을 위한 API KEY를 생성하고 프로그래밍 언어에서 쉽게 사용할 수 있도록 파이썬의 경우 openai 패키지를 설치하고 문법에 맞춰 코드를 작성하면 된다.\n\n\n계정생성\nAPI KEY 발급\nopenai 패키지 설치\n헬로월드\n보안강화\n\n\n\nOpenAI 계정 생성 웹사이트에서 계정을 생성한다.\n\n\n\n\n\n그림 28.3: OpenAI 계정 생성\n\n\n\n\nAPI keys 웹사이트에서 API KEY를 발급받는다.\n\n\n\n\n\n그림 28.4: API 키 생성\n\n\n\n\nAPI Reference 안내에 따라 openai 패키지를 설치한다.\n$ pip install openai\n\n\nAPI키를 직접 파이썬 프로그램에 명시하고 결과를 확인한다.\n\nimport openai\n\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"OpenAI API가 뭔가요?\"\n)\n\nprint(response)\n\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\nOpenAI API\\ub294 \\uc778\\uacf5\"\n    }\n  ],\n  \"created\": 1689745304,\n  \"id\": \"cmpl-7duESieoaT985f4IKPskfcYQ3AH7F\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 14,\n    \"prompt_tokens\": 15,\n    \"total_tokens\": 29\n  }\n}\n\n\npython-dotenv 패키지를 사용하여 API KEY를 환경변수로 관리한다. .env 파일에 OpenAI 발급받은 API KEY를 저장하고 파이썬 프로그램에서 .env 파일을 읽어서 API KEY를 사용한다. 버전 관리 시스템에 .env 파일을 포함시키지 않도록 .gitignore 파일에 추가한다. 이와 같은 방식으로 API KEY를 관리하면 API KEY가 노출되는 것을 방지할 수 있다.\n\nimport requests\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n#&gt; True\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"OpenAI API가 뭔가요?\"\n)\n\nprint(response[\"choices\"][0]['text'])\n\nOpenAI API는 OpenAI가 \n\n\n\n\n\n\n그림 28.1: 파이썬 설치 과정\n그림 28.2: RStudio 부조종사 설정방법\n그림 28.3: OpenAI 계정 생성\n그림 28.4: API 키 생성",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>챗GPT 환경 구축</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#질의응답-함수",
    "href": "gpt_prompt_ds.html#질의응답-함수",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "",
    "text": "GPT-3.5\n데이터 탐색은 데이터를 분석하기 전에 데이터셋을 살펴보고 이해하는 단계를 말합니다. 데이터 탐색 단계에서는 데이터의 구조, 패턴, 이상치, 결측값 등을 파악하고 데이터의 특성을 이해하기 위해 시각화와 통계 분석을 활용합니다.\n데이터 과학에서 데이터 탐색은 매우 중요한 단계로, 데이터의 특성과 패턴을 파악하는 데 도움을 줍니다. 데이터 탐색을 통해 데이터의 잠재적인 문제나 패턴을 발견할 수 있고, 데이터 전처리나 모델링 단계에서 필요한 추가적인 작업을 수행할 수 있습니다.\n또한, 데이터 탐색을 통해 데이터의 관계나 인사이트를 발견하고 이를 시각화하여 이해하기 쉽게 전달할 수 있습니다. 따라서 데이터 탐색은 데이터 과학의 핵심적인 부분이며, 데이터를 더 잘 이해하고 분석할 수 있는 기반을 제공합니다. 이러한 데이터 탐색 과정을 R 언어를 사용하여 다양한 시각화 기법과 통계 분석을 통해 진행할 수 있습니다.\n\n\n\n\nGPT-4\n데이터 탐색 혹은 탐색적 데이터 분석(Exploratory Data Analysis, EDA)는 데이터를 열어보고 분석하는 단계입니다. 그래프나 통계량을 사용하여 수집된 데이터의 구조 및 관계를 파악하는 데 사용됩니다.\n데이터 탐색의 목적은 데이터의 주요 특성을 찾아내고, 구조를 이해하거나 특정 변수 사이의 관계를 확인하거나, 예상치 못한 패턴이나 이상치를 찾는 것입니다.\n데이터 과학에서 데이터 탐색은 매우 중요합니다. 일단 분석 전에 데이터의 특성을 이해하는 것은 추론 및 모델링 과정에서 중요한 역할을 합니다. 데이터 탐색을 통해 이상치를 발견할 수 있고, 이를 통해 데이터 클리닝 및 전처리를 이행할 수 있습니다. 또한, 변수들간의 관계 및 패턴을 분석함으로써, 이후의 분석 방향을 결정하거나 가설을 설정하는데 도움이 됩니다.\n예를 들어, 어떤 상점의 판매 데이터를 분석한다고 가정해봅시다. 데이터 탐색 과정에서 고객 연령대별로 구매 패턴이 유의미하게 차이가 난다면 이를 분석에 반영하여 연령대별 마케팅 전략을 수립하는데 도움이 될 수 있습니다. 또한, 특정 상품이 후불결제 시 결제 실패율이 높다는 이상치를 발견하였다면, 이 문제를 해결하기 위한 방안을 마련하는데 도움이 됩니다. 또한 데이터 탐색을 통해 데이터의 품질 및 적절성을 평가할 수 있으며, 데이터 과학자가 보다 의미있는 결과와 인사이트를 도출하는데 기여하게 됩니다.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  },
  {
    "objectID": "gpt_prompt_ds.html#프롬프트",
    "href": "gpt_prompt_ds.html#프롬프트",
    "title": "\n29  데이터 과학 프롬프트\n",
    "section": "\n29.2 프롬프트",
    "text": "29.2 프롬프트\n탐색적 데이터 분석에 초점을 맞춰 다음 질문에 대해서 챗GPT 응답결과를 정리해보자.\n\n데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\n데이터 탐색을 위한 인기있는 R 패키지는 무엇이며, 어떻게 사용되나요?\nR을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\nt-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\ntibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\ntibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\ntibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\ntibble과 ggplot2를 사용하여 산점도와 라인 차트를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n\n\n\n\ngraph LR\n    subgraph 이해하기[\"이해하기\"]\n        모형 --&gt; 시각화\n        변환 --&gt; 모형\n        시각화 --&gt; 변환\n    end\n    import --&gt; 깔끔화 --&gt; 이해하기\n    이해하기 --&gt; 의사소통\n\n    classDef modern fill:#fff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:12px;\n    classDef emphasize fill:#8CBDE3,stroke:#333,stroke-width:3px,color:#333,font-family:MaruBuri,font-size:15px,font-weight:bold;\n    classDef subgraphStyle fill:#f0f8ff,stroke:#333,stroke-width:2px,color:#333,font-family:MaruBuri,font-size:15px;\n\n    class import,깔끔화,변환,모형,시각화,의사소통 modern;\n    class 이해하기 subgraphStyle;\n\n\n\n\n\n\n\n29.2.1 질문 1\n\nlibrary(reticulate)\nquestion_01 &lt;- \"데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\"\n\nanswer_01 &lt;- py$answer_question(question_01)\n\n\n\n질문\n답변\n\n\n\n데이터 탐색이란 무엇이며, 데이터 과학에서 어떻게 유용한가요?\n\n\n데이터 탐색 혹은 탐색적 데이터 분석(Exploratory Data Analysis, EDA)는 데이터를 열어보고 분석하는 단계입니다. 그래프나 통계량을 사용하여 수집된 데이터의 구조 및 관계를 파악하는 데 사용됩니다.\n데이터 탐색의 목적은 데이터의 주요 특성을 찾아내고, 구조를 이해하거나 특정 변수 사이의 관계를 확인하거나, 예상치 못한 패턴이나 이상치를 찾는 것입니다.\n데이터 과학에서 데이터 탐색은 매우 중요합니다. 일단 분석 전에 데이터의 특성을 이해하는 것은 추론 및 모델링 과정에서 중요한 역할을 합니다. 데이터 탐색을 통해 이상치를 발견할 수 있고, 이를 통해 데이터 클리닝 및 전처리를 이행할 수 있습니다. 또한, 변수들간의 관계 및 패턴을 분석함으로써, 이후의 분석 방향을 결정하거나 가설을 설정하는데 도움이 됩니다.\n예를 들어, 어떤 상점의 판매 데이터를 분석한다고 가정해봅시다. 데이터 탐색 과정에서 고객 연령대별로 구매 패턴이 유의미하게 차이가 난다면 이를 분석에 반영하여 연령대별 마케팅 전략을 수립하는데 도움이 될 수 있습니다. 또한, 특정 상품이 후불결제 시 결제 실패율이 높다는 이상치를 발견하였다면, 이 문제를 해결하기 위한 방안을 마련하는데 도움이 됩니다. 또한 데이터 탐색을 통해 데이터의 품질 및 적절성을 평가할 수 있으며, 데이터 과학자가 보다 의미있는 결과와 인사이트를 도출하는데 기여하게 됩니다.\n\n\n\n\n29.2.2 질문 2\n\nquestion_02 &lt;- \"R을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_02 &lt;-  py$answer_question(question_02)\n\n\n\n질문\n답변\n\n\n\nR을 사용한 기본적인 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n\ncat(\"물론입니다. R 언어를 활용한 간단한 데이터 탐색 스크립트를 제공하겠습니다.\\n\\n우선, 내장 데이터셋인 \\\"mtcars\\\"을 사용하여 살펴보겠습니다.\\n\\n```R\\n# 데이터 불러오기\\ndata(mtcars)\\n\\n# 데이터의 상위 6개 행 보기\\nhead(mtcars)\\n\\n# 데이터의 기본적인 구조 보기\\nstr(mtcars)\\n\\n# 간단한 요약통계 보기\\nsummary(mtcars)\\n\\n# 변수 간 상관관계 보기\\ncor(mtcars)\\n\\n# mpg 변수에 대한 히스토그램 생성\\nhist(mtcars$mpg)\\n\\n# 변수 간 산점도 행렬 생성\\npairs(mtcars)\\n```\\n\\n위의 예시는 mtcars 데이터셋을 불러와 파일의 상위 6줄을 보는 것으로 시작합니다. 그 신 dataset의 전반적인 구조 (변수의 이름, 유형, 데이터 셋에서 행의 수 등) 를 확인하고, 요약 통계를 제공합니다. 그 다음으로, 변수 간 상관관계를 설명하며 mpg 변수를 가진 히스토그램을 그립니다. 마지막으로, 산점도 행렬을 생성하여 변수들 사이의 관계를 한눈에 보여줍니다. \\n\\n이러한 단계들은 기본적인 데이터 탐색 과정에서 중요하며, 상황에 따라 추가분석이 필요한 경우가 있습니다.\\n이는 데이터에 대한 조사나, 누락 된 값의 확인, 이상치 검사 등이 포함될 수 있습니다.\")\n\n\n\n\n\n29.2.3 질문 3\n\nquestion_03 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_03 &lt;- py$answer_question(question_03)\n\n\n\n질문\n답변\n\n\n\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n\n#&gt; PCA (Principal Component Analysis)는 데이터의 차원 축소를 위한 테크닉 중 하나입니다. 이는 관찰된 변수들 간의 상관관계를 기반으로 데이터의 차원을 축소하는 방법입니다. \n#&gt; \n#&gt; PCA를 사용하여 R에서 변수의 관계를 탐색하려면 다음과 같이 수행할 수 있습니다:\n#&gt; \n#&gt; 1. 데이터 전처리: 분석을 위한 데이터는 종종 어떤 변수도 누락되지 않도록 전처리되어야 합니다. 또한, PCA는 숫자 변수에 대해 작동하므로 범주형 데이터는 적절하게 변환되어야 합니다.\n#&gt; \n#&gt; 2. 스케일링: PCA는 상관행렬이나 공분산 행렬에 대해 수행되므로 모든 변수가 동일한 스케일을 가져야 합니다. 이를 위해 `scale()` 함수를 사용하여 데이터를 정규화 할 수 있습니다.\n#&gt; \n#&gt; 3. PCA 수행: `prcomp()` 또는 `princomp()` 함수를 사용하여 PCA를 수행하고 결과를 저장할 수 있습니다.\n#&gt; \n#&gt; ```r\n#&gt; pca &lt;- prcomp(data, scale = TRUE)\n#&gt; ```\n#&gt; 4. 결과 시각화 및 해석: `summary()` 함수와 `biplot()` 함수를 사용하여 결과를 해석하고 시각화 할 수 있습니다. `summary(pca)`를 사용하여 설명된 분산의 백분율을 검토해 보고, `biplot(pca)`를 사용하여 기여도를 시각화 할 수 있습니다.\n#&gt; \n#&gt; ```r\n#&gt; summary(pca)\n#&gt; biplot(pca)\n#&gt; ```\n#&gt; \n#&gt; 이렇게 하면 각 주성분이 데이터의 분산을 어느 정도 설명하는지 그리고 각 변수가 주성분에 어떻게 기여하는지 쉽게 이해할 수 있습니다. 이 정보를 통해 변수간의 관계를 분석하고 이해할 수 있습니다.\n\n\n\n\n\n29.2.4 질문 4\n\nquestion_04 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_04 &lt;- py$answer_question(question_04)\n\n\n\n질문\n답변\n\n\n\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n\n#&gt; PCA(Principal Component Analysis, 주성분 분석)는 많은 변수에서 가장 중요한 패턴을 찾을 수 있도록 도와주는 통계적 기법으로, 이를 통해 데이터의 차원을 축소할 수 있습니다. R에서는 'prcomp' 또는 'princomp' 함수를 사용하여 PCA를 수행할 수 있습니다. \n#&gt; \n#&gt; 추상적인 개념이지만 구체적인 예제를 통해 설명해보겠습니다.\n#&gt; \n#&gt; 1. 데이터 준비: 먼저, 분석할 데이터셋을 로드하고 정제합니다. 모든 변수는 수치형이어야 하며, 결측치가 없어야 합니다.\n#&gt; \n#&gt; 2. 데이터 정규화: PCA는 변수의 스케일에 민감하므로, 데이터는 진행하기 전에 반드시 정규화(normalization)되어야 합니다. R에서는 scale() 함수를 사용해서 모든 변수를 같은 스케일로 조정할 수 있습니다.\n#&gt; \n#&gt; 3. PCA 수행: prcomp() 함수를 사용하여 PCA를 실행합니다. 예를 들어, 데이터 프레임이 df라면 아래와 같이 코드를 작성할 수 있습니다.\n#&gt;     ```\n#&gt;     pca_result &lt;- prcomp(df, center = TRUE, scale. = TRUE)\n#&gt;     ```\n#&gt;     이 함수는 PCA 결과를 반환하며, 각 주성분의 중요도, 각 원래 변수가 주성분에 얼마나 기여하는지 (rotation 또는 loadings라고 함) 등을 제공합니다.\n#&gt; \n#&gt; 4. 결과 분석: summary() 함수를 사용하여 각 주성분이 데이터의 얼마나 큰 비율을 설명하는지 확인할 수 있습니다. \n#&gt;     ```\n#&gt;     summary(pca_result)\n#&gt;     ```\n#&gt;     또한 R에서 제공하는 다양한 시각화 기능 (예: biplot)을 사용하여 결과를 시각화하고 분석할 수 있습니다. 이를 통해 변수 간의 관계를 더 자세히 조사하거나, 데이터를 더 낮은 차원으로 투영하여 차원 축소를 수행할 수 있습니다. \n#&gt; \n#&gt; 따라서 PCA는 데이터의 차원을 줄이면서도 가장 중요한 정보를 유지하는 데 도움이 되며, 이는 특히 데이터의 패턴을 탐색하거나 시각화할 때 유용합니다.\n\n\n\n\n\n29.2.5 질문 5\n\nquestion_05 &lt;- \"t-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_05 &lt;- py$answer_question(question_05)\n\n\n\n질문\n답변\n\n\n\nt-SNE, PCA 및 클러스터링을 사용하여 변수간 관계를 탐색하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n\n#&gt; 네, 말씀하신대로 R 언어를 사용하여 t-SNE, PCA 및 클러스터링을 사용하여 데이터의 변수 간 관계를 탐색하는 샘플 스크립트를 아래에 제공합니다. 이 예시에서는 내장 데이터 세트인 'iris'를 사용합니다.\n#&gt; \n#&gt; ```R\n#&gt; # 필요한 패키지를 불러오기\n#&gt; library(tidyverse)\n#&gt; library(Rtsne)\n#&gt; library(cluster)\n#&gt; library(factoextra)\n#&gt; \n#&gt; # 데이터 불러오기\n#&gt; data(\"iris\")\n#&gt; \n#&gt; # PCA를 이용한 변수 간 관계 탐색\n#&gt; pca_iris &lt;- prcomp(iris[, 1:4], scale = TRUE)\n#&gt; \n#&gt; # PCA 결과 확인\n#&gt; fviz_pca_ind(pca_iris, geom.ind='point',\n#&gt;              pointshape = 21, pointsize = 2, \n#&gt;              fill.ind = iris$Species, palette = \"jco\", \n#&gt;              addEllipses = TRUE, ellipse.level = 0.95)\n#&gt; \n#&gt; # t-SNE를 이용한 변수간 관계 탐색\n#&gt; tsne_iris &lt;- Rtsne(iris[, 1:4], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500)\n#&gt; \n#&gt; # t-SNE 결과 확인 \n#&gt; tsne_iris$Y %&gt;%\n#&gt;   data.frame() %&gt;%\n#&gt;   setNames(c(\"X\", \"Y\")) %&gt;%\n#&gt;   mutate(Species = iris$Species) %&gt;%\n#&gt;   ggplot(aes(X, Y, color = Species)) +\n#&gt;   geom_point() +\n#&gt;   theme_minimal()\n#&gt; \n#&gt; # 클러스터링을 이용한 변수간 관계 탐색\n#&gt; dist_iris &lt;- dist(iris[, 1:4])\n#&gt; hclust_iris &lt;- hclust(dist_iris)\n#&gt; cluster_iris &lt;- cutree(hclust_iris, k = 3)\n#&gt; \n#&gt; # 클러스터링 결과 확인\n#&gt; table(cluster_iris, iris$Species)\n#&gt; plot(hclust_iris)\n#&gt; rect.hclust(hclust_iris, k = 3, border = \"red\")\n#&gt; ```\n#&gt; t-SNE와 PCA는 고차원 데이터를 시각화하기 위해 사용되며, 이를 통해 변수들 사이의 관계를 기하학적으로 이해할 수 있습니다. 클러스터링은 데이터 세트를 비슷한 특성을 가진 그룹으로 분류할 때 사용하며, 이를 통해 관계를 파악할 수 있습니다.\n#&gt; 출력 그래프와 함께 이들 결과를 해석하면 더 깊은 인사이트를 얻을 수 있습니다.\n\n\n\n\n\n29.2.6 질문 6\n\nquestion_06 &lt;- \"tibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\"\n\n# answer_06 &lt;- py$answer_question(question_06)\n\n\n\n질문\n답변\n\n\n\ntibble과 ggplot2를 사용하여 시계열 데이터에서 패턴과 추세를 어떻게 식별할 수 있나요?\n\n\n\n#&gt; R 언어에서 tibble과 ggplot2 패키지는 시계열 데이터에서 패턴과 추세를 식별하는데 사용할 수 있는 유용한 도구입니다.\n#&gt; \n#&gt; 1. 첫 번째로, tibble은 R에서 데이터 프레임을 다루기 위한 새로운 데이터 타입입니다. 기본 R 데이터 프레임의 단점을 보완하면서, 동일한 구조와 작업을 유지합니다. 그러므로 tibble은 아주 큰 데이터셋에서도 처리 속도가 빠르며, 시계열 데이터를 포함한 모든 종류의 데이터를 쉽게 다룰 수 있게 해줍니다.\n#&gt; \n#&gt; 예를 들어, 우리는 다음과 같은 방법으로 시계열 데이터를 tibble로 변환할 수 있습니다:\n#&gt; \n#&gt; ```r\n#&gt; # 필요한 패키지 불러오기\n#&gt; library(tidyverse)\n#&gt; \n#&gt; # 일별 주가 데이터를 tibble로 변환\n#&gt; stock_prices &lt;- as_tibble(stock_prices)\n#&gt; ```\n#&gt; \n#&gt; 2. 다음으로, ggplot2는 'The Grammar of Graphics'를 기반으로 하는 데이터 시각화 패키지입니다. 이를 이용해 시계열 데이터에서 패턴과 추세를 직관적으로 파악할 수 있습니다.\n#&gt; \n#&gt; 예를 들어, ggplot2를 이용해 주가의 시간에 따른 변화를 그래프로 그릴 수 있습니다.\n#&gt; \n#&gt; ```r\n#&gt; ggplot(data = stock_prices, aes(x = Date, y = Close)) +\n#&gt;   geom_line() +\n#&gt;   labs(title = \"Stock prices over time\", x = \"Date\", y = \"Close price\")\n#&gt; ```\n#&gt; \n#&gt; 여기서 geom_line() 함수는 시간에 따른 주가의 변화를 선 그래프로 그려줍니다. 이를 통해 데이터에서 패턴이나 트렌드를 시각적으로 파악할 수 있습니다. \n#&gt; \n#&gt; 또한, ggplot2 패키지는 다양한 통계 변환 함수와 smoothers를 제공하여 시계열 데이터의 추세를 더 잘 나타낼 수 있습니다. 예를 들어, geom_smooth() 함수는 일련의 데이터 포인트에 적합한 부드러운 곡선을 추가하여 데이터의 전반적인 추세를 도식화해 줍니다.\n\n\n\n\n\n29.2.7 질문 7\n\nquestion_07 &lt;- \"tibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\"\n\n# answer_07 &lt;- py$answer_question(question_07)\n\n\n\n질문\n답변\n\n\n\ntibble과 ggplot2를 사용하여 데이터에서 패턴과 추세를 식별하는 데이터 탐색 스크립트의 예시를 제공할 수 있나요?\n\n\n\n#&gt; 네, 물론입니다. 다음의 예를 들어보겠습니다. 먼저 필요한 라이브러리를 로드합니다:\n#&gt; \n#&gt; ```R\n#&gt; library(tidyverse)  # includes both tibble and ggplot2\n#&gt; ```\n#&gt; \n#&gt; 다음으로, 패턴이나 추세를 확인하려는 데이터를 예로 들어, 내장되어 있는 미국 여성의 생존 기대치를 보여주는 데이터셋인 `women`을 불러옵니다:\n#&gt; \n#&gt; ```R\n#&gt; data(\"women\")\n#&gt; ```\n#&gt; \n#&gt; `tibble`을 사용해 데이터를 보다 더 보기 쉽게 확인할 수 있습니다:\n#&gt; \n#&gt; ```R\n#&gt; women_tibble &lt;- as_tibble(women)\n#&gt; print(women_tibble)\n#&gt; ```\n#&gt; \n#&gt; `ggplot2`을 사용해 이 데이터의 패턴과 추세를 시각화하겠습니다. 본 예시에서는 women 데이터셋에서 나이에 따른 평균 체중을 나타내는 그래프를 그립니다:\n#&gt; \n#&gt; ```R\n#&gt; ggplot(data = women_tibble, aes(x = height, y = weight)) +\n#&gt;   geom_point(shape=1) +    # Use hollow circles\n#&gt;   geom_smooth(method = lm, se = FALSE, color=\"red\") +  # Add linear regression line, without CI\n#&gt;   labs(title = \"Age vs Weight in Women\", x = \"Height\", y = \"Weight\") # labels\n#&gt; ```\n#&gt; 데이터를 plot해 본 결과, `women` 데이터셋에서 여성의 키와 몸무게 사이에 양의 상관 관계가 있음을 발견할 수 있습니다. `geom_smooth` 함수를 사용하면, 이 데이터의 추세를 더 명확하게 파악할 수 있습니다. 이는 `ggplot2` 패키지의 많은 기능 중 하나입니다.\n#&gt; \n#&gt; 이런 식으로 `tibble`과 `ggplot2`를 활용하면, 데이터를 쉽게 관리하고 시각화하면서 여러 패턴과 추세를 찾아낼 수 있습니다. 이는 데이터 과학자가 데이터를 이해하는 데 매우 중요한 과정입니다.\n\n\n\n\n\n29.2.8 질문 8\n\nquestion_08 &lt;- \"tibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\"\n\n# answer_08 &lt;- py$answer_question(question_08)\n\n\n\n질문\n답변\n\n\n\ntibble과 ggplot2를 사용하여 변수간 관계를 탐색하기 위한 일반적인 기술은 무엇인가요?\n\n\n\n#&gt; tibble과 ggplot2는 R 언어에서 데이터를 다루고 시각화하는 데 널리 사용되는 도구입니다. 변수 간 관계를 탐색하는 일반적인 기법은 아래와 같습니다.\n#&gt; \n#&gt; 1. **tibble 사용:**\n#&gt; \n#&gt;    tibble은 데이터 프레임을 다루는 R 패키지로, 데이터 프레임을 더 유연하게 조작할 수 있도록 만들어진 향상된 데이터 프레임입니다. tibble을 사용하면, 데이터 요약 및 변환을 위한 dplyr 함수들을 사용할 수 있습니다. \n#&gt; \n#&gt;    변수 간 관계를 탐색하는 tibble의 일반적인 방법은 'group_by', 'summarize'와 같은 dplyr 함수를 사용하여 특정 변수에 따라 데이터를 그룹 지어 분석하는 것입니다. 'filter', 'select', 'mutate', 'arrange'와 같은 함수들도 유용하게 사용될 수 있습니다.\n#&gt; \n#&gt; 2. **ggplot2 사용:**\n#&gt; \n#&gt;    ggplot2는 R에서 가장 인기 있는 데이터 시각화 패키지 중 하나로, 데이터를 그래프로 그려 복잡한 데이터 패턴을 빠르게 이해할 수 있도록 합니다.\n#&gt; \n#&gt;    ggplot2를 사용해서 변수 간 관계를 탐색하는 일반적인 방법 중 하나는 산점도 (scatter plot)를 그리는 것입니다. 두 변수 간의 관계를 보여주는 가장 기본적인 그래프 형태입니다. \n#&gt; \n#&gt;    또한, ggplot2의 'facet_wrap' 또는 'facet_grid' 기능을 사용하면, 한 변수의 서로 다른 수준에 따른 다른 변수의 분포를 여러 개의 작은 패널로 보여주는 그래프를 만들 수 있습니다. \n#&gt; \n#&gt;    'geom_smooth'를 사용하면 추세선을 추가하여 변수 간 관계를 보다 명확하게 확인할 수도 있습니다.\n#&gt;    \n#&gt; 이외에도 histogram, boxplot, violin plot 등 다양한 그래프를 활용하여 변수 간 관계를 탐색할 수 있습니다. 주어진 데이터 특성과 질문에 따라 적절한 방법을 선택하면 됩니다.\n\n\n\n\n\n29.2.9 질문 9\n\nquestion_09 &lt;- \"tibble과 ggplot2를 사용하여 산점도와 선 그래프를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\"\n\n# answer_09 &lt;- py$answer_question(question_09)\n\n\n\n질문\n답변\n\n\n\ntibble과 ggplot2를 사용하여 산점도와 선 그래프를 생성하여 변수간 관계를 탐색할 수 있는 방법은 무엇인가요?\n\n\n\ncat(\"R에서 'tibble'이란 데이터 프레임을 보다 쉽게 다룰 수 있게 설계된 데이터 구조이며, 'ggplot2'는 복잡한 그래프를 효과적으로 생성할 수 있게 도와주는 패키지입니다. \\n\\n선 그래프와 산점도를 생성하여 변수 간의 관계를 탐색하는 방식은 아래와 같이 구성할 수 있습니다.\\n\\n먼저, 필요한 패키지를 설치하고 불러와야 합니다.\\n```R\\ninstall.packages(\\\"dplyr\\\")\\ninstall.packages(\\\"ggplot2\\\")\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n```\\n\\n다음으로, tibble 형태의 데이터를 생성하거나 불러옵니다.\\n```R\\ndf &lt;- tibble::tibble(\\n  x = rnorm(100),  # 100개의 표준 정규분포 난수 생성\\n  y = 1.5 * x + rnorm(100)  # x값에 따른 y값 생성\\n)\\n```\\n이 데이터는 x와 y이라는 두 개의 변수로 이루어진 100개의 관측치를 담고 있습니다.\\n\\n그 후, 'ggplot2'를 사용하여 산점도를 생성할 수 있습니다. \\n```R\\nggplot(data = df, mapping = aes(x = x, y = y)) +\\n  geom_point()  # 산점도 생성\\n```\\n\\n선 그래프를 생성하기 위해서는 'geom_line()'을 사용합니다.\\n```R\\nggplot(data = df, mapping = aes(x = x, y = y)) +\\n  geom_line()  # 선 그래프 생성\\n```\\n\\n이렇게 하면 x와 y의 관계를 산점도와 선 그래프로 시각적으로 표현할 수 있습니다. 각 점이나 선의 위치와 형태를 통해 변수들 간의 관계를 살펴볼 수 있게 됩니다. \\n\\n추가적으로, 여러 그래픽 파라미터를 조정하는 것으로 그래프를 보다 세밀하게 조정할 수 있습니다. 예를 들어, 'labs()', 'theme()', 'geom_smooth()', 'facet_wrap()' 등의 함수들을 조합하면 다양한 복잡한 그래프를 만들 수 있습니다.\")\n#&gt; R에서 'tibble'이란 데이터 프레임을 보다 쉽게 다룰 수 있게 설계된 데이터 구조이며, 'ggplot2'는 복잡한 그래프를 효과적으로 생성할 수 있게 도와주는 패키지입니다. \n#&gt; \n#&gt; 선 그래프와 산점도를 생성하여 변수 간의 관계를 탐색하는 방식은 아래와 같이 구성할 수 있습니다.\n#&gt; \n#&gt; 먼저, 필요한 패키지를 설치하고 불러와야 합니다.\n#&gt; ```R\n#&gt; install.packages(\"dplyr\")\n#&gt; install.packages(\"ggplot2\")\n#&gt; library(dplyr)\n#&gt; library(ggplot2)\n#&gt; ```\n#&gt; \n#&gt; 다음으로, tibble 형태의 데이터를 생성하거나 불러옵니다.\n#&gt; ```R\n#&gt; df &lt;- tibble::tibble(\n#&gt;   x = rnorm(100),  # 100개의 표준 정규분포 난수 생성\n#&gt;   y = 1.5 * x + rnorm(100)  # x값에 따른 y값 생성\n#&gt; )\n#&gt; ```\n#&gt; 이 데이터는 x와 y이라는 두 개의 변수로 이루어진 100개의 관측치를 담고 있습니다.\n#&gt; \n#&gt; 그 후, 'ggplot2'를 사용하여 산점도를 생성할 수 있습니다. \n#&gt; ```R\n#&gt; ggplot(data = df, mapping = aes(x = x, y = y)) +\n#&gt;   geom_point()  # 산점도 생성\n#&gt; ```\n#&gt; \n#&gt; 선 그래프를 생성하기 위해서는 'geom_line()'을 사용합니다.\n#&gt; ```R\n#&gt; ggplot(data = df, mapping = aes(x = x, y = y)) +\n#&gt;   geom_line()  # 선 그래프 생성\n#&gt; ```\n#&gt; \n#&gt; 이렇게 하면 x와 y의 관계를 산점도와 선 그래프로 시각적으로 표현할 수 있습니다. 각 점이나 선의 위치와 형태를 통해 변수들 간의 관계를 살펴볼 수 있게 됩니다. \n#&gt; \n#&gt; 추가적으로, 여러 그래픽 파라미터를 조정하는 것으로 그래프를 보다 세밀하게 조정할 수 있습니다. 예를 들어, 'labs()', 'theme()', 'geom_smooth()', 'facet_wrap()' 등의 함수들을 조합하면 다양한 복잡한 그래프를 만들 수 있습니다.\n\n\n\n\n\n29.2.10 질문 10\n\nquestion_10 &lt;- \"PCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\"\n\n# answer_10 &lt;- py$answer_question(question_10)\n\n\n\n질문\n답변\n\n\n\nPCA를 사용하여 변수간 관계를 탐색하기 위해 차원 축소를 어떻게 수행할 수 있나요?\n\n\n\n#&gt; 주성분 분석(PCA)는 R에서 차원 축소를 수행하는 데 사용되는 기법 중 하나로, 변동이 가장 큰 데이터의 축을 찾아 높은 차원의 데이터를 낮은 차원으로 변환시키는 기법을 설명합니다.\n#&gt; \n#&gt; 다음은 R에서 PCA를 수행하기 위한 간단한 단계입니다:\n#&gt; \n#&gt; 1. 데이터를 준비합니다. PCA는 연속형 변수에만 적용될 수 있으므로, 범주형 변수를 포함하고 있다면 이를 제거하거나 숫자로 변환해야 합니다.\n#&gt; \n#&gt; 2. 데이터 스케일링을 수행합니다. PCA는 변수의 범위에 따라 결과가 크게 달라질 수 있으므로, 데이터의 표준화 또는 정규화 과정이 필요합니다. R의 `scale()` 함수를 사용할 수 있습니다.\n#&gt; \n#&gt; 3. `prcomp()` 또는 `princomp()` 함수를 사용하여 PCA를 수행합니다. 주성분의 개수를 지정하거나, 데이터에 총 변동성의 몇 퍼센트를 설명하도록 할지 결정할 수 있습니다.\n#&gt; \n#&gt; 4. `summary()` 함수를 사용하여 결과를 확인합니다. 여기에는 각 주성분이 설명하는 변동성의 비율, 누적 설명된 변동중 어느 정도의 비율을 차지하는 지 등의 정보가 포함되어 있습니다.\n#&gt; \n#&gt; 5. 필요에 따라 결과를 시각화합니다. 이를 통해 각 주성분이 어떤 변수와 강한 연관성을 보이는지 확인할 수 있습니다. `biplot()` 함수를 사용하면 주성분과 개별 변수 사이의 관계를 한눈에 볼 수 있습니다.\n#&gt; \n#&gt; 이런 방법을 통해 데이터 세트의 차원을 축소하고, 변수 간의 패턴이나 관계를 더 잘 이해하고 시각화하는 데 도움이 될 수 있습니다.\n\n\n\n\n\n\n\n그림 29.1: 데이터 과학자 업무방식 변화\n\n\n\nGencay, I. 2023. “ChatGPT Guide for Data Scientists: Top 40 Most Important Prompts”. Towards AI, 4월. https://pub.towardsai.net/chatgpt-guide-for-data-scientists-top-40-most-important-prompts-cdb911f3a427.",
    "crumbs": [
      "**8부** 챗GPT",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>데이터 과학 프롬프트</span>"
    ]
  }
]